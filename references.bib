
@article{mahalingam_institutional_2007,
	title = {Institutional {Theory} as a {Framework} for {Analyzing} {Conflicts} on {Global} {Projects}},
	volume = {133},
	issn = {0733-9364},
	url = {https://ascelibrary.org/doi/abs/10.1061/%28ASCE%290733-9364%282007%29133%3A7%28517%29},
	doi = {10.1061/(ASCE)0733-9364(2007)133:7(517)},
	abstract = {Global construction projects that involve collaboration between participants from multiple countries often result in unique challenges, and costs due to cross-national interactions. Case studies performed to investigate the cross-national interactions and tensions present on global projects suggest that institutional differences—differences in workplace norms, legal regulations, and cultural values—contribute to these costs. We demonstrate how institutional theory—a branch of organizational theory—can comprehensively describe the cross-national challenges on global projects. We show how this theory can help practitioners to more accurately classify the cross-national issues they encounter, determine the causes behind the conflicts, and judge the relative ease with which each type of conflict can be resolved. However, there are gaps in the extant application of institutional theory that prevent us from predicting institutional conflicts on global projects and devising solution strategies. These gaps are identified and a research trajectory to understand them is proposed. This paper is aimed at starting a much-needed dialogue on the mitigation of cross-national issues on global projects, and not as a demonstration of methods to eliminate all cross-national conflicts.},
	language = {EN},
	number = {7},
	urldate = {2021-03-17},
	journal = {Journal of Construction Engineering and Management},
	author = {Mahalingam, Ashwin and Levitt, Raymond E.},
	month = jul,
	year = {2007},
	note = {Publisher: American Society of Civil Engineers},
	keywords = {Case reports, Conflict, International factors, Organizations, Project management},
	pages = {517--528},
}

@incollection{kuhrmann_possibilities_2018,
	address = {Cham},
	title = {Possibilities of {Applying} {Institutional} {Theory} in the {Study} of {Hybrid} {Software} {Development} {Concepts} and {Practices}},
	volume = {11271},
	isbn = {978-3-030-03672-0 978-3-030-03673-7},
	url = {http://link.springer.com/10.1007/978-3-030-03673-7_35},
	abstract = {Nowadays, hybrid software development approaches represent an important trend. By creatively combining various software development methods and techniques, companies seek to beneﬁt from an increased ﬂexibility in their software-intensive domains. This conceptual paper has two goals. First, it attempts to extend the concept of hybridity beyond the visible aspects of software development. Second, it introduces the concept of “institutional logic” as a cornerstone adopted from institutional theory. I propose to use this theory as a lens to improve our understanding of the waterfall/agile type of hybridity, i.e. when the logic of Traditional Software Engineering and the logic of Agile Software Development are concurrently adopted in an organization. Also, a relation between institutional logics and organizational cultures is proposed. The seeds of theory presented in this paper lead to a further theory building effort that will hopefully result in a better characterization of adoption motives and strategies related to hybrid software development.},
	language = {en},
	urldate = {2021-03-17},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Doležel, Michal},
	editor = {Kuhrmann, Marco and Schneider, Kurt and Pfahl, Dietmar and Amasaki, Sousuke and Ciolkowski, Marcus and Hebig, Regina and Tell, Paolo and Klünder, Jil and Küpper, Steffen},
	year = {2018},
	doi = {10.1007/978-3-030-03673-7_35},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {441--448},
}

@article{scott_approaching_2008,
	title = {Approaching adulthood: the maturing of institutional theory},
	volume = {37},
	issn = {0304-2421, 1573-7853},
	shorttitle = {Approaching adulthood},
	url = {http://link.springer.com/10.1007/s11186-008-9067-z},
	doi = {10.1007/s11186-008-9067-z},
	abstract = {I summarize seven general trends in the institutional analysis of organizations which I view as constructive and provide evidence of progress in the development of this perspective. I emphasize corrections in early theoretical limitations as well as improvements in the use of empirical indicators and an expansion of the types of organizations included and issues addressed by institutional theorists.},
	language = {en},
	number = {5},
	urldate = {2021-03-17},
	journal = {Theory and Society},
	author = {Scott, W. Richard},
	month = oct,
	year = {2008},
	pages = {427--442},
}

@article{dimaggio_iron_1983,
	title = {The {Iron} {Cage} {Revisited}: {Institutional} {Isomorphism} and {Collective} {Rationality} in {Organizational} {Fields}},
	volume = {48},
	issn = {0003-1224},
	shorttitle = {The {Iron} {Cage} {Revisited}},
	url = {https://www.jstor.org/stable/2095101},
	doi = {10.2307/2095101},
	abstract = {What makes organizations so similar? We contend that the engine of rationalization and bureaucratization has moved from the competitive marketplace to the state and the professions. Once a set of organizations emerges as a field, a paradox arises: rational actors make their organizations increasingly similar as they try to change them. We describe three isomorphic processes--coercive, mimetic, and normative--leading to this outcome. We then specify hypotheses about the impact of resource centralization and dependency, goal ambiguity and technical uncertainty, and professionalization and structuration on isomorphic change. Finally, we suggest implications for theories of organizations and social change.},
	number = {2},
	urldate = {2021-03-17},
	journal = {American Sociological Review},
	author = {DiMaggio, Paul J. and Powell, Walter W.},
	year = {1983},
	note = {Publisher: [American Sociological Association, Sage Publications, Inc.]},
	pages = {147--160},
}

@inproceedings{wojtynek_assisted_2020,
	title = {Assisted {Planning} and {Setup} of {Collaborative} {Robot} {Applications} in {Modular} {Production} {Systems}},
	volume = {1},
	doi = {10.1109/ETFA46521.2020.9212083},
	abstract = {Modern production requires flexible systems to fulfill customer desires of highly specialized products down to a batch size of one. Collaborative robots are considered as the key element for a versatile and flexible production in a state-of-the-art and modular production environment. Although modularization is a well known concept since the 80s, the composition of modular fabrication units and the flexible programming of robots are still challenging and depend strongly on the experience of the industrial engineer. For this reason, we present an assisted approach for interactive workspace setup targeting the operator to plan and commission efficiently applications containing collaborative robots in flexible production systems. As a central contribution, we introduce a planning workflow to take the human-in-the-loop for the workspace configuration of the robot and the arrangement of components in the modular production system. We evaluate our contribution quantitatively in an initial experimental setup performing the automatized layout in complex configurations to showcase its effectiveness.},
	booktitle = {2020 25th {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	author = {Wojtynek, M. and Leichert, J. and Wrede, S.},
	month = sep,
	year = {2020},
	note = {ISSN: 1946-0759},
	keywords = {Flexible Robotic System, Industry 4.0, Intelligent Manufacturing, Modular Production, Workspace Reconfiguration, collaborative robot applications, collaborative robots, commission efficiently applications, flexible manufacturing systems, flexible production systems, flexible systems, highly specialized products, industrial control, industrial robots, interactive workspace setup, modular fabrication units, modular production environment, modular production system, modularization, production engineering computing, versatile production},
	pages = {387--394},
}

@article{faccio_influence_2020,
	title = {The influence of the product characteristics on human-robot collaboration: a model for the performance of collaborative robotic assembly},
	volume = {106},
	issn = {0268-3768, 1433-3015},
	shorttitle = {The influence of the product characteristics on human-robot collaboration},
	url = {http://link.springer.com/10.1007/s00170-019-04670-6},
	doi = {10.1007/s00170-019-04670-6},
	language = {en},
	number = {5-6},
	urldate = {2021-01-12},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Faccio, Maurizio and Minto, Riccardo and Rosati, Giulio and Bottin, Matteo},
	month = jan,
	year = {2020},
	pages = {2317--2331},
}

@article{boschetti_multi-robot_2021,
	title = {Multi-robot multi-operator collaborative assembly systems: a performance evaluation model},
	issn = {0956-5515, 1572-8145},
	shorttitle = {Multi-robot multi-operator collaborative assembly systems},
	url = {http://link.springer.com/10.1007/s10845-020-01714-7},
	doi = {10.1007/s10845-020-01714-7},
	abstract = {In the last decade, collaborative assembly systems (CAS) are becoming increasingly common due to their ability to merge the ﬂexibility of a manual assembly system with the performance of traditional robotics. Technical constraints, e.g., dedicated tools or resources, or performance requirements, e.g., throughput, could encourage the use of a CAS built around a multirobot and multi-operator layout, i.e., with a number of resources greater than 2. Starting from the development of a prototype multi-robot multi-operator collaborative workcell, a simulation environment was developed to evaluate the makespan and the degree of collaboration in multi-robot multi-operator CAS. From the simulation environment, a mathematical model was conceptualized. The presented model allows estimating, with a certain degree of accuracy, the performances of the system. The results have investigated how several process characteristics, i.e. the number and type of resources, the resources layout, the task allocation method, and the number of feeding devices, inﬂuence the degree of collaboration between the resources. Lastly, the authors propose a compact analytic formulation, based on an exponential function, and deﬁne the methods and the inﬂuence factors to determine its parameters.},
	language = {en},
	urldate = {2021-03-04},
	journal = {Journal of Intelligent Manufacturing},
	author = {Boschetti, Giovanni and Bottin, Matteo and Faccio, Maurizio and Minto, Riccardo},
	month = jan,
	year = {2021},
}

@inproceedings{fletcher_cambridge_2003,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Cambridge} {Packing} {Cell} — {A} {Holonic} {Enterprise} {Demonstrator}},
	isbn = {978-3-540-45023-8},
	doi = {10.1007/3-540-45023-8_51},
	abstract = {Many modern manufacturing systems are highly automated and are now requiring decentralised ‘smart’ architectures to control hardware and manage the flow of materials / knowledge, in order to provide responsiveness. This responsiveness is needed to satisfy an ever increasing consumer need for goods that satisfy their unique requirements and are delivered to market both quickly and economically. A key route to achieve this mass-customisation with distributed control is to apply the holonic enterprise paradigm, and one manufacturing process that exhibits a high potential for responsiveness is packaging. Therefore this paper presents some of the main features of such an enterprise — the Holonic Packing Cell demonstrator being built at Cambridge University’s Institute for Manufacturing. It must be emphasised that this cell is constructed from state-of-the-art industrial strength facilities to demonstrate a spectrum of responsive manufacturing ideas — it is not built from Lego bricks.},
	language = {en},
	booktitle = {Multi-{Agent} {Systems} and {Applications} {III}},
	publisher = {Springer},
	author = {Fletcher, Martyn and McFarlane, Duncan and Lucas, Andrew and Brusey, James and Jarvis, Dennis},
	editor = {Mařík, Vladimír and Pěchouček, Michal and Müller, Jörg},
	year = {2003},
	keywords = {Batch Order, Mass Customisation, Plan Library, Simple Object Access Protocol Message, Storage Unit},
	pages = {533--543},
}

@article{zhang_big_2018,
	title = {A big data driven analytical framework for energy-intensive manufacturing industries},
	volume = {197},
	journal = {Journal of Cleaner Production},
	author = {Zhang, Yingfeng and Ma, Shuaiyin and Yang, Haidong and Lv, Jingxiang and Liu, Yang},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {57--72},
}

@article{xu_energy_2017,
	title = {Energy condition perception and {Big} {Data} analysis for industrial cloud robotics},
	volume = {61},
	journal = {Procedia CIRP},
	author = {Xu, Wei and Liu, Quan and Xu, Wenjun and Zhou, Zude and Pham, Duc Truong and Lou, Ping and Ai, Qingsong and Zhang, Xiaomei and Hu, Jiwei},
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {370--375},
}

@article{tous_combined_2015,
	title = {Combined heat and power production planning in a waste-to-energy plant on a short-term basis},
	volume = {90},
	journal = {Energy},
	author = {Touš, Michal and Pavlas, Martin and Putna, Ondřej and Stehlík, Petr and Crha, Lukáš},
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {137--147},
}

@article{teng_recent_2021,
	title = {Recent advances on industrial data-driven energy savings: {Digital} twins and infrastructures},
	volume = {135},
	shorttitle = {Recent advances on industrial data-driven energy savings},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Teng, Sin Yong and Touš, Michal and Leong, Wei Dong and How, Bing Shen and Lam, Hon Loong and Máša, Vítězslav},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {110208},
}

@article{teng_principal_2019,
	title = {Principal component analysis-aided statistical process optimisation ({PASPO}) for process improvement in industrial refineries},
	volume = {225},
	journal = {Journal of Cleaner Production},
	author = {Teng, Sin Yong and How, Bing Shen and Leong, Wei Dong and Teoh, Jun Hao and Cheah, Adrian Chee Siang and Motavasel, Zahra and Lam, Hon Loong},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {359--375},
}

@article{tabaa_green_2020,
	title = {Green {Industrial} {Internet} of {Things} from a smart industry perspectives},
	volume = {6},
	journal = {Energy Reports},
	author = {Tabaa, Mohamed and Monteiro, Fabrice and Bensag, Hassna and Dandache, Abbas},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {430--446},
}

@article{serin_integrated_2020,
	title = {Integrated energy-efficient machining of rotary impellers and multi-objective optimization},
	volume = {35},
	number = {4},
	journal = {Materials and Manufacturing Processes},
	author = {Serin, Gokberk and Ozbayoglu, Murat and Unver, Hakki Ozgur},
	year = {2020},
	note = {Publisher: Taylor \& Francis},
	pages = {478--490},
}

@article{rentsch_artificial_2015,
	title = {Artificial intelligence for an energy and resource efficient manufacturing chain design and operation},
	volume = {33},
	journal = {Procedia CIRP},
	author = {Rentsch, Rüdiger and Heinzel, Carsten and Brinksmeier, E.},
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {139--144},
}

@article{nishant_artificial_2020,
	title = {Artificial intelligence for sustainability: {Challenges}, opportunities, and a research agenda},
	volume = {53},
	shorttitle = {Artificial intelligence for sustainability},
	journal = {International Journal of Information Management},
	author = {Nishant, Rohit and Kennedy, Mike and Corbett, Jacqueline},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {102104},
}

@article{miriyala_transform-ann_2018,
	title = {{TRANSFORM}-{ANN} for online optimization of complex industrial processes: {Casting} process as case study},
	volume = {264},
	shorttitle = {{TRANSFORM}-{ANN} for online optimization of complex industrial processes},
	number = {1},
	journal = {European Journal of Operational Research},
	author = {Miriyala, Srinivas Soumitri and Subramanian, Venkat R. and Mitra, Kishalay},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {294--309},
}

@article{mao_opportunities_2019,
	title = {Opportunities and challenges of artificial intelligence for green manufacturing in the process industry},
	volume = {5},
	number = {6},
	journal = {Engineering},
	author = {Mao, Shuai and Wang, Bing and Tang, Yang and Qian, Feng},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {995--1002},
}

@article{le_energy_2013,
	title = {An energy data-driven decision support system for high-performance manufacturing industries},
	volume = {1},
	number = {1},
	journal = {International Journal of Automation and Logistics},
	author = {Le, Cao Vinh and Pang, Chee Khiang},
	year = {2013},
	note = {Publisher: Inderscience Publishers Ltd},
	pages = {61--79},
}

@article{katchasuwanmanee_development_2016,
	title = {Development of the {Energy}-smart {Production} {Management} system (e-{ProMan}): {A} {Big} {Data} driven approach, analysis and optimisation},
	volume = {230},
	shorttitle = {Development of the {Energy}-smart {Production} {Management} system (e-{ProMan})},
	number = {5},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture},
	author = {Katchasuwanmanee, Kanet and Bateman, Richard and Cheng, Kai},
	year = {2016},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {972--978},
}

@article{ji_big_2019,
	title = {A big data analytics based machining optimisation approach},
	volume = {30},
	number = {3},
	journal = {Journal of Intelligent Manufacturing},
	author = {Ji, Wei and Yin, Shubin and Wang, Lihui},
	year = {2019},
	note = {Publisher: Springer},
	pages = {1483--1495},
}

@article{gao_machine_2014,
	title = {Machine learning applications for data center optimization},
	author = {Gao, Jim},
	year = {2014},
}

@article{gallagher_intellimav_2019,
	title = {{IntelliMaV}: {A} cloud computing measurement and verification 2.0 application for automated, near real-time energy savings quantification and performance deviation detection},
	volume = {185},
	shorttitle = {{IntelliMaV}},
	journal = {Energy and Buildings},
	author = {Gallagher, Colm V. and Leahy, Kevin and O’Donovan, Peter and Bruton, Ken and O’Sullivan, Dominic TJ},
	year = {2019},
	note = {Publisher: Elsevier},
	pages = {26--38},
}

@article{durrani_artificial_2018,
	title = {An artificial intelligence method for energy efficient operation of crude distillation units under uncertain feed composition},
	volume = {11},
	number = {11},
	journal = {Energies},
	author = {Durrani, Muhammad Amin and Ahmad, Iftikhar and Kano, Manabu and Hasebe, Shinji},
	year = {2018},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {2993},
}

@article{botticella_multi-criteria_2018,
	title = {Multi-criteria (thermodynamic, economic and environmental) analysis of possible design options for residential heating split systems working with low {GWP} refrigerants},
	volume = {87},
	journal = {International Journal of Refrigeration},
	author = {Botticella, F. and De Rossi, F. and Mauro, A. W. and Vanoli, G. P. and Viscito, L.},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {131--153},
}

@article{ascione_real_2020,
	title = {A real industrial building: {Modeling}, calibration and {Pareto} optimization of energy retrofit},
	volume = {29},
	shorttitle = {A real industrial building},
	journal = {Journal of Building Engineering},
	author = {Ascione, Fabrizio and Bianco, Nicola and Iovane, Teresa and Mauro, Gerardo Maria and Napolitano, Davide Ferdinando and Ruggiano, Antonio and Viscido, Lucio},
	year = {2020},
	note = {Publisher: Elsevier},
	pages = {101186},
}

@article{ascione_casa_2017,
	title = {{CASA}, cost-optimal analysis by multi-objective optimisation and artificial neural networks: {A} new framework for the robust assessment of cost-optimal energy retrofit, feasible for any building},
	volume = {146},
	shorttitle = {{CASA}, cost-optimal analysis by multi-objective optimisation and artificial neural networks},
	journal = {Energy and Buildings},
	author = {Ascione, Fabrizio and Bianco, Nicola and De Stasio, Claudio and Mauro, Gerardo Maria and Vanoli, Giuseppe Peter},
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {200--219},
}

@inproceedings{ak_data_2015,
	title = {Data analytics and uncertainty quantification for energy prediction in manufacturing},
	booktitle = {2015 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Ak, Ronay and Bhinge, Raunak},
	year = {2015},
	pages = {2782--2784},
}

@article{ahmad_artificial_2021,
	title = {Artificial {Intelligence} in {Sustainable} {Energy} {Industry}: {Status} {Quo}, {Challenges} and {Opportunities}},
	shorttitle = {Artificial {Intelligence} in {Sustainable} {Energy} {Industry}},
	journal = {Journal of Cleaner Production},
	author = {Ahmad, Tanveer and Zhang, Dongdong and Huang, Chao and Zhang, Hongcai and Dai, Ningyi and Song, Yonghua and Chen, Huanxin},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {125834},
}

@book{haykin_neural_2009,
	address = {New York},
	edition = {3rd ed},
	title = {Neural networks and learning machines},
	isbn = {978-0-13-147139-9},
	language = {en},
	publisher = {Prentice Hall},
	author = {Haykin, Simon S.},
	year = {2009},
	note = {OCLC: ocn237325326},
	keywords = {Adaptive filters, Neural networks (Computer science)},
}

@article{schmidhuber_deep_2015,
	title = {Deep learning in neural networks: {An} overview},
	volume = {61},
	issn = {0893-6080},
	shorttitle = {Deep learning in neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
	doi = {10.1016/j.neunet.2014.09.003},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	language = {en},
	urldate = {2021-02-26},
	journal = {Neural Networks},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {2015},
	keywords = {Deep learning, Evolutionary computation, Reinforcement learning, Supervised learning, Unsupervised learning},
	pages = {85--117},
}

@article{meng_machine_2020,
	title = {Machine {Learning} in {Additive} {Manufacturing}: {A} {Review}},
	volume = {72},
	issn = {1543-1851},
	shorttitle = {Machine {Learning} in {Additive} {Manufacturing}},
	url = {https://doi.org/10.1007/s11837-020-04155-y},
	doi = {10.1007/s11837-020-04155-y},
	abstract = {In this review article, the latest applications of machine learning (ML) in the additive manufacturing (AM) field are reviewed.These applications, such as parameter optimization and anomaly detection, are classified into different types of ML tasks, including regression, classification, and clustering. The performance of various ML algorithms in these types of AM tasks are compared and evaluated.Finally, several future research directions are suggested.},
	language = {en},
	number = {6},
	urldate = {2021-02-26},
	journal = {JOM},
	author = {Meng, Lingbin and McWilliams, Brandon and Jarosinski, William and Park, Hye-Yeong and Jung, Yeon-Gil and Lee, Jehyun and Zhang, Jing},
	month = jun,
	year = {2020},
	pages = {2363--2377},
}

@article{abdufattokhov_gaussian_2020,
	title = {Gaussian {Processes} {Regression} based {Energy} {System} {Identification} of {Manufacturing} {Process} for {Model} {Predictive} {Control}},
	volume = {8},
	issn = {23473983},
	url = {http://www.warse.org/IJETER/static/pdf/file/ijeter06892020.pdf},
	doi = {10.30534/ijeter/2020/06892020},
	abstract = {To overcome environmental impacts of a manufacturing factory over its life cycle, the role of sustainable energy effectiveness is vital. For this reason, implementing energy conservation technologies to empower energy efficiency has become an important business for majority of manufacturing plants. Data driven control set ups seem to be a novel idea to handle energy efficiency of such complex systems, while machine learning is becoming well-known in system engineering community. In this paper, a new approach together with optimal control application is considered to open promising energy saving ideas through investigating machines of a factory using machine learning, specifically, Gaussian Processes Regression (GPR), where the model is built by correlating the dynamics, complexity, and interrelated energy consumption recordings. We connect the idea with controlling of a manufacturing system energy in optimized way, where Model Predictive Control loop delivers optimal solutions for each control time step. In the end, numerical example is demonstrated to give a clear picture of the proposed modeling method potentials.},
	language = {en},
	number = {9},
	urldate = {2021-02-12},
	journal = {International Journal of Emerging Trends in Engineering Research},
	author = {Abdufattokhov, Shokhjakhon and Ibragimova, Kamila and Gulyamova, Dilfuza and Tulaganov, Komiljon},
	month = sep,
	year = {2020},
	keywords = {Gaussian processes regression (GPR)},
	pages = {4927--4932},
}

@article{harashima_mechatronics_1996,
	title = {Mechatronics - "{What} {Is} {It}, {Why}, and {How}?" {An} editorial},
	volume = {1},
	issn = {1941-014X},
	shorttitle = {Mechatronics - "{What} {Is} {It}, {Why}, and {How}?},
	doi = {10.1109/TMECH.1996.7827930},
	abstract = {It is our great pleasure to welcome you to the new publication of the IEEE/ASME TRANSACTIONS ON MECHATRONICS. This TRANSACTIONS is published quarterly from this premiere issue in 1996 as a joint publication of the IEEE Industrial Electronics Society, the IEEE Robotics and Automation Society, and the ASME Dynamic Systems and Control Division, aiming at establishing a high-quality archival journal which represents stme of the art of mechatronics, its recent advances, and practical applications. Many of you are quite familiar with the basic concepts and commercial products utilizing the control of mechanical systems in conjunction with modern electronics technology. The word "Mechatronics" is a new word for the blending of mechanics and electronics invented in Japan at the end of the 1960's. Yaskawa Electric Company once commercially registered the word "Mechatronics," which is now freely used by anyone [1]. In this editorial, we look back at the history of mechatronics, project it into the future, and state the scope and direction of this new journal.},
	number = {1},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Harashima, F. and Tomizuka, M. and Fukuda, T.},
	month = mar,
	year = {1996},
	note = {Conference Name: IEEE/ASME Transactions on Mechatronics},
	pages = {1--4},
}

@article{goos_computer_2021,
	series = {8th {CIRP} {Conference} of {Assembly} {Technology} and {Systems}},
	title = {Computer assisted ergonomic assembly cell design},
	volume = {97},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S221282712031427X},
	doi = {10.1016/j.procir.2020.05.208},
	abstract = {When designing or modifying an assembly cell, the question at hand is "What will it cost, and what will we gain"? This research features the evaluation of assembly cell variants, taking operator timing and ergonomics into account. For each assembly step requiring an operator e.g. using a screwdriver, the ergonomic score is evaluated by considering the (automatically generated) poses of the operator at the start and finish of each assembly action. Furthermore, throughput is estimated by applying Methods-Time Measurement (MTM) to an inverted kinematics model of a virtual operator. We present our findings on an industrial use case: the assembly of a 15 kg compressor. Starting from an existing cobot cell, alternative assembly cells, with an adjusted resource allocation and a modified cell layout, are evaluated. Besides the performance metrics, human design factors, that are hard to model mathematically, can be incorporated in the design by visualizing and experiencing the work cell in virtual reality. This allows to further fine-tune the cell design.},
	language = {en},
	urldate = {2021-02-15},
	journal = {Procedia CIRP},
	author = {Goos, Jan and Lietaert, Pieter and Cools, Robbe},
	month = jan,
	year = {2021},
	keywords = {Human operator support \& ergonomics in assembly, Methods-Time measurement (MTM), Performance assessment of assembly systems},
	pages = {87--91},
}

@article{gualtieri_design_2020,
	title = {Design of {Human}-{Centered} {Collaborative} {Assembly} {Workstations} for the {Improvement} of {Operators}’ {Physical} {Ergonomics} and {Production} {Efficiency}: {A} {Case} {Study}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Design of {Human}-{Centered} {Collaborative} {Assembly} {Workstations} for the {Improvement} of {Operators}’ {Physical} {Ergonomics} and {Production} {Efficiency}},
	url = {https://www.mdpi.com/2071-1050/12/9/3606},
	doi = {10.3390/su12093606},
	abstract = {Industrial collaborative robotics is one of the main enabling technologies of Industry 4.0. Collaborative robots are innovative cyber-physical systems, which allow safe and efficient physical interactions with operators by combining typical machine strengths with inimitable human skills. One of the main uses of collaborative robots will be the support of humans in the most physically stressful activities through a reduction of work-related biomechanical overload, especially in manual assembly activities. The improvement of operators\&rsquo; occupational work conditions and the development of human-centered and ergonomic production systems is one of the key points of the ongoing fourth industrial revolution. The factory of the future should focus on the implementation of adaptable, reconfigurable, and sustainable production systems, which consider the human as their core and valuable part. Strengthening actual assembly workstations by integrating smart automation solutions for the enhancement of operators\&rsquo; occupational health and safety will be one of the main goals of the near future. In this paper, the transformation of a manual workstation for wire harness assembly into a collaborative and human-centered one is presented. The purpose of the work is to present a case study research for the design of a collaborative workstation to improve the operators\&rsquo; physical ergonomics while keeping or increasing the level of productivity. Results demonstrate that the achieved solution provides valuable benefits for the operators\&rsquo; working conditions as well as for the production performance of the companies. In particular, the biomechanical overload of the worker has been reduced by 12.0\% for the right part and by 28\% for the left part in terms of manual handling, and by 50\% for the left part and by 57\% for the right part in terms of working postures. In addition, a reduction of the cycle time of 12.3\% has been achieved.},
	language = {en},
	number = {9},
	urldate = {2021-02-15},
	journal = {Sustainability},
	author = {Gualtieri, Luca and Palomba, Ilaria and Merati, Fabio Antonio and Rauch, Erwin and Vidoni, Renato},
	month = jan,
	year = {2020},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {SME, assembly, collaborative robotics, human-centered design, human-robot collaboration, industry 4.0, physical ergonomics, small and medium sized enterprise},
	pages = {3606},
}

@article{shin_holonic-based_2019,
	title = {A {Holonic}-{Based} {Self}-{Learning} {Mechanism} for {Energy}-{Predictive} {Planning} in {Machining} {Processes}},
	volume = {7},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2227-9717/7/10/739},
	doi = {10.3390/pr7100739},
	abstract = {The present work proposes a holonic-based mechanism for self-learning factories based on a hybrid learning approach. The self-learning factory is a manufacturing system that gains predictive capability by machine self-learning, and thus automatically anticipates the performance results during the process planning phase through learning from past experience. The system mechanism, including a modeling method, architecture, and operational procedure, is structured to agentize machines and manufacturing objects under the paradigm of Holonic Manufacturing Systems. This mechanism allows machines and manufacturing objects to acquire their data and model interconnection and to perform model-driven autonomous and collaborative behaviors. The hybrid learning approach is designed to obtain predictive modeling ability in both data-existent and even data-absent environments via accommodating machine learning (which extracts knowledge from data) and transfer learning (which extracts knowledge from existing knowledge). The present work also implements a prototype system to demonstrate automatic predictive modeling and autonomous process planning for energy reduction in milling processes. The prototype generates energy-predictive models via hybrid learning and seeks the minimum energy-using machine tool through the contract net protocol combined with energy prediction. As a result, the prototype could achieve a reduction of 9.70\% with respect to energy consumption as compared with the maximum energy-using machine tool.},
	language = {en},
	number = {10},
	urldate = {2021-02-12},
	journal = {Processes},
	author = {Shin, Seung-Jun and Kim, Young-Min and Meilanitasari, Prita},
	month = oct,
	year = {2019},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {cyber-physical production systems, holonic manufacturing systems, machine learning, predictive analytics, self-learning factory, transfer learning},
	pages = {739},
}

@article{morariu_machine_2020,
	title = {Machine learning for predictive scheduling and resource allocation in large scale manufacturing systems},
	volume = {120},
	issn = {01663615},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166361519311595},
	doi = {10.1016/j.compind.2020.103244},
	abstract = {The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data.},
	language = {en},
	urldate = {2021-02-12},
	journal = {Computers in Industry},
	author = {Morariu, Cristina and Morariu, Octavian and Răileanu, Silviu and Borangiu, Theodor},
	month = sep,
	year = {2020},
	keywords = {Predictive production planning},
	pages = {103244},
}

@article{schwung_actor-critic_2019,
	title = {Actor-critic reinforcement learning for energy optimization in hybrid production environments},
	volume = {18},
	abstract = {This paper presents a centralized approach for energy optimization in large scale industrial production systems based on an actor-critic reinforcement learning (ACRL) framework. The objective of the on-line capable self-learning algorithm is the optimization of the energy consumption of a production process while meeting certain manufacturing constraints like a demanded throughput. Our centralized ACRL algorithm works with two artificial neural networks (ANN) for function approximation using Gaussian radial-basis functions (RBF), one for the critic and another for the actor, respectively. This kind of actorcritic design enables the handling of both, a discrete and continuous state and action space, which is essential for hybrid systems where discrete and continuous actuator behavior is combined. The ACRL algorithm is exemplary validated on a dynamic simulation model of a bulk good system for the task of supplying bulk good to a subsequent dosing section while consuming as low energy as possible. The simulation results clearly show the applicability and capability of our machine learning (ML) approach for energy optimization in hybrid production environments. © Research Institute for Intelligent Computer Systems, 2019. All rights reserved.},
	number = {4},
	journal = {International Journal of Computing},
	author = {Schwung, D. and Schwung, A. and Ding, S.X.},
	year = {2019},
	keywords = {Actor-critic reinforcement learning, Energy optimization, Gaussian radial-basis functions (RBF), Hybrid systems, Machine learning, Manufacturing systems, Radial-basis function neural networks, Reinforcement learning, Self-learning},
	pages = {360--371},
}

@article{tsao_energy-efficient_2020,
	title = {Energy-efficient single-machine scheduling problem with controllable job processing times under differential electricity pricing},
	volume = {161},
	issn = {0921-3449},
	url = {https://www.sciencedirect.com/science/article/pii/S0921344920302202},
	doi = {10.1016/j.resconrec.2020.104902},
	abstract = {The framework of Industry 3.5 has stimulated the development of smart manufacturing systems that efficiently utilize available resources, including energy; this has led to disruptive innovations in the paradigm of manufacturing systems. The key aim of daily planning and scheduling systems is to maximize productivity by optimizing resource allocation in an organization. This study addresses the energy-efficient single-machine scheduling problem in which the job processing time depends on the quantity of allocated resources and the resource allocation cost is uncertain due to differential electricity pricing. The described problem must respond to the three prominent aspects of Industry 3.5: total resource management, digital decision-making, and smart manufacturing. To meet the total resource management and smart manufacturing requirements, a mixed-integer programming model is developed to determine the operation status of the single machine (i.e., “on” or “idle”) with variable electricity costs, assigned job processing times for time periods, and resource allocation quantities to reduce job processing times. The objective is to minimize the total energy consumption cost by considering both financial (i.e., budget for resource allocation) and environmental constraints (i.e., carbon footprint threshold). This study proposes a type-2 fuzzy control approach integrated with a genetic algorithm (GA), which could be implemented in a decision-support system, thus responding to the digital decision-making requirements of Industry 3.5. The GA is utilized to re-optimize the output of the type-2 fuzzy controller to provide an effective scheduling solution. Our experimental results indicate a 4.20\% reduction in the total energy consumption cost compared to the GA approach without controllable processing times. The proposed algorithm can efficiently address the studied problem on a large scale (approximately 200 jobs), where the average computational time required is less than 1 h.},
	language = {en},
	urldate = {2021-02-12},
	journal = {Resources, Conservation and Recycling},
	author = {Tsao, Yu-Chung and Thanh, Vo-Van and Hwang, Feng-Jang},
	month = oct,
	year = {2020},
	keywords = {Scheduling problem, differential electricity pricing, energy efficient, genetic algorithm, industry 3.5, type -2 fuzzy control},
	pages = {104902},
}

@inproceedings{morariu_distributed_2019,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {A {Distributed} {Approach} for {Machine} {Learning} in {Large} {Scale} {Manufacturing} {Systems}},
	isbn = {978-3-030-03003-2},
	doi = {10.1007/978-3-030-03003-2_3},
	abstract = {Large scale manufacturing systems are capable to execute manufacturing operations across multiple product batches by coordinating many shop floor actors. Monitoring and processing in real time the information flow from these systems becomes an essential part in optimizing and detecting faults that might affect the production schedule. This paper proposes an architecture that uses big data concepts and map-reduce algorithms to process the information streams in large scale manufacturing systems, focusing on energy consumptions aggregated at various layers. Once the information is aggregated in logical streams and consolidated based on relevant metadata, a neural network is trained and used to learn historical patterns in data on each layer. This novel approach also allows accurate forecasting of the energy consumption patterns during the production cycle by using Long Short Term Memory neural networks. The paper presents a practical example on how map-reduce algorithms can be implemented and how repetitive patterns in energy consumption can be learned.},
	language = {en},
	booktitle = {Service {Orientation} in {Holonic} and {Multi}-{Agent} {Manufacturing}},
	publisher = {Springer International Publishing},
	author = {Morariu, Cristina and Răileanu, Silviu and Borangiu, Theodor and Anton, Florin},
	editor = {Borangiu, Theodor and Trentesaux, Damien and Thomas, André and Cavalieri, Sergio},
	year = {2019},
	keywords = {Big data, Energy consumption, Forecasting, LSTM, Machine learning, Neural networks},
	pages = {41--52},
}

@article{grasler_implementation_2017,
	series = {Manufacturing {Systems} 4.0 – {Proceedings} of the 50th {CIRP} {Conference} on {Manufacturing} {Systems}},
	title = {Implementation of an {Adapted} {Holonic} {Production} {Architecture}},
	volume = {63},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S221282711730358X},
	doi = {10.1016/j.procir.2017.03.176},
	abstract = {Recent developments in information and computation technologies open up possibilities for the practical implementation of flexible, self-controlling production systems. The decentralization of existing production systems and their control plays a decisive role in creating the demanded flexibility and achieving an overall self-controlled system. The basic concept of the decentralization of production systems was set by the paradigm of Holonic Manufacturing Systems (HMS). In a HMS an element (holon) of the production system works autonomously with its own schedule and properties. Just through the cooperation with other holons central tasks of the production system are determined and subsequently executed. In this paper the flexibility and self-control of the production system was applicated through the distribution of decision-making by adding supplementary information acquisition and processing tools to former executive units. The previously procedural connection of the machines was dissolved and replaced by a peer-to-peer communication protocol. Superordinate controlling units, mostly PLCs, were abolished and instead decentralized controlling agents were implemented. The automatic control of the entire system is reached just through the communication of all devices, a machine-based scheduling and additional monitoring agents. The underlying architecture is based on Holonic Manufacturing concepts including order agent, machine, resource, product, logistic and supply holons. In this paper the adapted architecture is presented and subsequently the practical implementation in a research laboratory is described.},
	language = {en},
	urldate = {2021-02-10},
	journal = {Procedia CIRP},
	author = {Gräßler, Iris and Pöhler, Alexander},
	month = jan,
	year = {2017},
	keywords = {Holonic manufacturing system, Industry 4.0 implementation, Production system decentralization},
	pages = {138--143},
}

@inproceedings{macherki_q-holon_2020,
	title = {The {Q}-{Holon}: a quadridimensional holon to design and operate an adaptive and scalable architecture for {CPPS}},
	shorttitle = {The {Q}-{Holon}},
	doi = {10.1109/REM49740.2020.9313895},
	abstract = {Production systems environment may undergo different changes: changes in the economic and political context, variations in demand or supply, equipment failure or unavailability, etc. These changes affect production plans, scheduling and configuration of production systems to varying degrees. It is therefore necessary to readapt the production plan, the scheduling and/or to reconfigure the production system to cope with new demands and constraints. Scientific and technical advances have enabled the creation of a new class of production system: CPPS (Cyber Physical Production Systems). As regards the control architecture, several new paradigms implementing the oligarchic control architecture have emerged. The Holonic Manufacturing Systems (HMS) paradigm discussed in this research work is one example of these paradigms. In this research work, we are interested in the basic unit of holonic architectures: the Holon. We propose an enriched Holon, the “Q-Holon”, with four dimensions (physical, cyber, human and energetic). The Q-Holon is generic enough to adapt to all situations and allows the representation of the elementary constituents as well as the grouping of some or all of a production system's constituents.},
	booktitle = {2020 21st {International} {Conference} on {Research} and {Education} in {Mechatronics} ({REM})},
	author = {Macherki, D. and Diallo, T. M. L. and Choley, J.-Y. and Guizani, A. and Barkallah, M. and Haddar, M.},
	month = dec,
	year = {2020},
	keywords = {Adaptive architecture, Architecture, CPPS, Computer architecture, Holon, Holonic Manufacturing Systems, Industry 4.0, Job shop scheduling, Laboratories, Manufacturing, Manufacturing systems, Task analysis, reconfigurable manufacturing system},
	pages = {1--6},
}

@article{cardin_evolution_2018,
	series = {16th {IFAC} {Symposium} on {Information} {Control} {Problems} in {Manufacturing} {INCOM} 2018},
	title = {Evolution of holonic control architectures towards {Industry} 4.0: {A} short overview},
	volume = {51},
	issn = {2405-8963},
	shorttitle = {Evolution of holonic control architectures towards {Industry} 4.0},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896318315465},
	doi = {10.1016/j.ifacol.2018.08.420},
	abstract = {The flexibility claimed by the next generation production systems induces a deep modification of the behavior and the core itself of the control systems. Overconnectivity and data management abilities targeted by Industry 4.0 paradigm enable the emergence of more flexible and reactive control systems, based on the cooperation of autonomous and connected entities in the decision making process. For the last 20 years, holonic paradigm has become the core paradigm of those evolutions, and evolved in itself. This contribution aims at emphasizing the conceptual evolutions in the application of holonic paradigm in the control architectures of manufacturing systems and highlighting the current research trends in this field.},
	language = {en},
	number = {11},
	urldate = {2021-02-10},
	journal = {IFAC-PapersOnLine},
	author = {Cardin, Olivier and Derigent, William and Trentesaux, Damien},
	month = jan,
	year = {2018},
	keywords = {Control architecture, Holonic manufacturing systems, Industrial control, Industry 4.0, Manufacturing systems},
	pages = {1243--1248},
}

@article{van_brussel_reference_1998,
	title = {Reference architecture for holonic manufacturing systems: {PROSA}},
	volume = {37},
	issn = {0166-3615},
	shorttitle = {Reference architecture for holonic manufacturing systems},
	url = {https://www.sciencedirect.com/science/article/pii/S016636159800102X},
	doi = {10.1016/S0166-3615(98)00102-X},
	abstract = {Future manufacturing systems need to cope with frequent changes and disturbances. As such, their control requires constant adaptation and high flexibility. Holonic manufacturing is a highly distributed control paradigm that promises to handle these problems successfully. It is based on the concept of autonomous co-operating agents, called `holons'. This paper gives an overview of the holonic reference architecture for manufacturing systems as developed at PMA-KULeuven. This architecture, called PROSA, consists of three types of basic holons: order holons, product holons, and resource holons. They are structured using the object-oriented concepts of aggregation and specialisation. Staff holons can be added to assist the basic holons with expert knowledge. The resulting architecture has a high degree of self-similarity, which reduces the complexity to integrate new components and enables easy reconfiguration of the system. PROSA shows to cover aspects of both hierarchical as well as heterarchical control approaches. As such, it can be regarded as a generalisation of the two former approaches. More importantly, PROSA introduces significant innovations: the system structure is decoupled from the control algorithm, logistical aspects can be decoupled from technical ones, and PROSA opens opportunities to achieve more advanced hybrid control algorithms.},
	language = {en},
	number = {3},
	urldate = {2021-02-09},
	journal = {Computers in Industry},
	author = {Van Brussel, Hendrik and Wyns, Jo and Valckenaers, Paul and Bongaerts, Luc and Peeters, Patrick},
	month = nov,
	year = {1998},
	keywords = {CIM, Holonic manufacturing system, IMS, Reference architecture, Shop floor control},
	pages = {255--274},
}

@book{mella_holonic_2009,
	address = {Pavia},
	title = {The holonic revolution: holons, holarchies and holonic networks : the ghost in the production machine},
	isbn = {978-88-96764-00-8},
	shorttitle = {The holonic revolution},
	language = {en},
	publisher = {University Press},
	author = {Mella, Piero},
	year = {2009},
	note = {OCLC: 955269454},
}

@inproceedings{teiwes_identifying_2016,
	title = {Identifying the potential of human-robot collaboration in automotive assembly lines using a standardised work description},
	doi = {10.1109/IConAC.2016.7604898},
	abstract = {The increased availability of sensitive and compliant lightweight robots for use in assembly lines collaborating with the human promises significant improvements of different socio-technical aspects of work. Workplaces can be reorganized to assign monotonous or unergonomic tasks to the robot. Also unproductive jobs currently done by the human can be minimized by an improved work distribution. Since there is only little experience with the new generation of collaborating robots, the implementation of workplaces shared by human and robot is often influenced by subjective perspectives. In this paper, an approach to assess the collaboration potential of workplaces is presented. Based on existing standardised work descriptions, the suitability for human-robot collaboration can be derived and therefore a more objective evaluation and comparison of the whole assembly can be achieved.},
	booktitle = {2016 22nd {International} {Conference} on {Automation} and {Computing} ({ICAC})},
	author = {Teiwes, J. and Bänziger, T. and Kunz, A. and Wegener, K.},
	month = sep,
	year = {2016},
	keywords = {Assembly, Automation, Automation Potential, Collaboration, Collision avoidance, Employment, Future Work, Human Robot Interaction, Mass Customization, Mass Production, Methods Time Measurement (MTM), Robot sensing systems, Time measurement, assembling, automobile industry, automotive assembly lines, human-robot collaboration, human-robot interaction, industrial robots, lightweight robots, socio-technical aspects, standardised work description},
	pages = {78--83},
}

@inproceedings{vitolo_multi-layer_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Mechanical} {Engineering}},
	title = {A {Multi}-layer {Approach} for the {Identification} and {Evaluation} of {Collaborative} {Robotic} {Workplaces} {Within} {Industrial} {Production} {Plants}},
	isbn = {978-3-030-31154-4},
	doi = {10.1007/978-3-030-31154-4_61},
	abstract = {Collaborative robotic solutions, where humans and robots share a common workspace performing tasks concurrently without physical safety barriers dividing them, are entering the 4.0 manufacturing market. Some proven and tested use cases of Human-Robot Collaboration have been implemented, but their identification process is often just based on the intuition of planning engineers. The purpose of this work is to propose a systematic approach for the identification of potential collaborative workstations within an industrial production plant. In order to do this a multi-layer modelling approach was used and enriched. The multi-layer approach defines the overall goal of the industrial process, the sub-processes that made it possible, the activity models that enables a flow of activities and, finally, a set of methods to carry out the activities. A morphological box of methods that can be used to achieve the specific goal of identifying suitable collaborative workplaces in an industrial plant, through a process of HRC potential analysis is, therefore, ready to be deeply investigated and used.},
	language = {en},
	booktitle = {Design {Tools} and {Methods} in {Industrial} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Vitolo, Ferdinando and Pasquariello, Agnese and Patalano, Stanislao and Gerbino, Salvatore},
	editor = {Rizzi, Caterina and Andrisano, Angelo Oreste and Leali, Francesco and Gherardini, Francesco and Pini, Fabio and Vergnano, Alberto},
	year = {2020},
	keywords = {Collaborative workplaces, Evaluation criteria, Multi-layer approach},
	pages = {719--730},
}

@misc{knut_meta-modeling_nodate,
	title = {Meta-{Modeling} and {Modeling} {Languages}},
	language = {en},
	author = {Knut, Hinkelmann},
}

@incollection{goos_metamodelling_2002,
	address = {Berlin, Heidelberg},
	title = {Metamodelling {Platforms}},
	volume = {2455},
	isbn = {978-3-540-44137-3 978-3-540-45705-3},
	url = {http://link.springer.com/10.1007/3-540-45705-4_19},
	abstract = {The state-of-the-art in the area of modelling of organisations is based on fixed metamodels. Due to rapid changing business requirements the complexity in developing applications which deliver business solutions is continually growing. To manage this complexity, environments providing flexible metamodelling capabilities instead of fixed metamodels has shown to be helpful. The main characteristic of such environments is that the formalism of modelling - the metamodel - can be freely defined and therefore be adapted to the problem under consideration. This paper gives an introduction into metamodelling concepts and presents a generic architecture for metamodelling platforms. Three best practice examples from industry projects applying metamodelling concepts in the area of business process modelling for e-business, e-learning, and knowledge management are presented. Finally, an outlook to future developments and research directions in the area of metamodelling is given.},
	language = {en},
	urldate = {2021-01-29},
	booktitle = {E-{Commerce} and {Web} {Technologies}},
	publisher = {Springer Berlin Heidelberg},
	author = {Karagiannis, Dimitris and Kühn, Harald},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Bauknecht, Kurt and Tjoa, A Min and Quirchmayr, Gerald},
	year = {2002},
	doi = {10.1007/3-540-45705-4_19},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {182--182},
}

@article{simpson_metamodels_2001,
	title = {Metamodels for computer-based engineering design: survey and recommendations},
	volume = {17},
	shorttitle = {Metamodels for computer-based engineering design},
	abstract = {The use of statistical techniques to build approximations of expensive computer analysis codes pervades much of today’s engineering design. These statistical approximations, or metamodels, are used to replace the actual expensive computer analyses, facilitating multidisciplinary, multiobjective optimization and concept exploration. In this paper, we review several of these techniques, including design of experiments, response surface methodology, Taguchi methods, neural networks, inductive learning and kriging. We survey their existing application in engineering design, and then address the dangers of applying traditional statistical techniques to approximate deterministic computer analysis codes. We conclude with recommendations for the appropriate use of statistical approximation techniques in given situations, and how common pitfalls can be avoided.},
	number = {2},
	journal = {Engineering with computers},
	author = {Simpson, Timothy W. and Poplinski, J. D. and Koch, Patrick N. and Allen, Janet K.},
	year = {2001},
	note = {Publisher: Springer},
	pages = {129--150},
}

@article{bork_survey_2020,
	title = {A survey of modeling language specification techniques},
	volume = {87},
	issn = {0306-4379},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437919303035},
	doi = {10.1016/j.is.2019.101425},
	abstract = {Visual modeling languages such as the Business Process Model and Notation and the Unified Modeling Language are widely used in industry and academia for the analysis and design of information systems. Such modeling languages are usually introduced in overarching specifications which are maintained by standardization institutions such as the Object Management Group or the Open Group. Being the primary – often the single – source of information, such specifications are of paramount importance for modelers, researchers, and tool vendors. However, structure, content, and specification techniques of such documents have never been systematically analyzed. This paper addresses this gap by reporting on a Systematic Literature Review aimed to analyze published standard modeling language specifications. In total, eleven specifications were found and comprehensively analyzed. The survey reveals heterogeneity in: (i) the modeling language concepts being specified, and (ii) the techniques being employed for the specification of these concepts. The identified specification techniques are analyzed and presented by referring to their utilization in the specifications. This survey provides a foundation for research aiming to increase consistency and improve comprehensiveness of information systems modeling languages.},
	language = {en},
	urldate = {2020-12-28},
	journal = {Information Systems},
	author = {Bork, Dominik and Karagiannis, Dimitris and Pittl, Benedikt},
	month = jan,
	year = {2020},
	keywords = {Evaluation, Metamodel, Modeling language, Specification, Standards},
	pages = {101425},
}

@article{chen_operations_2016,
	title = {Operations of a shared, autonomous, electric vehicle fleet: {Implications} of vehicle \& charging infrastructure decisions},
	volume = {94},
	issn = {0965-8564},
	shorttitle = {Operations of a shared, autonomous, electric vehicle fleet},
	url = {http://www.sciencedirect.com/science/article/pii/S096585641630756X},
	doi = {10.1016/j.tra.2016.08.020},
	abstract = {There are natural synergies between shared autonomous vehicle (AV) fleets and electric vehicle (EV) technology, since fleets of AVs resolve the practical limitations of today’s non-autonomous EVs, including traveler range anxiety, access to charging infrastructure, and charging time management. Fleet-managed AVs relieve such concerns, managing range and charging activities based on real-time trip demand and established charging-station locations, as demonstrated in this paper. This work explores the management of a fleet of shared autonomous electric vehicles (SAEVs) in a regional, discrete-time, agent-based model. The simulation examines the operation of SAEVs under various vehicle range and charging infrastructure scenarios in a gridded city modeled roughly after the densities of Austin, Texas. Results based on 2009 NHTS trip distance and time-of-day distributions indicate that fleet size is sensitive to battery recharge time and vehicle range, with each 80-mile range SAEV replacing 3.7 privately owned vehicles and each 200-mile range SAEV replacing 5.5 privately owned vehicles, under Level II (240-volt AC) charging. With Level III 480-volt DC fast-charging infrastructure in place, these ratios rise to 5.4 vehicles for the 80-mile range SAEV and 6.8 vehicles for the 200-mile range SAEV. SAEVs can serve 96–98\% of trip requests with average wait times between 7 and 10minutes per trip. However, due to the need to travel while “empty” for charging and passenger pick-up, SAEV fleets are predicted to generate an additional 7.1–14.0\% of travel miles. Financial analysis suggests that the combined cost of charging infrastructure, vehicle capital and maintenance, electricity, insurance, and registration for a fleet of SAEVs ranges from \$0.42 to \$0.49 per occupied mile traveled, which implies SAEV service can be offered at the equivalent per-mile cost of private vehicle ownership for low-mileage households, and thus be competitive with current manually-driven carsharing services and significantly cheaper than on-demand driver-operated transportation services. When Austin-specific trip patterns (with more concentrated trip origins and destinations) are introduced in a final case study, the simulation predicts a decrease in fleet “empty” vehicle-miles (down to 3–4\% of all SAEV travel) and average wait times (ranging from 2 to 4minutes per trip), with each SAEV replacing 5–9 privately owned vehicles.},
	language = {en},
	urldate = {2021-01-28},
	journal = {Transportation Research Part A: Policy and Practice},
	author = {Chen, T. Donna and Kockelman, Kara M. and Hanna, Josiah P.},
	month = dec,
	year = {2016},
	keywords = {Agent-based modeling, Autonomous vehicles, Carsharing, Electric vehicles},
	pages = {243--254},
}

@article{abar_agent_2017,
	title = {Agent {Based} {Modelling} and {Simulation} tools: {A} review of the state-of-art software},
	volume = {24},
	issn = {1574-0137},
	shorttitle = {Agent {Based} {Modelling} and {Simulation} tools},
	url = {http://www.sciencedirect.com/science/article/pii/S1574013716301198},
	doi = {10.1016/j.cosrev.2017.03.001},
	abstract = {The key intent of this work is to present a comprehensive comparative literature survey of the state-of-art in software agent-based computing technology and its incorporation within the modelling and simulation domain. The original contribution of this survey is two-fold: (1) Present a concise characterization of almost the entire spectrum of agent-based modelling and simulation tools, thereby highlighting the salient features, merits, and shortcomings of such multi-faceted application software; this article covers eighty five agent-based toolkits that may assist the system designers and developers with common tasks, such as constructing agent-based models and portraying the real-time simulation outputs in tabular/graphical formats and visual recordings. (2) Provide a usable reference that aids engineers, researchers, learners and academicians in readily selecting an appropriate agent-based modelling and simulation toolkit for designing and developing their system models and prototypes, cognizant of both their expertise and those requirements of their application domain. In a nutshell, a significant synthesis of Agent Based Modelling and Simulation (ABMS) resources has been performed in this review that stimulates further investigation into this topic.},
	language = {en},
	urldate = {2021-01-28},
	journal = {Computer Science Review},
	author = {Abar, Sameera and Theodoropoulos, Georgios K. and Lemarinier, Pierre and O’Hare, Gregory M. P.},
	month = may,
	year = {2017},
	keywords = {Agent Based Modelling and Simulation (ABMS) tools, Artificial life / social science simulations, Modelling complex systems, Multi-agent computing, Software agent, Swarm intelligence},
	pages = {13--33},
}

@article{macal_everything_2016,
	title = {Everything you need to know about agent-based modelling and simulation},
	volume = {10},
	issn = {1747-7786},
	url = {https://doi.org/10.1057/jos.2016.7},
	doi = {10.1057/jos.2016.7},
	abstract = {This paper addresses the background and current state of the field of agent-based modelling and simulation (ABMS). It revisits the issue of ABMS represents as a new development, considering the extremes of being an overhyped fad, doomed to disappear, or a revolutionary development, shifting fundamental paradigms of how research is conducted. This paper identifies key ABMS resources, publications, and communities. It also proposes several complementary definitions for ABMS, based on practice, intended to establish a common vocabulary for understanding ABMS, which seems to be lacking. It concludes by suggesting research challenges for ABMS to advance and realize its potential in the coming years.},
	language = {en},
	number = {2},
	urldate = {2021-01-28},
	journal = {Journal of Simulation},
	author = {Macal, C M},
	month = may,
	year = {2016},
	pages = {144--156},
}

@book{rinaldi_analysis_nodate,
	title = {Analysis and evaluation of energy efficiency of a shrinkwrap-packer},
	abstract = {Abstract. Since some years, European companies are interested in the issue of energy saving. In such context, a classification of industrial machines from the energetic point of view is becoming more and more important; currently, in the industrial context, no similar classifications exist. The aim of this work is to formulate a standard method to evaluate the energy performance of an industrial machine, such as a shrinkwrap-packer. To this purpose, we studied a shrinkwrap-packer produced by OCME S.r.l, a mechanical machinery company located in Parma (Italy). Specifically, the shrinkwrap-packer was analyzed in terms of energy dissipations and other aspects related to the energy source utilized to power the oven. As a result, we propose a general procedure that allows to compare different solutions from different perspectives related to energy efficiency. More precisely, we include three key performance indicators (KPIs) of energy efficiency, to build a comprehensive evaluation model. The first KPI concerns the energy efficiency class of the oven, the second KPI takes into account the impact of the oven on the work environment and the third one is an index of degradation of the energy resource utilized. A Global Index of Energy Efficiency (GI) is finally computed as the area of a triangle represented on a radar diagram on three axes, one for each KPI.},
	author = {Rinaldi, M. and Ferretti, G. and Montanari, R. and Bottani, E. and Vignali, G. and Solari, F. and Armenzoni, M. and Marchini, D.},
}

@article{gualtieri_evaluation_2019,
	title = {An evaluation methodology for the conversion of manual assembly systems into human-robot collaborative workcells},
	volume = {38},
	issn = {23519789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2351978920300470},
	doi = {10.1016/j.promfg.2020.01.046},
	language = {en},
	urldate = {2021-01-15},
	journal = {Procedia Manufacturing},
	author = {Gualtieri, Luca and Rauch, Erwin and Vidoni, Renato and Matt, Dominik T.},
	year = {2019},
	keywords = {Criteri, KPI, Q2},
	pages = {358--366},
}

@article{jonsson_kpis_2017,
	title = {{KPIs} for measuring performance of production systems for residential building: {A} production strategy perspective},
	volume = {17},
	issn = {1471-4175},
	shorttitle = {{KPIs} for measuring performance of production systems for residential building},
	url = {https://doi.org/10.1108/CI-06-2016-0034},
	doi = {10.1108/CI-06-2016-0034},
	abstract = {Purpose This paper aims to define key performance indicators (KPIs) for measuring performance of production systems for residential building from a production strategy perspective. Design/methodology/approach A literature review is done to identify suitable competitive priorities and to provide grounds for developing KPIs to measure them. The KPIs are evaluated and validated through interviews with industry experts from five case companies producing multifamily residences. Furthermore, two of the case companies are used to illustrate how the KPIs can be employed for analysing different production systems from a manufacturing strategy perspective. Findings Defined, and empirically validated, KPIs for measuring the competitive priorities quality, cost (level and dependability), delivery (speed and dependability) and flexibility (volume and mix) of different production systems. Research limitations/implications To further validate the KPIs, more empirical tests need to be done and further research also needs to address mix flexibility, which better needs to account for product range to provide a trustworthy KPI. Practical implications The defined KPIs can be used to evaluate and monitor the performance of different production systems’ ability to meet market demands, hence focusing on the link between the market and the firm’s production function. The KPIs can also be used to track a production systems’ ability to perform over time. Originality/value Most research that evaluate and compare production systems for residential building is based on qualitative estimations of manufacturing outputs. There is a lack of quantitative KPIs to measure performance at a strategic level. This research does this, identifying what to measure, but also how to measure four competitive priorities through 14 defined KPIs.},
	number = {3},
	urldate = {2021-01-20},
	journal = {Construction Innovation},
	author = {Jonsson, Henric and Rudberg, Martin},
	month = jan,
	year = {2017},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Construction production, KPI, Key performance indicators, Open manufacturing/Offsite construction, Performance measurement, Production strategy, Q2, Residential building},
	pages = {381--403},
}

@article{malik_developing_2019,
	title = {Developing a reference model for human–robot interaction},
	volume = {13},
	issn = {1955-2513, 1955-2505},
	url = {http://link.springer.com/10.1007/s12008-019-00591-6},
	doi = {10.1007/s12008-019-00591-6},
	abstract = {The use of collaborative robots is on the rise for human-centered automation for developing fexible production systems.
The deployment of collaborative robots and distribution of tasks between human and robot carries several challenges. One
challenge is assessing the right level of human engagement with the fellow robot in the collaborative work while remaining
in line with safety limitations and cycle time constraints. This study explores various types and levels of interactions between
humans and robots in a manufacturing domain. A synthesizing architecture of human–robot collaboration is suggested based
on three dimensions of team composition, level of engagement and safety. The architecture describes the collaboration using
a 3-dimensional reference scale.},
	language = {en},
	number = {4},
	urldate = {2021-01-20},
	journal = {International Journal on Interactive Design and Manufacturing (IJIDeM)},
	author = {Malik, Ali Ahmad and Bilberg, Arne},
	month = dec,
	year = {2019},
	keywords = {KPI, Q2},
	pages = {1541--1547},
}

@article{ramis_ferrer_implementing_2018,
	title = {Implementing and {Visualizing} {ISO} 22400 {Key} {Performance} {Indicators} for {Monitoring} {Discrete} {Manufacturing} {Systems}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2075-1702/6/3/39},
	doi = {10.3390/machines6030039},
	abstract = {The employment of tools and techniques for monitoring and supervising the performance of industrial systems has become essential for enterprises that seek to be more competitive in today\&rsquo;s market. The main reason is the need for validating tasks that are executed by systems, such as industrial machines, which are involved in production processes. The early detection of malfunctions and/or improvable system values permits the anticipation to critical issues that may delay or even disallow productivity. Advances on Information and Communication Technologies (ICT)-based technologies allows the collection of data on system runtime. In fact, the data is not only collected but formatted and integrated in computer nodes. Then, the formatted data can be further processed and analyzed. This article focuses on the utilization of standard Key Performance Indicators (KPIs), which are a set of parameters that permit the evaluation of the performance of systems. More precisely, the presented research work demonstrates the implementation and visualization of a set of KPIs defined in the ISO 22400 standard-Automation systems and integration, for manufacturing operations management. The approach is validated within a discrete manufacturing web-based interface that is currently used for monitoring and controlling an assembly line at runtime. The selected ISO 22400 KPIs are described within an ontology, which the description is done according to the data models included in the KPI Markup Language (KPIML), which is an XML implementation developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.},
	language = {en},
	number = {3},
	urldate = {2021-01-20},
	journal = {Machines},
	author = {Ramis Ferrer, Borja and Muhammad, Usman and Mohammed, Wael M. and Martínez Lastra, José L.},
	month = sep,
	year = {2018},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ISO 22400, KPI, KPIML, Q2, User interface, discrete manufacturing systems, key performance indicators, knowledge-based system, ontology},
	pages = {39},
}

@article{zanella_criteria_2017,
	title = {Criteria {Definition} for the {Identification} of {HRC} {Use} {Cases} in {Automotive} {Manufacturing}},
	volume = {11},
	issn = {23519789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2351978917303244},
	doi = {10.1016/j.promfg.2017.07.120},
	abstract = {The study aims at the definition of a methodology for the objective identification of the most suitable applicative use cases for a
profitable exploitation of HRC technology. The analysis is based on the preliminary assignment of values to multiple Key
Parameters (KPs). The KPs identification is based on a methodological analysis applied to multiple manufacturing cells in
production. Core of the process was the identification of the criteria and the KPs. A systematic application of the tool was made
to test and fine-tune the developed methodology. Criteria and methodology that were defined in the study are summarized.},
	language = {en},
	urldate = {2021-01-20},
	journal = {Procedia Manufacturing},
	author = {Zanella, Alessandro and Cisi, Alessandro and Costantino, Marco and Di Pardo, Massimo and Pasquettaz, Giorgio and Vivo, Giulio},
	year = {2017},
	keywords = {KPI, Q2},
	pages = {372--379},
}

@inproceedings{nielsen_mass_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Production} {Engineering}},
	title = {Mass {Customization} {Measurements} {Metrics}},
	isbn = {978-3-319-04271-8},
	doi = {10.1007/978-3-319-04271-8_31},
	abstract = {A recent survey has indicated that 17 \% of companies have ceased mass customizing less than 1 year after initiating the effort. This paper presents measurement for a company’s mass customization performance, utilizing metrics within the three fundamental capabilities: robust process design, choice navigation, and solution space development. A mass customizer when assessing performance with these metrics can identify within which areas improvement would increase competitiveness the most and enable more efficient transition to mass customization.},
	language = {en},
	booktitle = {Proceedings of the 7th {World} {Conference} on {Mass} {Customization}, {Personalization}, and {Co}-{Creation} ({MCPC} 2014), {Aalborg}, {Denmark}, {February} 4th - 7th, 2014},
	publisher = {Springer International Publishing},
	author = {Nielsen, Kjeld and Brunoe, Thomas D. and Joergensen, Kaj A. and Taps, Stig B.},
	editor = {Brunoe, Thomas D. and Nielsen, Kjeld and Joergensen, Kaj A. and Taps, Stig B.},
	year = {2014},
	keywords = {Capabilities, KPI, Mass customization, Measurement, Metrics},
	pages = {359--375},
}

@book{geraedts_design_2008,
	title = {Design for {Change}; {Flexibility} {Key} {Performance} {Indicators}},
	abstract = {With Flexibility Key Performance Indicators a detailed picture is obtained of the flexibility demanded or offered. They can be used to judge the various flexibility aspects of existing buildings, but also to formulate the requirements with respect to flexibility when new buildings are to be realized. Thus, these Indicators form a mean of communication on both the supply and the demand side of the building market. This research is describing four key indicators for measuring the flexibility of buildings: partitionability, adaptability, extendibility and multifunctionality. In order to assess the flexibility of buildings or their components, each key indicator is divided into a number of sub aspects. For each sub aspect assessment criteria are formulated leading to three possible ratings. Ratings multiplied by weighting factors give the different indicator scores. Thus, a final judgement can be given to the overall flexibility of a building. Here too, weighting factors, scores and flexibility classes have been developed.},
	author = {Geraedts, Rob},
	month = may,
	year = {2008},
	keywords = {KPI},
}

@article{tsarouchi_decision_2016,
	title = {A {Decision} {Making} {Framework} for {Human} {Robot} {Collaborative} {Workplace} {Generation}},
	volume = {44},
	issn = {22128271},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212827116003851},
	doi = {10.1016/j.procir.2016.02.103},
	language = {en},
	urldate = {2021-01-15},
	journal = {Procedia CIRP},
	author = {Tsarouchi, Panagiota and Spiliotopoulos, Jason and Michalos, George and Koukas, Spyros and Athanasatos, Athanasios and Makris, Sotiris and Chryssolouris, George},
	year = {2016},
	keywords = {Criteri},
	pages = {228--232},
}

@article{tsarouchi_humanrobot_2017,
	title = {On a human–robot workplace design and task allocation system},
	volume = {30},
	issn = {0951-192X},
	url = {https://doi.org/10.1080/0951192X.2017.1307524},
	doi = {10.1080/0951192X.2017.1307524},
	abstract = {This paper proposes a method for human–robot (HR) task planning, considering at the same time, the design of the workplace. A model for the representation of humans and robots as a team of active resources is proposed, while equipment such as working tables and fixtures are considered passive resources. The HR workload is structured in a three-level model. A multi-criteria decision-making framework is used for the formulation of alternative layouts and task allocations. Both analytical models and simulation are used for the estimation of the criteria values, allowing for the evaluation of the different alternatives. A software prototype has been implemented and tested in white goods and in automotive industry cases, demonstrating that the tool can identify good quality solutions in a short time frame.},
	number = {12},
	urldate = {2020-12-30},
	journal = {International Journal of Computer Integrated Manufacturing},
	author = {Tsarouchi, Panagiota and Michalos, George and Makris, Sotiris and Athanasatos, Thanasis and Dimoulas, Konstantinos and Chryssolouris, George},
	month = dec,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0951192X.2017.1307524},
	keywords = {Cell scheduling, Criteri, ergonomics, hybrid assembly system, multi-criteria decision-making, simulation},
	pages = {1272--1279},
}

@article{ore_human_2016,
	title = {Human – {Industrial} {Robot} {Collaboration}: {Application} of {Simulation} {Software} for {Workstation} {Optimisation}},
	volume = {44},
	issn = {22128271},
	shorttitle = {Human – {Industrial} {Robot} {Collaboration}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2212827116002195},
	doi = {10.1016/j.procir.2016.02.002},
	language = {en},
	urldate = {2021-01-15},
	journal = {Procedia CIRP},
	author = {Ore, Fredrik and Vemula, Bhanoday Reddy and Hanson, Lars and Wiktorsson, Magnus},
	year = {2016},
	keywords = {Criteri},
	pages = {181--186},
}

@article{ore_simulation_2019,
	title = {Simulation methodology for performance and safety evaluation of human–industrial robot collaboration workstation design},
	volume = {3},
	issn = {2366-5971, 2366-598X},
	url = {http://link.springer.com/10.1007/s41315-019-00097-0},
	doi = {10.1007/s41315-019-00097-0},
	language = {en},
	number = {3},
	urldate = {2021-01-15},
	journal = {International Journal of Intelligent Robotics and Applications},
	author = {Ore, Fredrik and Vemula, Bhanoday and Hanson, Lars and Wiktorsson, Magnus and Fagerström, Björn},
	month = sep,
	year = {2019},
	keywords = {Criteri},
	pages = {269--282},
}

@article{ore_method_2017,
	title = {Method for {Design} of {Human}-industrial {Robot} {Collaboration} {Workstations}},
	volume = {11},
	issn = {23519789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2351978917303165},
	doi = {10.1016/j.promfg.2017.07.112},
	language = {en},
	urldate = {2021-01-15},
	journal = {Procedia Manufacturing},
	author = {Ore, Fredrik and Hansson, Lars and Wiktorsson, Magnus},
	year = {2017},
	keywords = {Criteri},
	pages = {4--12},
}

@inproceedings{lietaert_model-based_2019,
	address = {Wels, Austria},
	title = {Model-based {Multi}-{Attribute} {Collaborative} {Production} {Cell} {Layout} {Optimization}},
	isbn = {978-1-5386-9257-8},
	url = {https://ieeexplore.ieee.org/document/8744136/},
	doi = {10.1109/REM.2019.8744136},
	urldate = {2021-01-15},
	booktitle = {2019 20th {International} {Conference} on {Research} and {Education} in {Mechatronics} ({REM})},
	publisher = {IEEE},
	author = {Lietaert, Pieter and Billen, Niels and Burggraeve, Sofie},
	month = may,
	year = {2019},
	keywords = {Criteri},
	pages = {1--7},
}

@inproceedings{awad_integrated_2017,
	address = {Limassol},
	title = {Integrated risk assessment and safety consideration during design of {HRC} workplaces},
	isbn = {978-1-5090-6505-9},
	url = {http://ieeexplore.ieee.org/document/8247648/},
	doi = {10.1109/ETFA.2017.8247648},
	urldate = {2021-01-15},
	booktitle = {2017 22nd {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Factory} {Automation} ({ETFA})},
	publisher = {IEEE},
	author = {Awad, Ramez and Fechter, Manuel and van Heerden, Jessica},
	month = sep,
	year = {2017},
	keywords = {Criteri},
	pages = {1--10},
}

@article{zhang_generating_2018,
	title = {Generating significant subassemblies from {3D} assembly models for design reuse},
	volume = {56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046007494&doi=10.1080%2f00207543.2018.1465608&partnerID=40&md5=4b6f1947e9d8a3f7abf28c057e0ce1e7},
	doi = {10.1080/00207543.2018.1465608},
	abstract = {Significant subassemblies are defined as the reusable regions of pre-existing 3D assembly models. A significant subassembly has great significances for design reuse as it aggregates abundant knowledge in a vivid 3D CAD model and enables designers to reuse existing mature designs from a high-level perspective. Consequently, this paper contributes to significant subassembly generation from pre-existing 3D assembly models for design reuse. The paper first gives an explicit definition of significant subassemblies and further explores the multilevel knowledge embedded in these significant subassemblies. Based on the above definition and multilevel knowledge, a knowledge-based approach is then proposed for significant subassembly generation, which includes three phases: (1) identifying candidate subassemblies with high cohesion inside and low coupling outside using the Markov clustering process; (2) removing normal candidate subassemblies with low reusability and less information, and generating filtered subassemblies using the proposed assembly frequency–inverse mean subassembly frequency based scheme; and (3) determining significant subassemblies by measuring the complexity of the filtered subassemblies. Finally, a computer numerical control honing machine model is taken as an application example to demonstrate the effectiveness of the proposed approach. © 2018, © 2018 Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {14},
	journal = {International Journal of Production Research},
	author = {Zhang, C. and Zhou, G. and Lu, Q. and Chang, F.},
	year = {2018},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {3D CAD Modeling, Application examples, Assembly, Assembly model, Cluster analysis, Clustering algorithms, Computer aided design, Computer control systems, Computer numerical control, Computer software reusability, Information filtering, Knowledge based systems, Knowledge management, Knowledge-based approach, Markov clustering, Product design, Reusability, Three phasis, significant subassembly generation},
	pages = {4744--4761},
}

@article{zhan_bridging_2019,
	title = {Bridging customer knowledge to innovative product development: a data mining approach},
	volume = {57},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060122867&doi=10.1080%2f00207543.2019.1566662&partnerID=40&md5=9621c03f189a276baef58cc881c47527},
	doi = {10.1080/00207543.2019.1566662},
	abstract = {In the big data era, firms are inundated with customer data, which are valuable in improving services, developing new products, and identifying new markets. However, it is not clear how companies apply data-driven methods to facilitate customer knowledge management when developing innovative new products. Studies have investigated the specific benefits of applying data-driven methods in customer knowledge management, but failed to systematically investigate the specific mechanics of how firms realised these benefits. Accordingly, this study proposes a systematic approach to link customer knowledge with innovative product development in a data-driven environment. To mine customer needs, this study adopts the Apriori algorithm and C5.0 in addition to the association rule and decision tree methodologies for data mining. It provides a systematic and effective method for managers to extract knowledge ‘from’ and ‘about’ customers to identify their preferences, enabling firms to develop the right products and gain competitive advantages. The findings indicate that the knowledge-based approach is effective, and the knowledge extracted is shown as a set of rules that can be used to identify useful patterns for both innovative product development and marketing strategies. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {20},
	journal = {International Journal of Production Research},
	author = {Zhan, Y. and Tan, K.H. and Huo, B.},
	year = {2019},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Apriori algorithms, Commerce, Competition, Competitive advantage, Customer knowledge, Customer knowledge management, Data mining, Data-driven methods, Decision trees, Innovative product development, Knowledge acquisition, Knowledge based systems, Knowledge management, Knowledge-based approach, Marketing strategy, Planning, Product development, Sales, Trees (mathematics)},
	pages = {6335--6350},
}

@article{yus_knowledge-based_2020,
	title = {A {Knowledge}-{Based} {Approach} to {Enhance} {Provision} of {Location}-{Based} {Services} in {Wireless} {Environments}},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084924770&doi=10.1109%2fACCESS.2020.2991051&partnerID=40&md5=5035b8522cdc4885ebdc652354e2e956},
	doi = {10.1109/ACCESS.2020.2991051},
	abstract = {Location-Based Services (LBS) are attracting a great interest with the fast expansion of mobile computing nowadays. These services use the user location to customize the offered information. However, most of those services are designed for specific scenarios and goals with implicit knowledge about the application context. As a consequence, hundreds of them are available (even with the same purpose). So, it is difficult for users to choose the most suitable service as they are in charge of knowing/finding the services which will be interesting for them, and handle the information that such services need. In this paper, we present an approach to handle LBS for mobile users which relieves them from knowing and managing the knowledge related to such services. This approach consists of a proposal for the modeling of such information as ontologies, which are handled by an agent-based architecture. Also, we propose to maintain updated the knowledge each mobile device contains by leveraging the exchange of information with others. For accessing the local knowledge, we present an SPARQL-like query language which avoids the ambiguities of natural language. Finally, we propose an approach to translate the user information needs into formal requests expressed in this query language, which could be later processed against the knowledge repositories to obtain the results the user needs. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Yus, R. and Bobed, C. and Mena, E.},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Agent based architectures, Application contexts, Exchange of information, Implicit knowledge, Knowledge based systems, Knowledge management, Knowledge repository, Knowledge-based approach, Location, Location based services, Query languages, Telecommunication services, User information need, Wireless environment},
	pages = {80030--80048},
}

@article{wang_new_2018,
	title = {New {Knowledge}-{Based} {Scoring} {Function} with {Inclusion} of {Backbone} {Conformational} {Entropies} from {Protein} {Structures}},
	volume = {58},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044421335&doi=10.1021%2facs.jcim.7b00601&partnerID=40&md5=d0d3f5fb97eddcda2f34eb3aedb54a74},
	doi = {10.1021/acs.jcim.7b00601},
	abstract = {Accurate prediction of a protein's structure requires a reliable free energy function that consists of both enthalpic and entropic contributions. Although considerable progresses have been made in the calculation of potential energies in protein structure prediction, the computation for entropies of protein has lagged far behind, due to the challenge that estimation of entropies often requires expensive conformational sampling. In this study, we have used a knowledge-based approach to estimate the backbone conformational entropies from experimentally determined structures. Instead of conducting computationally expensive MD/MC simulations, we obtained the entropies of protein structures based on the normalized probability distributions of back dihedral angles observed in the native structures. Our new knowledge-based scoring function with inclusion of the backbone entropies, which is referred to as ITScoreDA or ITDA, was extensively evaluated on 16 commonly used decoy sets and compared with 50 other published scoring functions. It was shown that ITDA is significantly superior to the other tested scoring functions in selecting native structures from decoys. The present study suggests the role of backbone conformational entropies in protein structures and provides a way for fast estimation of the entropic effect. © 2018 American Chemical Society.},
	number = {3},
	journal = {Journal of Chemical Information and Modeling},
	author = {Wang, X. and Zhang, D. and Huang, S.-Y.},
	year = {2018},
	note = {Publisher: American Chemical Society},
	keywords = {Accurate prediction, Algorithms, Bioinformatics, Conformational entropy, Conformational samplings, Databases, Dihedral angle, Entropic contributions, Entropy, Free energy, Free energy function, Knowledge Bases, Knowledge based systems, Knowledge-based approach, Potential energy, Probability distributions, Protein, Protein Conformation, Protein Folding, Protein structure prediction, Protein structures, Proteins, Thermodynamics, algorithm, chemistry, entropy, knowledge base, protein, protein conformation, protein database, protein folding, thermodynamics},
	pages = {724--732},
}

@article{wang_integrating_2019,
	title = {Integrating {Bonded} and {Nonbonded} {Potentials} in the {Knowledge}-{Based} {Scoring} {Function} for {Protein} {Structure} {Prediction}},
	volume = {59},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066913320&doi=10.1021%2facs.jcim.9b00057&partnerID=40&md5=cfa68e37367174a8ec608d9f8a97b24d},
	doi = {10.1021/acs.jcim.9b00057},
	abstract = {An accurate energy scoring function is crucial for protein structure prediction. Given the increasing number of experimentally determined structures, knowledge-based approaches have been widely used to develop scoring functions for protein structure prediction in the past three decades. However, current scoring functions often only consider nonbonded interactions and neglect bonded potentials like covalent bonds and angles for the sake of speed and simplicity. Although such scoring functions may be successful on fully relaxed conformations, they would have difficulties in ranking those decoys with distorted bonds or angles, especially when being used for conformational sampling in structure prediction. Therefore, such a scoring function may perform well on one or several decoy sets, but it often has limited accuracy on large diverse sets. Addressing the limitation, we have developed a composite knowledge-based scoring function, named as ITCPS, by integrating bonded and nonbonded potentials as well as orientation-dependent and hydrophobic interactions. Our scoring function ITCPS was extensively evaluated on 18 decoy sets of 927 proteins including three sets of 3DRobot, AMBER benchmarking set, HR, CASP5-8, CASP9-13, eight sets of Decoy 'R' Us, MOULDER, ROSETTA, and I-TASSER set and compared with 51 other scoring functions. It was shown that overall ITCPS performed the best among the 52 scoring functions and achieved a good performance on all the test sets. Of 927 proteins, ITCPS recognized the native structures for 842 proteins, giving a success rate of 90.8\% and an average Z-score of 3.36. Moreover, ITCPS also exhibited a strong ability to distinguish the best near-native structure among decoys and achieved a significantly better performance than other tested scoring functions. The present model is expected to be beneficial for the development of scoring functions for other interactions. © 2019 American Chemical Society.},
	number = {6},
	journal = {Journal of Chemical Information and Modeling},
	author = {Wang, X. and Huang, S.-Y.},
	year = {2019},
	note = {Publisher: American Chemical Society},
	keywords = {Computational Biology, Conformational samplings, Forecasting, Hydrophobic interactions, Hydrophobicity, Knowledge Bases, Knowledge based systems, Knowledge-based approach, Nonbonded interaction, Orientation dependent, Protein structure prediction, Proteins, Scoring functions, Structure prediction, Thermodynamics, biology, chemistry, knowledge base, metabolism, procedures, protein, thermodynamics},
	pages = {3080--3090},
}

@article{tang_knowledge-based_2002,
	title = {Knowledge-based stampability evaluation system},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036759791&partnerID=40&md5=0e60ca3bd95b96333089f7e0efc2ee5c},
	abstract = {The aim, characteristics and requirements of stampability evaluation are identified. As stampability evaluation is highly skill-intensive and requires a wide variety of design expertise and knowledge, a knowledge-based system is proposed for implementing the stampability evaluation. The stampability evaluation knowledge representation, and processing phases are illustrated. A case study demonstrates the feasibility of the knowledge-based approach to stampability evaluation.},
	number = {3},
	journal = {High Technology Letters},
	author = {Tang, D. and Zheng, L. and Li, Z.},
	year = {2002},
	keywords = {Design, Evaluation, Knowledge based systems, Knowledge representation, Stampability evaluation, Stamping},
	pages = {78--82},
}

@article{stenmark_distributed_2015,
	title = {On {Distributed} {Knowledge} {Bases} for {Robotized} {Small}-{Batch} {Assembly}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027936743&doi=10.1109%2fTASE.2015.2408264&partnerID=40&md5=4556c1d0eb11b681d50ae3ba3c5d0c4f},
	doi = {10.1109/TASE.2015.2408264},
	abstract = {The flexibility demands in manufacturing are severe, e.g., for rapid-change-over to new product variants, while robots are flexible machines that potentially can be adapted to a large variety of production tasks. Task definitions such as explicit robot programs are hardly reusable from an application point-of-view. To improve the situation, a knowledge-based approach exploiting distributed declarative information and cloud computing offers many possibilities for knowledge exchange and reuse, and it has the potential to facilitate new business models for industrial solutions. However, there are many unresolved questions yet, e.g., those related to reliability, consistency, or legal responsibility. To investigate some of these issues, different knowledge-based architectures have been prototyped and evaluated by confronting the solution candidates with key application demands. The conclusion is that distributed cloud-based approaches offer many possibilities, but there is still a need for further research and better infrastructure before this approach can become industrially attractive. © 2004-2012 IEEE.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Stenmark, M. and Malec, J. and Nilsson, K. and Robertsson, A.},
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Application programs, Cognitive robotics, Computer software reusability, Distributed knowledge basis, Industrial solutions, Knowledge based systems, Knowledge basis, Knowledge exchange, Knowledge management, Knowledge-based approach, New business models, Robot programming, Robots, Social networking (online), Web services, reuse of knowledge},
	pages = {519--528},
}

@article{siontorou_knowledge-based_2010,
	title = {A knowledge-based approach to online fault diagnosis of {FET} biosensors},
	volume = {59},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955708887&doi=10.1109%2fTIM.2009.2036464&partnerID=40&md5=8926ab20e7229b31ee4b4fdd3f443500},
	doi = {10.1109/TIM.2009.2036464},
	abstract = {Real-time diagnosis of insulatorsemiconductor field-effect transistor (ISFET)-based biosensor systems aims at promptly correcting errors caused by insufficient function; insufficiency is judged by the operational behavior of the sensor, i.e., the data that it produces. Ultimately, a complete failure of the system (i.e., a dead sensor) should easily be recognized. Much more difficult is the recognition of a gradual malfunction of this complex system, which may be attributed to faults or failures in one or more of its subsystems. Evidently, the identification of the possible fault modes and their symptoms requires in-depth knowledge of sensor's design and operation, both from the biochemical and electrical/electronic points of view, along with tackling uncertain, incomplete, or imprecise information. In this paper, a novel real-time diagnostic expert scheme for field-effect transistor (FET)-based biosensing is proposed. This paper 1) investigates the causes of sensor misfunction by means of fault tree analysis (FTA) relying on fuzzy reasoning to account for uncertainty and 2) proposes a computer-aided method for diagnosing biosensor failure during operation through an algorithmic procedure that is based on a nested loop mechanism. The tree (dendritic) structure (built using the information provided by the biosensor components and their intrarelations/interrelations on a surface- and a deep-knowledge level) serves as the knowledge base (KB), and the fuzzy-rules-based decision mechanism is the inference engine for fault detection and isolation. © 2006 IEEE.},
	number = {9},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Siontorou, C.G. and Batzias, F.A. and Tsakiri, V.},
	year = {2010},
	keywords = {Algorithmic procedure, Bio-sensor systems, Bioelectronics, Biosensing, Biosensors, Complex systems, Computer aided analysis, Decision mechanism, Design and operations, Diagnosis, Fault detection, Fault detection and isolation, Fault diagnosis, Fault modes, Fault tree analysis, Fault-trees, Field effect transistors, Fuzzy inference, Fuzzy reasoning, Imprecise information, In-depth knowledge, Knowledge base, Knowledge based systems, Knowledge level, Knowledge-based approach, MESFET devices, Nested Loops, On-line fault diagnosis, Operational behavior, Quality assurance, Real time systems, Real-time diagnosis, Real-time diagnostics, Reliability, Uncertainty analysis},
	pages = {2345--2364},
}

@article{riazuelo_roboearth_2015,
	title = {{RoboEarth} {Semantic} {Mapping}: {A} {Cloud} {Enabled} {Knowledge}-{Based} {Approach}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027971073&doi=10.1109%2fTASE.2014.2377791&partnerID=40&md5=df9f81c018619a848028f3a1015feeca},
	doi = {10.1109/TASE.2014.2377791},
	abstract = {The vision of the RoboEarth project is to design a knowledge-based system to provide web and cloud services that can transform a simple robot into an intelligent one. In this work, we describe the RoboEarth semantic mapping system. The semantic map is composed of: 1) an ontology to code the concepts and relations in maps and objects and 2) a SLAM map providing the scene geometry and the object locations with respect to the robot. We propose to ground the terminological knowledge in the robot perceptions by means of the SLAM map of objects. RoboEarth boosts mapping by providing: 1) a subdatabase of object models relevant for the task at hand, obtained by semantic reasoning, which improves recognition by reducing computation and the false positive rate; 2) the sharing of semantic maps between robots; and 3) software as a service to externalize in the cloud the more intensive mapping computations, while meeting the mandatory hard real time constraints of the robot. To demonstrate the RoboEarth cloud mapping system, we investigate two action recipes that embody semantic map building in a simple mobile robot. The first recipe enables semantic map building for a novel environment while exploiting available prior information about the environment. The second recipe searches for a novel object, with the efficiency boosted thanks to the reasoning on a semantically annotated map. Our experimental results demonstrate that, by using RoboEarth cloud services, a simple robot can reliably and efficiently build the semantic maps needed to perform its quotidian tasks. In addition, we show the synergetic relation of the SLAM map of objects that grounds the terminological knowledge coded in the ontology. © 2004-2012 IEEE.},
	number = {2},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Riazuelo, L. and Tenorth, M. and Di Marco, D. and Salas, M. and Gálvez-López, D. and Mösenlechner, L. and Kunze, L. and Beetz, M. and Tardós, J.D. and Montano, L. and Montiel, J.M.M.},
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Distributed database systems, False positive rates, Intelligent robots, Knowledge based systems, Knowledge representation, Knowledge-based approach, Machine design, Mapping, Mapping systems, Object recognition, Prior information, Real time systems, Robot perception, Robots, Semantic mapping, Semantic reasoning, Semantics, Software as a service (SaaS), Terminology, Visual SLAM, Web services},
	pages = {432--443},
}

@article{ralyte_knowledge-based_2008,
	title = {A knowledge-based approach to manage information systems interoperability},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49849088240&doi=10.1016%2fj.is.2008.01.008&partnerID=40&md5=de33b0f258180c411d17b5fe1b25d113},
	doi = {10.1016/j.is.2008.01.008},
	abstract = {Interoperability is a key property of enterprise applications, which is hard to achieve due to the large number of interoperating components and semantic heterogeneity. The inherent complexity of interoperability problems implies that there exists no silver bullet to solve them. Rather, the knowledge about how to solve wicked interoperability problems is hidden in the application cases that expose those problems. The paper addresses the question of how to organise and use method knowledge to resolve interoperability problems. We propose the structure of a knowledge-based system that can deliver situation-specific solutions, called method chunks. Situational Method Engineering promotes modularisation and formalisation of method knowledge in the form of reusable method chunks, which can be combined to compose a situation-specific method. The method chunks are stored in a method chunk repository. In order to cater for management and retrieval, we introduce an Interoperability Classification Framework, which is used to classify and tag method chunks and to assess the project situation in which they are to be used. The classification framework incorporates technical as well as business and organisational aspects of interoperability. This is an important feature as interoperability problems typically are multifaceted spanning multiple aspects. We have applied the approach to analyse an industry case from the insurance sector to identify and classify a set of method chunks. © 2008 Elsevier B.V. All rights reserved.},
	number = {7-8},
	journal = {Information Systems},
	author = {Ralyté, J. and Jeusfeld, M.A. and Backlund, P. and Kühn, H. and Arni-Bloch, N.},
	year = {2008},
	keywords = {Chlorine compounds, Classification (of information), Classification framework, Computer software reusability, Enterprise applications, Formalisation, Information theory, Inherent complexity, Interoperability, Interoperability Classification Framework, Interoperability classification, Interoperability problems, Knowledge base, Knowledge based systems, Knowledge engineering, Knowledge-based approach, Manage information, Method chunk, Method repository, Organisational aspects, Paper addresses, Semantic heterogeneity, Silver, Situational Method Engineering, Technology, Web services},
	pages = {754--784},
}

@article{plaza_studying_2011,
	title = {Studying the correlation between different word sense disambiguation methods and summarization effectiveness in biomedical texts},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052032602&doi=10.1186%2f1471-2105-12-355&partnerID=40&md5=9943074455219eafd88e470f6a50a74b},
	doi = {10.1186/1471-2105-12-355},
	abstract = {Background: Word sense disambiguation (WSD) attempts to solve lexical ambiguities by identifying the correct meaning of a word based on its context. WSD has been demonstrated to be an important step in knowledge-based approaches to automatic summarization. However, the correlation between the accuracy of the WSD methods and the summarization performance has never been studied.Results: We present three existing knowledge-based WSD approaches and a graph-based summarizer. Both the WSD approaches and the summarizer employ the Unified Medical Language System (UMLS) Metathesaurus as the knowledge source. We first evaluate WSD directly, by comparing the prediction of the WSD methods to two reference sets: the NLM WSD dataset and the MSH WSD collection. We next apply the different WSD methods as part of the summarizer, to map documents onto concepts in the UMLS Metathesaurus, and evaluate the summaries that are generated. The results obtained by the different methods in both evaluations are studied and compared.Conclusions: It has been found that the use of WSD techniques has a positive impact on the results of our graph-based summarizer, and that, when both the WSD and summarization tasks are assessed over large and homogeneous evaluation collections, there exists a correlation between the overall results of the WSD and summarization tasks. Furthermore, the best WSD algorithm in the first task tends to be also the best one in the second. However, we also found that the improvement achieved by the summarizer is not directly correlated with the WSD performance. The most likely reason is that the errors in disambiguation are not equally important but depend on the relative salience of the different concepts in the document to be summarized. © 2011 Plaza et al; licensee BioMed Central Ltd.},
	journal = {BMC Bioinformatics},
	author = {Plaza, L. and Jimeno-Yepes, A.J. and Díaz, A. and Aronson, A.R.},
	year = {2011},
	keywords = {Algorithms, Automatic summarization, Biomedical text, Data Mining, Graphic methods, Humans, Knowledge Bases, Knowledge based systems, Knowledge sources, Knowledge-based approach, Lexical ambiguity, Natural Language Processing, Natural language processing systems, Speech recognition, UMLS metathesaurus, Unified Medical Language System, Unified medical language systems, Word-sense disambiguation, algorithm, article, data mining, human, knowledge base, medical information system, natural language processing},
}

@article{matelli_cogeneration_2014,
	title = {Cogeneration design problem {Computational} complexity analysis and solution through an expert system},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937834094&doi=10.1108%2fEC-03-2012-0045&partnerID=40&md5=8435175a9ea28181c7d486bac28e6186},
	doi = {10.1108/EC-03-2012-0045},
	abstract = {Purpose-The purpose of this paper is twofold: to analyze the computational complexity of the cogeneration design problem; to present an expert system to solve the proposed problem, comparing such an approach with the traditional searching methods available. Design/methodology/approach-The complexity of the cogeneration problem is analyzed through the transformation of the well-known knapsack problem. Both problems are formulated as decision problems and it is proven that the cogeneration problem is np-complete. Thus, several searching approaches, such as population heuristics and dynamic programming, could be used to solve the problem. Alternatively, a knowledge-based approach is proposed by presenting an expert system and its knowledge representation scheme. Findings-The expert system is executed considering two case-studies. First, a cogeneration plant should meet power, steam, chilled water and hot water demands. The expert system presented two different solutions based on high complexity thermodynamic cycles. In the second case-study the plant should meet just power and steam demands. The system presents three different solutions, and one of them was never considered before by our consultant expert. Originality/value-The expert system approach is not a "blind" method, i.e. it generates solutions based on actual engineering knowledge instead of the searching strategies from traditional methods. It means that the system is able to explain its choices, making available the design rationale for each solution. This is the main advantage of the expert system approach over the traditional search methods. On the other hand, the expert system quite likely does not provide an actual optimal solution. All it can provide is one or more acceptable solutions. © Emerald Group Publishing Limited.},
	number = {6},
	journal = {Engineering Computations (Swansea, Wales)},
	author = {Matelli, J.A. and Silva, J.C. and Bazzo, E.},
	year = {2014},
	note = {Publisher: Emerald Group Publishing Ltd.},
	keywords = {Cogeneration plants, Combinatorial optimization, Computational complexity, Computational complexity analysis, Design, Dynamic programming, Engineering knowledge, Expert systems, Knapsack problems, Knowledge based systems, Knowledge representation, Knowledge-based approach, Optimal solutions, Paper-type, Problem solving, Searching strategy, Thermodynamic cycle, Thermodynamic properties},
	pages = {1034--1051},
}

@article{marchetta_artificial_2010,
	title = {An artificial intelligence planning approach to manufacturing feature recognition},
	volume = {42},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-75849116863&doi=10.1016%2fj.cad.2009.11.007&partnerID=40&md5=b355275dae490c1d56e6aa0cebfa8cf8},
	doi = {10.1016/j.cad.2009.11.007},
	abstract = {Within manufacturing, features have been widely accepted as useful concepts, and in particular they are used as an interface between CAD and CAPP systems. Previous research on feature recognition focus on the issues of intersecting features and multiple interpretations, but do not address the problem of custom features representation. Representation of features is an important aspect for making feature recognition more applicable in practice. In this paper a hybrid procedural and knowledge-based approach based on artificial intelligence planning is presented, which addresses both classic feature interpretation and also feature representation problems. STEP designs are presented as case studies in order to demonstrate the effectiveness of the model. © 2009 Elsevier Ltd. All rights reserved.},
	number = {3},
	journal = {CAD Computer Aided Design},
	author = {Marchetta, M.G. and Forradellas, R.Q.},
	year = {2010},
	keywords = {AI planning, Artificial intelligence, Artificial intelligence planning, Feature recognition, Feature representation, Knowledge based systems, Knowledge representation, Knowledge-based approach, Manufacturing features, Process control, Process engineering},
	pages = {248--256},
}

@article{loatman_hybrid_1987,
	title = {A hybrid architecture for natural language understanding},
	volume = {786},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953605616&doi=10.1117%2f12.940649&partnerID=40&md5=cd1af002c03f354bfbe7edca65e1857d},
	doi = {10.1117/12.940649},
	abstract = {The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) is an environment for developing natural language understanding (NLU) systems. It uses a knowledge-based approach in an integrated hybrid architecture based on a factoring of the NLU problem into its lexi-cal, syntactic, conceptual, domain-specific, and pragmatic components. The goal is a robust system that benefits from the strengths of several NLU methodologies, each applied where most appropriate. PAKTUS employs a frame-based knowledge representation and associative networks throughout. The lexical component uses morphological knowledge and word experts. Syntactic knowledge is represented in an Augmented Transition Network (ATN) grammar that incorporates rule-based programming. Case grammar is used for canonical conceptual representation with constraints. Domain-specific templates represent knowledge about specific applications as patterns of the form used in logic programming. Pragmatic knowledge may augment any of the other types and is added wherever needed for a particular domain. The system has been constructed in an interactive graphic programming environment. It has been used successfully to build a prototype front end for an expert system. This integration of existing technologies makes limited but practical NLU feasible now for narrow, well-defined domains. © 1987 SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Loatman, R.B.},
	year = {1987},
	keywords = {Associative network, Augmented transition networks, Expert systems, Hybrid architectures, Interactive graphics, Knowledge based, Knowledge representation, Knowledge-based approach, Logic programming, Natural language processing systems, Natural language understanding, Network architecture, Pragmatic knowledge, Syntactics},
	pages = {416--423},
}

@article{lin_protein_2009,
	title = {Protein subcellular localization prediction of eukaryotes using a knowledge-based approach},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549120704&doi=10.1186%2f1471-2105-10-S15-S8&partnerID=40&md5=b1f72e9085edccda3da873bb8f9c6846},
	doi = {10.1186/1471-2105-10-S15-S8},
	abstract = {Background: The study of protein subcellular localization (PSL) is important for elucidating protein functions involved in various cellular processes. However, determining the localization sites of a protein through wet-lab experiments can be time-consuming and labor-intensive. Thus, computational approaches become highly desirable. Most of the PSL prediction systems are established for single-localized proteins. However, a significant number of eukaryotic proteins are known to be localized into multiple subcellular organelles. Many studies have shown that proteins may simultaneously locate or move between different cellular compartments and be involved in different biological processes with different roles. Results: In this study, we propose a knowledge based method, called KnowPredsite, to predict the localization site(s) of both single-localized and multi-localized proteins. Based on the local similarity, we can identify the "related sequences" for prediction. We construct a knowledge base to record the possible sequence variations for protein sequences. When predicting the localization annotation of a query protein, we search against the knowledge base and used a scoring mechanism to determine the predicted sites. We downloaded the dataset from ngLOC, which consisted of ten distinct subcellular organelles from 1923 species, and performed ten-fold cross validation experiments to evaluate KnowPredsite's performance. The experiment results show that KnowPredsiteachieves higher prediction accuracy than ngLOC and Blast-hit method. For single-localized proteins, the overall accuracy of KnowPredsiteis 91.7\%. For multi-localized proteins, the overall accuracy of KnowPredsiteis 72.1\%, which is significantly higher than that of ngLOC by 12.4\%. Notably, half of the proteins in the dataset that cannot find any Blast hit sequence above a specified threshold can still be correctly predicted by KnowPredsite. Conclusion: KnowPredsitedemonstrates the power of identifying related sequences in the knowledge base. The experiment results show that even though the sequence similarity is low, the local similarity is effective for prediction. Experiment results show that KnowPredsiteis a highly accurate prediction method for both single- and multi-localized proteins. It is worth-mentioning the prediction process of KnowPredsiteis transparent and biologically interpretable and it shows a set of template sequences to generate the prediction result. The KnowPredsiteprediction server is available at http://bio-cluster.iis.sinica.edu.tw/kbloc/. © 2009 Lin et al; licensee BioMed Central Ltd.},
	number = {SUPPL. 15},
	journal = {BMC Bioinformatics},
	author = {Lin, H.-N. and Chen, C.-T. and Sung, T.-Y. and Ho, S.-Y. and Hsu, W.-L.},
	year = {2009},
	note = {Publisher: BioMed Central Ltd.},
	keywords = {Accurate prediction, Bioinformatics, Biological process, Cellular compartments, Cellular process, Computational Biology, Computational approach, Cross validation, Data processing, Data sets, Databases, Eukaryota, Eukaryotic proteins, Experiments, Forecasting, Knowledge Bases, Knowledge base, Knowledge based systems, Knowledge-based approach, Knowledge-based methods, Local similarity, Prediction accuracy, Prediction process, Prediction systems, Protein, Protein functions, Protein sequences, Protein subcellular localization, Protein subcellular localization prediction, Proteins, Query proteins, Sequence Analysis, Sequence similarity, Sequence variations, Software, Subcellular organelles, accuracy, amino acid sequence, article, biology, cell organelle, cellular distribution, chemistry, computer program, eukaryote, information processing, knowledge, knowledge base, methodology, nonhuman, performance, prediction, protein, protein database, protein localization, sequence analysis},
}

@article{li_application_2013,
	title = {Application of knowledge-based approaches in software architecture: {A} systematic mapping study},
	volume = {55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875214360&doi=10.1016%2fj.infsof.2012.11.005&partnerID=40&md5=cf228fb0c460ffe25d18b79191fcc60d},
	doi = {10.1016/j.infsof.2012.11.005},
	abstract = {Context: Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results: Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which "Embedded software" has received the most attention. © 2012 Elsevier B.V. All rights reserved.},
	number = {5},
	journal = {Information and Software Technology},
	author = {Li, Z. and Liang, P. and Avgeriou, P.},
	year = {2013},
	keywords = {Architectural design, Architectural design decisions, Architectural element, Architectural evaluation, Embedded software, Engineering activities, Knowledge based systems, Knowledge management, Knowledge management technology, Knowledge-based approach, Mapping, Software architecting, Software architecture, Systematic mapping studies},
	pages = {777--794},
}

@article{lewis_afterword_2010,
	title = {Afterword: {Data}, knowledge, and e-discovery},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951775303&doi=10.1007%2fs10506-010-9101-0&partnerID=40&md5=2acf0c62b5553f0e63d9adb9ca46a3a2},
	doi = {10.1007/s10506-010-9101-0},
	abstract = {Research in Artificial Intelligence (AI) and the Law has maintained an emphasis on knowledge representation and formal reasoning during a period when statistical, data-driven approaches have ascended to dominance within AI as a whole. Electronic discovery is a legal application area, with substantial commercial and research interest, where there are compelling arguments in favor of both empirical and knowledge-based approaches. We discuss the cases for both perspectives, as well as the opportunities for beneficial synergies. © Springer Science+Business Media B.V. 2010.},
	number = {4},
	journal = {Artificial Intelligence and Law},
	author = {Lewis, D.D.},
	year = {2010},
	keywords = {Application area, Automata theory, Automated reasoning, Categorization, Data-driven approach, ESI, Electronic discoveries, Electronically stored information, Formal reasoning, Knowledge based systems, Knowledge representation, Knowledge-based approach, Pattern recognition, Problem solving, Quality control},
	pages = {481--486},
}

@article{lee_identification_2019,
	title = {Identification of {I}-equivalent subnetworks in {Bayesian} networks to incorporate experts' knowledge},
	volume = {36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054914813&doi=10.1111%2fexsy.12346&partnerID=40&md5=0015e9a04e01bcd48594198a23b5a01d},
	doi = {10.1111/exsy.12346},
	abstract = {Bayesian networks (BNs) have been widely used in causal analysis because they can express the statistical relationship between significant variables. To gain superior causal analysis results, numerous studies have emphasized the importance of combining a knowledge-based approach and a data-based approach. However, combining these two approaches is a difficult task because it can reduce the effectiveness of the BN structure learning. Further, the learning schemes of BNs for computational efficiency can cause an inadequate causal analysis. To address these problems, we propose a knowledge-driven BN structure calibration algorithm for rich causal semantics. We first present an algorithm that can efficiently identify the subnetworks that can be altered to satisfy the learning condition of the BNs. We then reflect experts' knowledge to reduce erroneous causalities from the learned network. Experiments on various simulation and benchmark data sets were conducted to examine the properties of the proposed method and to compare its performance with an existing method. Further, an experimental study with real data from semiconductor fabrication plants demonstrated that the proposed method provided superior performance in improving structural accuracy. © 2018 John Wiley \& Sons, Ltd},
	number = {1},
	journal = {Expert Systems},
	author = {Lee, S.M. and Kim, S.B.},
	year = {2019},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {Bayesian Networks (bns), Bayesian networks, Benchmarking, Computational efficiency, Inductive learning, Knowledge based systems, Knowledge representation, Knowledge-based approach, Semantics, Semiconductor fabrication plant, Significant variables, Statistical relationship, Structure-learning, expert priors},
}

@article{lee_knowledge-based_1994,
	title = {A knowledge-based approach to the local area network design problem},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028387686&doi=10.1007%2fBF00872053&partnerID=40&md5=03de038b6a79dcf8f24285f1a54de332},
	doi = {10.1007/BF00872053},
	abstract = {Computer networks are an essential tool for people in business, industries, government, and schools. With the rapid rate of change in network technology and products, and the emergence of highly sophisticated network users, network design has become an increasingly complex task. Although the computer society aims at agreeing to a series of international standards for describing network architectures, the design of a computer network remains an ill-structured problem that lends itself perfectly to expert systems solutions. We propose an expert system that is able to design local area networks meeting the requirements specified by the user. Rules and guidelines pertaining to local area network design are formulated and incorporated into the knowledge base. The system is built on an object-oriented paradigm. The object-oriented approach and the hierarchical rule structure paradigm are discussed. We also employ the blackboard technique through which rules can access dynamic objects conveniently. © 1994 Kluwer Academic Publishers.},
	number = {1},
	journal = {Applied Intelligence},
	author = {Lee, S.-J. and Wu, C.-H.},
	year = {1994},
	note = {Publisher: Kluwer Academic Publishers},
	keywords = {Blackboard technique, Computer networks, Dynamic object access, Expert system solutions, Expert systems, Hierarchical rule structure paradigm, Knowledge-based approach, Local area network design problem, Local area networks, Network architecture standards},
	pages = {7--29},
}

@article{kurtz_hierarchical_2014,
	title = {A hierarchical knowledge-based approach for retrieving similar medical images described with semantic annotations},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902539126&doi=10.1016%2fj.jbi.2014.02.018&partnerID=40&md5=ced3f7d52d27eb51f1048ea407cc4bce},
	doi = {10.1016/j.jbi.2014.02.018},
	abstract = {Computer-assisted image retrieval applications could assist radiologist interpretations by identifying similar images in large archives as a means to providing decision support. However, the semantic gap between low-level image features and their high level semantics may impair the system performances. Indeed, it can be challenging to comprehensively characterize the images using low-level imaging features to fully capture the visual appearance of diseases on images, and recently the use of semantic terms has been advocated to provide semantic descriptions of the visual contents of images. However, most of the existing image retrieval strategies do not consider the intrinsic properties of these terms during the comparison of the images beyond treating them as simple binary (presence/absence) features. We propose a new framework that includes semantic features in images and that enables retrieval of similar images in large databases based on their semantic relations. It is based on two main steps: (1) annotation of the images with semantic terms extracted from an ontology, and (2) evaluation of the similarity of image pairs by computing the similarity between the terms using the Hierarchical Semantic-Based Distance (HSBD) coupled to an ontological measure. The combination of these two steps provides a means of capturing the semantic correlations among the terms used to characterize the images that can be considered as a potential solution to deal with the semantic gap problem. We validate this approach in the context of the retrieval and the classification of 2D regions of interest (ROIs) extracted from computed tomographic (CT) images of the liver. Under this framework, retrieval accuracy of more than 0.96 was obtained on a 30-images dataset using the Normalized Discounted Cumulative Gain (NDCG) index that is a standard technique used to measure the effectiveness of information retrieval algorithms when a separate reference standard is available. Classification results of more than 95\% were obtained on a 77-images dataset. For comparison purpose, the use of the Earth Mover's Distance (EMD), which is an alternative distance metric that considers all the existing relations among the terms, led to results retrieval accuracy of 0.95 and classification results of 93\% with a higher computational cost. The results provided by the presented framework are competitive with the state-of-the-art and emphasize the usefulness of the proposed methodology for radiology image retrieval and classification. © 2014 Elsevier Inc.},
	journal = {Journal of Biomedical Informatics},
	author = {Kurtz, C. and Beaulieu, C.F. and Napel, S. and Rubin, D.L.},
	year = {2014},
	note = {Publisher: Academic Press Inc.},
	keywords = {Classification (of information), Classification results, Computed tomographic, Computerized tomography, D region, Decision support systems, Earth Mover's distance, Image processing, Image retrieval, Information Storage and Retrieval, Knowledge Bases, Knowledge based systems, Knowledge-based approach, Liver lesions, Low-level image features, Ontology, Semantic image annotations, Semantic-based distances, Semantics, Tomography, X-Ray Computed, accuracy, algorithm, analytic method, article, computer assisted tomography, data base, hierarchical semantic based distance, image analysis, image retrieval, information retrieval, knowledge base, linguistics, liver, methodology, ontology, priority journal, semantics},
	pages = {227--244},
}

@article{kostopoulos_knowledge-based_2010,
	title = {Knowledge-based approaches in strategic management: {A} review of the literature},
	volume = {3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551578483&doi=10.1504%2fIJASS.2010.038350&partnerID=40&md5=a2c19fb79544df215ea038ca5624eac9},
	doi = {10.1504/IJASS.2010.038350},
	abstract = {Although Knowledge Management (KM) has been acknowledged as the most important resource and capability of an organisation, many academics' standpoints argue upon knowledge distinct characteristics. This paper is a critical review of KM literature and attempts to clarify different knowledge perspectives and offer a holistic picture of one of the most controversial issues to management science. We, first, build our critique on knowledge theory, explaining what knowledge is. Regarding different knowledge perspectives, such as Nonaka's, Tsoukas' and Blackler's, we attempt to clarify how knowledge interacts and is affected by their unique views. Second, we review KM theory according to the analysed knowledge perspectives. Finally, we categorise KM theory to three approaches: the cognitive, the community and the network approach, and we consider their impact to organisations. © 2010 Inderscience Enterprises Ltd.},
	number = {4},
	journal = {International Journal of Applied Systemic Studies},
	author = {Kostopoulos, K.C. and Brachos, D.A. and Philippidou, S.S. and Katsikis, I.N.},
	year = {2010},
	note = {Publisher: Inderscience Publishers},
	keywords = {Critical review, Knowledge based systems, Knowledge management, Knowledge perspective, Knowledge theory, Knowledge-based approach, Strategic management, Strategic planning},
	pages = {389--403},
}

@article{kokkoras_knowledge_2003,
	title = {A knowledge based approach on educational metadata use},
	volume = {2563},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248863575&doi=10.1007%2f3-540-38076-0_14&partnerID=40&md5=11240258925a65038a0f0c227ec967aa},
	doi = {10.1007/3-540-38076-0_14},
	abstract = {One of the most rapidly evolving e-services is e-Learning, that is, the creation of advanced educational resources that are accessible on-line and, potentially, offer numerous advantages over the traditional ones like intelligent access, interoperability between two or more educational resources and adaptation to the user. The driving force behind these approaches is the definition of the various standards about educational metadata, that is, data describing learning resources, the learner, assessment results, etc. The internal details of systems that utilize these metadata is an open issue since these efforts are primarily dealing with "what" and not "how". Under the light of these emerging efforts, we present CG-PerLS, a knowledge based approach for organizing and accessing educational resources. CG-PerLS is a model of a web portal for learning objects that encodes the educational metadata in the Conceptual Graph knowledge representation formalism, and uses related inference techniques to provide advanced functionality. The model allows learning resource creators to manifest their material and client-side learners to access these resources in a way tailored to their individual profile and educational needs. © Springer-Verlag Berlin Heidelberg 2003.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Kokkoras, F. and Sampson, D. and Vlahavas, I.},
	year = {2003},
	note = {Publisher: Springer Verlag},
	keywords = {E-learning, Education, Educational metadata, Educational needs, Educational resource, Inference techniques, Interoperability, Knowledge based systems, Knowledge representation, Knowledge representation formalism, Knowledge-based approach, Learning objects, Learning resource, Learning systems, Metadata, Portals},
	pages = {201--216},
}

@article{kohlbacher_strategic_2009,
	title = {Strategic knowledge-based marketing},
	volume = {3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-73849146114&doi=10.1504%2fijkms.2009.023487&partnerID=40&md5=474693afecf0a740aad494eb8b3d7d07},
	doi = {10.1504/ijkms.2009.023487},
	abstract = {This paper develops and presents the concept of strategic knowledge-based marketing in an effort to explain the role of knowledge in marketing and marketing strategy and the process of marketing knowledge (co-)creation and management. The purpose is to provide both academics and practitioners with a framework for understanding and analysing knowledge-based processes in marketing from a strategic perspective and learn how these can be leveraged to gain and sustain competitive advantage. Copyright © 2009 Inderscience Enterprises Ltd.},
	number = {1-2},
	journal = {International Journal of Knowledge Management Studies},
	author = {Kohlbacher, F.},
	year = {2009},
	note = {Publisher: Inderscience Publishers},
	pages = {154--175},
}

@article{kim_quantitative_2015,
	title = {A quantitative and knowledge-based approach to choosing security architectural tactics},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924257085&doi=10.1504%2fIJAHUC.2015.067780&partnerID=40&md5=9410c25e30a7e711d617d124de5166aa},
	doi = {10.1504/IJAHUC.2015.067780},
	abstract = {Biographical notes: Suntae Kim is an Assistant Professor of the Department of Software Engineering at Chonbuk National University. His research focuses on software architecture, design patterns, requirements engineering and mining software repository. Abstract: This paper presents a quantitative approach to choosing security architectural tactics using architectural tactic knowledge base. An architectural tactic is an architectural design building block pertaining to a software quality. The tactic knowledge base is a tactic repository composing of architectural tactic specifications defined in role based metamodelling language (RBML) and their relationships expressed in a feature model. In this paper, a cost of an architectural tactic is estimated by using the use case points method, and a level of tactic contribution for non-functional requirements (NFRs) is predicted by the analytic hierarchy process (AHP) and sensitivity analysis. Then, the proposed approach suggests the best possible fit which is likely to satisfy NFRs.We applied the approach to choosing security architectural tactics for building software architecture of an online trading system. Copyright © 2015 Inderscience Enterprises Ltd.},
	number = {1-2},
	journal = {International Journal of Ad Hoc and Ubiquitous Computing},
	author = {Kim, S.},
	year = {2015},
	note = {Publisher: Inderscience Publishers},
	keywords = {Analytic hierarchy process, Analytic hierarchy process (ahp), Computer software selection and evaluation, Knowledge base, Knowledge based systems, Knowledge-based approach, Mining software repositories, Non-functional requirements, Quantitative tactic selection, Secure software architectures, Security architectural tactics, Sensitivity analysis, Software architecture, Software quality},
	pages = {45--53},
}

@article{kang_knowledge-based_2014,
	title = {Knowledge-based extraction of adverse drug events from biomedical text},
	volume = {15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897915860&doi=10.1186%2f1471-2105-15-64&partnerID=40&md5=ed7d7efd722939f1b38301bb25dcc2cd},
	doi = {10.1186/1471-2105-15-64},
	abstract = {Background: Many biomedical relation extraction systems are machine-learning based and have to be trained on large annotated corpora that are expensive and cumbersome to construct. We developed a knowledge-based relation extraction system that requires minimal training data, and applied the system for the extraction of adverse drug events from biomedical text. The system consists of a concept recognition module that identifies drugs and adverse effects in sentences, and a knowledge-base module that establishes whether a relation exists between the recognized concepts. The knowledge base was filled with information from the Unified Medical Language System. The performance of the system was evaluated on the ADE corpus, consisting of 1644 abstracts with manually annotated adverse drug events. Fifty abstracts were used for training, the remaining abstracts were used for testing. Results: The knowledge-based system obtained an F-score of 50.5\%, which was 34.4 percentage points better than the co-occurrence baseline. Increasing the training set to 400 abstracts improved the F-score to 54.3\%. When the system was compared with a machine-learning system, jSRE, on a subset of the sentences in the ADE corpus, our knowledge-based system achieved an F-score that is 7 percentage points higher than the F-score of jSRE trained on 50 abstracts, and still 2 percentage points higher than jSRE trained on 90\% of the corpus.Conclusion: A knowledge-based approach can be successfully used to extract adverse drug events from biomedical text without need for a large training set. Whether use of a knowledge base is equally advantageous for other biomedical relation-extraction tasks remains to be investigated. © 2014 Kang et al.; licensee BioMed Central Ltd.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Kang, N. and Singh, B. and Bui, C. and Afzal, Z. and van Mulligen, E.M. and Kors, J.A.},
	year = {2014},
	note = {Publisher: BioMed Central Ltd.},
	keywords = {Abstracting, Artificial Intelligence, Concept recognition, Data Mining, Drug effects, Drug-Related Side Effects and Adverse Reactions, Extraction, Humans, Knowledge Bases, Knowledge base, Knowledge based systems, Knowledge-based approach, Learning systems, Medical information systems, Minimal training, Percentage points, Relation extraction, Unified Medical Language System, Unified medical language systems, adverse drug reaction, article, artificial intelligence, data mining, human, knowledge base, methodology},
}

@article{kamardeen_e-ohs_2011,
	title = {E-{OHS} planning system for builders},
	volume = {54},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952078458&doi=10.3763%2fasre.2010.0014&partnerID=40&md5=19b25e1f7cdbeac0c49fc8ebd181c95b},
	doi = {10.3763/asre.2010.0014},
	abstract = {The ability to identify safety and health hazards as early as possible and implement good occupational health and safety (OHS) plans is vital to a project of any size. It is also a core requirement in OHS regulations. Nonetheless, developing effective OHS plans faces significant challenges, including large scopes of construction projects, skill shortages within construction professionals and abundance of OHS knowledge in unorganized formats. A web-based system was developed to implement a knowledge-based approach to OHS planning as a way of addressing these challenges. The system was then evaluated by its potential end users. The evaluation results suggest that the system (1) captures OHS knowledge from different sources and retains it in a single and easily accessible virtual location, (2) is capable of providing on-demand OHS planning information, (3) could help site staff with on-the-job learning of OHS skills and (4) could help minimize accidents on site and thereby save time and money for builders. © 2011.},
	number = {1},
	journal = {Architectural Science Review},
	author = {Kamardeen, I.},
	year = {2011},
	keywords = {Construction industry, Construction professionals, Construction projects, End users, Evaluation results, Health, Health hazards, Industrial hygiene, Knowledge based systems, Knowledge management, Knowledge-based approach, OHS planning, OHS regulations, Occupational health and safety, Occupational health and safety (OHS), Occupational risks, Planning, Planning information, Planning systems, Skill shortage, Web-based system, World Wide Web},
	pages = {50--64},
}

@article{jelinek_knowledge-based_2020,
	title = {Knowledge-based approach in the field of non-destructive inspection of {CFRP} metal hybrid components via optical lock-in thermography},
	volume = {116},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088897087&doi=10.1016%2fj.ndteint.2020.102264&partnerID=40&md5=9f2a5fe7d61cdaf14825657c765d93ef},
	doi = {10.1016/j.ndteint.2020.102264},
	abstract = {The present development trend in lightweight construction is characterised by an increased application of material-hybrid transition areas which are frequently involved with an intensified and considerably challenging quality concern. This research explores a complete referencing process in order to acquire statistically secured datasets in the field of non-destructive inspection of CFRP metal hybrid components via optical lock-in thermography by introducing a novel defect abstraction method. Thereby, the knowledge-based management of the generated datasets constitutes the ultimate scope. The experimentally determined datasets comprise both an examiner-specific probability of detection and an adequate defect visibility in a preferably short measurement time. © 2020 Elsevier Ltd},
	journal = {NDT and E International},
	author = {Jelinek, M. and Reinhart, G.},
	year = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Abstraction methods, Defects, Knowledge based systems, Knowledge-based approach, Knowledge-based management, Light-weight constructions, Lockin thermography, Locks (fasteners), Non destructive inspection, Nondestructive examination, Probability of detection, Short measurement time, Thermography (imaging), Thermography (temperature measurement)},
}

@article{jang_framework_2014,
	title = {Framework for automatic delineation of second derivative of photoplethysmogram: {A} knowledge-based approach},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920938004&doi=10.5405%2fjmbe.1574&partnerID=40&md5=6465e517917cb77780424ec86ce9d786},
	doi = {10.5405/jmbe.1574},
	abstract = {Knowledge-based rules for delineating the second derivative of photoplethysmogram (SDPTG), which is widely used as an indicator of arterial stiffness, are proposed in this study. The SDPTG facilitates the distinction of five sequential waves, namely the initial positive wave (IPW), the early negative wave (ENW), the late upsloping wave (LUW), the late downsloping wave (LDW), and the diastolic positive wave (DPW). An analysis of these waves indicates that the SDPTG is a slowly time-varying signal and that the difference between two adjacent pulses cannot go beyond a certain range. It also indicates that the diastolic positive wave can be accurately estimated from the envelope of the SDPTG signal even with a noisy signal. To delineate the SDPTG, pulse waveforms are first divided into pulse segments, each of which contains one period of the SDPTG signal, using the slope sum function with an adaptive thresholding scheme, which simplifies detecting pulse onsets by enhancing the upslope of the pulse signal and suppressing the remainder of the signal. After pulse segmentation, IPW is first identified by picking the maximum positive peak in the segment. DPW is then extracted using a knowledge-based rule that uses the envelope of the SDPTG signal. In the range from IPW to DPW, ENW, LUW, and LDW are sequentially determined using knowledge- based rules. The proposed method is evaluated using the HIMS database, which includes 1,386 pulses. A positive predictive value of 99.71\% and a false negative rate of 1.02\% are obtained, and thus the proposed rules are expected to facilitate pulse diagnosis using SDPTG signals.},
	number = {6},
	journal = {Journal of Medical and Biological Engineering},
	author = {Jang, D.-G. and Park, S.-H. and Hahn, M.},
	year = {2014},
	note = {Publisher: Biomedical Engineering Society},
	keywords = {Adaptive thresholding, Arterial stiffness, Article, Digital volume pulse, Knowledge based systems, Knowledge-based approach, Knowledge-based rules, Photo-plethysmogram, Positive predictive values, Stiffness, Vascular age, arterial stiffness, automation, conceptual framework, diastole, knowledge base, photoelectric plethysmography, predictive value, waveform},
	pages = {547--553},
}

@article{henegar_knowledge_2004,
	title = {A knowledge based approach for automated signal generation in pharmacovigilance},
	volume = {107},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872403130&doi=10.3233%2f978-1-60750-949-3-626&partnerID=40&md5=8a30704f233be9e611c977f7509545b5},
	doi = {10.3233/978-1-60750-949-3-626},
	abstract = {Background: Pharmacovigilance experts detect new adverse drug reactions (ADR) by manually reviewing spontaneous reporting systems. Automated signal generation aims to focus the attention of experts on drug-adverse event associations which are disproportionally present in the database. Although adverse events are coded by means of controlled vocabularies such as the MedDRA dictionary, this semantic information is not taken into account for signal generation. Objective: To improve the performance of current signal detection algorithms using knowledge based approach. Method: We developed a formal ontology of ADRs and built a data mining tool that uses description logic representations of MedDRA terms to group medically related case reports. Results: This knowledge based approach increased the sensitivity of signal detection with no decrease in specificity. Discussion: A knowledge based approach improved the performance of signal detection tools. However, the huge workload involved in the knowledge engineering step limits the use of this approach for machine learning. © 2004 IMIA. All rights reserved.},
	journal = {Studies in Health Technology and Informatics},
	author = {Henegar, C. and Bousquet, C. and Agnès, A. and Degoulet, P. and Jaulent, M.-C.},
	year = {2004},
	note = {Publisher: IOS Press},
	keywords = {Adverse drug reactions, Automatic data processing, Data description, Data handling, Data mining, Drug adverse events, Knowledge based systems, Knowledge representation, Knowledge-based approach, Learning systems, Pharmacodynamics, Pharmacovigilance, Reporting systems, Semantic information, Semantics, Signal detection, Signal detection tools, Signal generators, Terminology},
	pages = {626--630},
}

@article{guo_evaluating_2019,
	title = {Evaluating automated entity extraction with respect to drug and non-drug treatment strategies},
	volume = {94},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064566344&doi=10.1016%2fj.jbi.2019.103177&partnerID=40&md5=2e2a00d1a5b94d0561fe0c5c829f80c2},
	doi = {10.1016/j.jbi.2019.103177},
	abstract = {Objectives: Treatment used in a randomized clinical trial is a critical data element both for physicians at the point of care and reviewers who are evaluating different interventions. Much of existing work on treatment extraction from the biomedical literature has focused on the extraction of pharmacological interventions. However, non-pharmacological interventions (e.g., exercise, diet, etc.) that are frequently used to address chronic conditions are less well studied. The goal of this study is to compare knowledge-based and machine learning strategies for the extraction of both drug and non-drug treatments. Methods: We collected 800 randomized clinical trial abstracts each for breast cancer and diabetes from PubMed. The treatments in the result/conclusion sentences of the abstracts were manually annotated and marked as drug/non-drug treatments. We then designed three methods to identify the treatments and evaluated the systems with respect to drug/non-drug treatments. The first method is solely based on knowledge base (here we used MetaMap). The second method is based on a machine learning model trained mainly on contextual features (ML\_only). The third method is a combination approach that integrates the previous two approaches. Results/discussion: Results show that MetaMap, when used with high precision semantic types, has better performance for drug compared to non-drug treatments (F1 = 0.77 vs. 0.64). The ML\_only approach has smaller performance difference between drug and non-drug treatments compared with the KB-based approach (F1 = 0.02 vs. 0.05, 0.07, and 0.13). The combination approach achieves significantly better performance than all MetaMap approaches alone for total treatments (F1 = 0.76 vs. 0.72, p {\textless} 0.001). The performance gain mainly comes from the non-drug treatments (0.03–0.08 improvement in F1), while the drug treatments do not benefit much from the combination approach (0–0.03 improvement in F1). Conclusion: These results suggest that a knowledge-based approach should be employed for medical conditions that are primarily treated with drugs whereas conditions that are treated with either a combination of drug and non-drug interventions or primarily non-drug interventions should use automated tools that combine machine learning and a knowledge-based approach to achieve optimal performance. © 2019 Elsevier Inc.},
	journal = {Journal of Biomedical Informatics},
	author = {Guo, J. and Blake, C. and Guan, Y.},
	year = {2019},
	note = {Publisher: Academic Press Inc.},
	keywords = {Abstracting, Article, Automation, Biomedical literature, Combination approaches, Drug Therapy, Drug therapy, Entity recognition, Extraction, Humans, Knowledge based systems, Knowledge-based approach, Learning systems, Machine Learning, Machine learning, Machine learning models, MetaMap, Optimal performance, Randomized Controlled Trials as Topic, Randomized clinical trials, Semantics, accuracy, antidiabetic agent, antineoplastic agent, automation, breast cancer, controlled study, data extraction, data processing, diabetes mellitus, diet therapy, drug therapy, exercise, human, information processing, knowledge base, machine learning, medical informatics, medical terminology, priority journal, randomized controlled trial (topic), semantics, software},
}

@article{garla_semantic_2012,
	title = {Semantic similarity in the biomedical domain: {An} evaluation across knowledge sources},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867218819&doi=10.1186%2f1471-2105-13-261&partnerID=40&md5=ad0534b40860f8c9aa67dd43dcfc8721},
	doi = {10.1186/1471-2105-13-261},
	abstract = {Background: Semantic similarity measures estimate the similarity between concepts, and play an important role in many text processing tasks. Approaches to semantic similarity in the biomedical domain can be roughly divided into knowledge based and distributional based methods. Knowledge based approaches utilize knowledge sources such as dictionaries, taxonomies, and semantic networks, and include path finding measures and intrinsic information content (IC) measures. Distributional measures utilize, in addition to a knowledge source, the distribution of concepts within a corpus to compute similarity; these include corpus IC and context vector methods. Prior evaluations of these measures in the biomedical domain showed that distributional measures outperform knowledge based path finding methods; but more recent studies suggested that intrinsic IC based measures exceed the accuracy of distributional approaches. Limitations of previous evaluations of similarity measures in the biomedical domain include their focus on the SNOMED CT ontology, and their reliance on small benchmarks not powered to detect significant differences between measure accuracy. There have been few evaluations of the relative performance of these measures on other biomedical knowledge sources such as the UMLS, and on larger, recently developed semantic similarity benchmarks.Results: We evaluated knowledge based and corpus IC based semantic similarity measures derived from SNOMED CT, MeSH, and the UMLS on recently developed semantic similarity benchmarks. Semantic similarity measures based on the UMLS, which contains SNOMED CT and MeSH, significantly outperformed those based solely on SNOMED CT or MeSH across evaluations. Intrinsic IC based measures significantly outperformed path-based and distributional measures. We released all code required to reproduce our results and all tools developed as part of this study as open source, available under http://code.google.com/p/ytex. We provide a publicly-accessible web service to compute semantic similarity, available under http://informatics.med.yale.edu/ytex.web/.Conclusions: Knowledge based semantic similarity measures are more practical to compute than distributional measures, as they do not require an external corpus. Furthermore, knowledge based measures significantly and meaningfully outperformed distributional measures on large semantic similarity benchmarks, suggesting that they are a practical alternative to distributional measures. Future evaluations of semantic similarity measures should utilize benchmarks powered to detect significant differences in measure accuracy. © 2012 Garla and Brandt; licensee BioMed Central Ltd.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Garla, V.N. and Brandt, C.},
	year = {2012},
	keywords = {Benchmarking, Biomedical domain, Biomedical ontologies, Context vector, Information contents, Information theory, Knowledge Bases, Knowledge based, Knowledge based systems, Knowledge sources, Knowledge-based approach, Measure-accuracy, Medical Subject Headings, Natural Language Processing, Open sources, Open systems, Path finding, Path-based, Relative performance, SNOMED-CT, Semantic network, Semantic similarity, Semantic similarity measures, Semantics, Significant differences, Similarity measure, Systematized Nomenclature of Medicine, Text processing, Unified Medical Language System, Web services, article, comparative study, knowledge base, natural language processing, semantics},
}

@article{fu_evidence_2010,
	title = {Evidence directed generation of plausible crime scenarios with identity resolution},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951200076&doi=10.1080%2f08839511003715154&partnerID=40&md5=e3ce1a22857f3bff26accd34fb3f4b77},
	doi = {10.1080/08839511003715154},
	abstract = {Given a set of collected evidence and a predefined knowledge base, some existing knowledge-based approaches have the capability of synthesizing plausible crime scenarios under restrictive conditions. However, significant challenges arise for problems where the degree of precision of available intelligence data can vary greatly, often involving vague and uncertain information. Also, the issue of identity disambiguation gives rise to another crucial barrier in crime investigation. That is, the generated crime scenarios may often refer to unknown referents (such as a person or certain objects), whereas these seemingly unrelated referents may actually be relevant to the common revealed. Inspired by such observation, this article presents a fuzzy compositional modeler to represent, reason, and propagate inexact information to support automated generation of crime scenarios. Further, the article offers a link-based approach to identifying potential duplicated referents within the generated scenarios. The applicability of this work is illustrated by means of an example for discovering unforseen crime scenarios.},
	number = {4},
	journal = {Applied Artificial Intelligence},
	author = {Fu, X. and Boongoen, T. and Shen, Q.},
	year = {2010},
	keywords = {Automated generation, Crime, Crime investigation, Degree of precision, Identity resolutions, Knowledge base, Knowledge based systems, Knowledge-based approach, Link-based approach, Restrictive conditions, Uncertain informations},
	pages = {253--276},
}

@article{ersen_learning_2015,
	title = {Learning behaviors of and interactions among objects through spatio-temporal reasoning},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925053039&doi=10.1109%2fTCIAIG.2014.2329770&partnerID=40&md5=8a16959a5854f576e46f7804ba45d72b},
	doi = {10.1109/TCIAIG.2014.2329770},
	abstract = {In this paper, we introduce an automated reasoning system for learning object behaviors and interactions through the observation of event sequences. We use an existing system to learn the models of objects and further extend it to model more complex behaviors. Furthermore, we propose a spatio-temporal reasoning based learning method for reasoning about interactions among objects. Experience gained through learning is to be used for achieving goals by these objects. We take The Incredible Machine game (TIM) as the main testbed to analyze our system. Tutorials of the game are used to train the system. We analyze the results of our reasoning system on four different input types: a knowledge base of relations; spatial information; temporal information; and spatio-temporal information from the environment. Our analysis reveals that if a knowledge base about relations is provided, most of the interactions can be learned. We have also demonstrated that our learning method which incorporates both spatial and temporal information gives close results to that of the knowledge-based approach. This is promising as gathering spatio-temporal information does not require prior knowledge about relations. Our second analysis of the spatio-temporal reasoning method in the Electric Box computer game domain verifies the success of our approach. © 2014 IEEE.},
	number = {1},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	author = {Ersen, M. and Sariel, S.},
	year = {2015},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Automated learning, Automated planning, Automation, Computer games, Knowledge based systems, Knowledge representation, Knowledge-based approach, Learning systems, Logical reasoning, Spatial informations, Spatio-temporal reasoning, Spatiotemporal information, Temporal information},
	pages = {75--87},
}

@article{ebejer_ligity_2019,
	title = {Ligity: {A} {Non}-{Superpositional}, {Knowledge}-{Based} {Approach} to {Virtual} {Screening}},
	volume = {59},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068065024&doi=10.1021%2facs.jcim.8b00779&partnerID=40&md5=a4639caf1865ab7c5e810cdd6a54b8af},
	doi = {10.1021/acs.jcim.8b00779},
	abstract = {We present Ligity, a hybrid ligand-structure-based, non-superpositional method for virtual screening of large databases of small molecules. Ligity uses the relative spatial distribution of pharmacophoric interaction points (PIPs) derived from the conformations of small molecules. These are compared with the PIPs derived from key interaction features found in protein-ligand complexes and are used to prioritize likely binders. We investigated the effect of generating PIPs using the single lowest energy conformer versus an ensemble of conformers for each screened ligand, using different bin sizes for the distance between two features, utilizing triangular sets of pharmacophoric features (3-PIPs) versus chiral tetrahedral sets (4-PIPs), fusing data for targets with multiple protein-ligand complex structures, and applying different similarity measures. Ligity was benchmarked using the Directory of Useful Decoys-Enhanced (DUD-E). Optimal results were obtained using the tetrahedral PIPs derived from an ensemble of bound ligand conformers and a bin size of 1.5 Å, which are used as the default settings for Ligity. The high-throughput screening mode of Ligity, using only the lowest-energy conformer of each ligand, was used for benchmarking against the whole of the DUD-E, and a more resource-intensive, "information-rich" mode of Ligity, using a conformational ensemble of each ligand, were used for a representative subset of 10 targets. Against the full DUD-E database, mean area under the receiver operating characteristic curve (AUC) values ranged from 0.44 to 0.99, while for the representative subset they ranged from 0.61 to 0.86. Data fusion further improved Ligity's performance, with mean AUC values ranging from 0.64 to 0.95. Ligity is very efficient compared to a protein-ligand docking method such as AutoDock Vina: if the time taken for the precalculation of Ligity descriptors is included in the comparason, then Ligity is about 20 times faster than docking. A direct comparison of the virtual screening steps shows Ligity to be over 5000 times faster. Ligity highly ranks the lowest-energy conformers of DUD-E actives, in a statistically significant manner, behavior that is not observed for DUD-E decoys. Thus, our results suggest that active compounds tend to bind in relatively low-energy conformations compared to decoys. This may be because actives - and thus their lowest-energy conformations - have been optimized for conformational complementarity with their cognate binding sites. © 2019 American Chemical Society.},
	number = {6},
	journal = {Journal of Chemical Information and Modeling},
	author = {Ebejer, J.-P. and Finn, P.W. and Wong, W.K. and Deane, C.M. and Morris, G.M.},
	year = {2019},
	note = {Publisher: American Chemical Society},
	keywords = {Algorithms, Binding Sites, Binding sites, Complexation, Conformational ensemble, Conformations, Data fusion, Drug Design, High throughput screening, Humans, Interaction features, Knowledge Bases, Knowledge based systems, Knowledge-based approach, Ligands, Low-energy conformations, Molecular Conformation, Molecular Docking Simulation, Molecules, Protein-ligand complexes, Protein-ligand docking, Proteins, Receiver operating characteristic curves, Small Molecule Libraries, Stereochemistry, Thermodynamics, algorithm, binding site, chemistry, conformation, drug design, human, knowledge base, ligand, metabolism, molecular docking, molecular library, pharmacology, protein, thermodynamics},
	pages = {2600--2616},
}

@article{chugh_effect_2020,
	title = {Effect of knowledge management on software product experience with mediating effect of perceived software process improvement: {An} empirical study for {Indian} software industry},
	volume = {46},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062702221&doi=10.1177%2f0165551519833610&partnerID=40&md5=1daaf2809832b34d3048a115379cacf4},
	doi = {10.1177/0165551519833610},
	abstract = {The software development industry is characterised by swift innovation and competition. To survive, software engineering (SE) organisations need to develop high-quality software products in a timely fashion and at low cost. Knowledge-based approaches to software development are extremely supportive to acquiring new knowledge and leveraging existing knowledge from software projects; this enables constant improvement of software development practices. In this empirical study of Indian SE organisations, we study the impact of managing knowledge for perceived software process improvement (PSPI) and its effect on software product quality. Information technology (IT) in knowledge management (KM) is an important facilitator for any SE organisation desiring to exploit evolving technologies for management of their knowledge assets and for carrying out various KM processes of knowledge capture, storage, retrieval and sharing. Surveys collected from Indian SE organisations were analysed to propose a model using a structured equation modelling (SEM) technique. Our findings reveal that the relation between KM and quality of software product is positively mediated by PSPI. These findings reinforce an arena that is of growing importance to researchers and practitioners and which has seen only a limited number of empirical studies to date in the context of Indian SE organisations. © The Author(s) 2019.},
	number = {2},
	journal = {Journal of Information Science},
	author = {Chugh, M. and Chanderwal, N. and Upadhyay, R. and Punia, D.K.},
	year = {2020},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {Competition, Cost engineering, High-quality software, Indian software engineering organisations, Indian software industries, Information technology, Knowledge based systems, Knowledge management, Knowledge-based approach, Process engineering, Software Process Improvement, Software design, Software development practices, Software product quality, Software products},
	pages = {258--272},
}

@article{canada-bago_new_2010,
	title = {A new collaborative knowledge-based approach for wireless sensor networks},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954798299&doi=10.3390%2fs100606044&partnerID=40&md5=23d61cf353107afdeefde3caebd082e5},
	doi = {10.3390/s100606044},
	abstract = {This work presents a new approach for collaboration among sensors in Wireless Sensor Networks. These networks are composed of a large number of sensor nodes with constrained resources: limited computational capability, memory, power sources, etc. Nowadays, there is a growing interest in the integration of Soft Computing technologies into Wireless Sensor Networks. However, little attention has been paid to integrating Fuzzy Rule-Based Systems into collaborative Wireless Sensor Networks. The objective of this work is to design a collaborative knowledge-based network, in which each sensor executes an adapted Fuzzy Rule-Based System, which presents significant advantages such as: experts can define interpretable knowledge with uncertainty and imprecision, collaborative knowledge can be separated from control or modeling knowledge and the collaborative approach may support neighbor sensor failures and communication errors. As a real-world application of this approach, we demonstrate a collaborative modeling system for pests, in which an alarm about the development of olive tree fly is inferred. The results show that knowledge-based sensors are suitable for a wide range of applications and that the behavior of a knowledge-based sensor may be modified by inferences and knowledge of neighbor sensors in order to obtain a more accurate and reliable output. © 2010 by the authors.},
	number = {6},
	journal = {Sensors},
	author = {Canada-Bago, J. and Fernandez-Prieto, J.A. and Gadeo-Martos, M.A. and Velasco, J.R.},
	year = {2010},
	keywords = {Algorithms, Biological, Collaborative approach, Collaborative knowledge, Communication, Computational capability, Computer Communication Networks, Computing technology, Constrained resources, Cooperating objects, Cooperative Behavior, Electric Power Supplies, Fuzzy Logic, Fuzzy rule-based systems, Fuzzy rules, Knowledge Bases, Knowledge based systems, Knowledge-based approach, Models, Remote Sensing Technology, Sensor nodes, Sensors, Soft computing, Theoretical, Uncertainty analysis, Wireless Technology, Wireless sensor networks, algorithm, article, biological model, computer network, cooperation, evaluation, fuzzy logic, instrumentation, knowledge base, methodology, power supply, remote sensing, theoretical model, wireless communication},
	pages = {6044--6062},
}

@article{calzada_using_2014,
	title = {Using the spatial {RIMER}+ approach to estimate negative self-rated health and its causes across {Northern} {Ireland}},
	volume = {8867},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921475747&doi=10.1007%2f978-3-319-13102-3_52&partnerID=40&md5=2b1c457c50073b818be926e2428cd3a5},
	doi = {10.1007/978-3-319-13102-3_52},
	abstract = {Self-rated health is a commonly-used survey technique that helps collecting information about the public health in an area. It is widely recognized that self-rated health has a strong correlation with key public-health variables such as deprivation, poverty, fear of crime or mortality. Therefore, it is a useful tool when assessing the public health situation of a neighborhood or town. This paper utilizes a recently-developed decision framework, named, Spatial RIMER+, to model a decision problem using real data where self-rated health is unknown in certain areas of Northern Ireland and needs to be estimated. The results retrieved in the study demonstrate the high accuracy of the methodology as well as its the flexibility and applicability to model a wide range of spatial decision scenarios. © Springer International Publishing Switzerland 2014.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Calzada, A. and Liu, J. and Nugent, C. and Wang, H. and Martinez, L.},
	year = {2014},
	note = {Publisher: Springer Verlag},
	keywords = {Artificial intelligence, Decision framework, Decision problems, Decision support systems, Decision theory, Health, High-accuracy, Knowledge based systems, Knowledge representation, Knowledge-based approach, Northern Ireland, Public health, Rating, Strong correlation, Survey techniques, Ubiquitous computing},
	pages = {312--319},
}

@article{calabrese_hierarchical-granularity_2010,
	title = {Hierarchical-granularity holonic modelling},
	volume = {1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649550986&doi=10.1007%2fs12652-010-0013-3&partnerID=40&md5=932b95efadd5b6f1ee1dfc6d4b1b0886},
	doi = {10.1007/s12652-010-0013-3},
	abstract = {Design criteria for distributed and pervasive intelligent systems, such as Multi Agent Systems (MAS), are generally led by the functional decomposition of the given application-dependent knowledge. Consequently, changes either in the problem semantics or in the granularity level description may have a significant impact on the overall system re-engineering process. In order to tackle better these issues, a novel framework called Hierarchical-Granularity Holonic Model (HGHM) is introduced as a holon-based approach to distributed intelligent systems modelling. A holon is an agent endowed with special features. Seen from the outside, a holon behaves like an intelligent agent; seen from the inside, it appears to be decomposable into other holons. This property allows for modelling complex distributed systems at multiple hierarchical-granularity levels by exploiting the different abstraction layers at which the design process is carried out. The major benefit of the proposed approach against traditional holonic systems and MAS is that the entire HGHM-based architecture can be derived directly from the problem ontology as a hierarchical composition of selfsimilar, modular blocks. This helps designers focussing more on knowledge representation at different granularity levels which is a very basic process, as in top-down problem decomposition. Starting from the literature on holonic systems, a theoretical model of HGHM is introduced and an architectural model is derived accordingly. Finally, a customized application for the case study of distributed indoor air quality monitoring systems is commented and improvements in terms of system design with respect to well-established solutions are considered. © Springer-Verlag 2010.},
	number = {3},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Calabrese, M. and Amato, A. and di Lecce, V. and Piuri, V.},
	year = {2010},
	keywords = {Abstracting, Abstraction layer, Air quality, Architectural models, Complex distributed system, Design, Design criterion, Design process, Distributed intelligent systems, Functional decomposition, Granularity levels, Hierarchical composition, Hierarchical systems, Hierarchical-granularity holonic model, Holonic system, Holonics, Indoor air pollution, Indoor air quality monitoring, Intelligent systems, Knowledge based systems, Knowledge representation, Knowledge-based approach, Multi agent, Multi agent systems, Ontology, Problem decomposition, Self-similar, Semantics, Significant impacts, System design, System re-engineering, Systems analysis, Theoretical models, Topdown},
	pages = {199--209},
}

@article{bratianu_students_2017,
	title = {Students’ perception on developing conceptual generic skills for business: {A} knowledge-based approach},
	volume = {47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034598569&doi=10.1108%2fVJIKMS-11-2016-0065&partnerID=40&md5=deb715ba1cdac78416c3e7479b829bea},
	doi = {10.1108/VJIKMS-11-2016-0065},
	abstract = {Purpose: The classical approach of teaching and learning mostly based on knowledge transfer is questionable as knowledge life cycle is shortening and new type of jobs appear every day with new knowledge request. In this vein, the purpose of this paper is to investigate how to switch the focus from learning knowledge to learning generic skills liable to help future professionals to think and learn by doing. Design/methodology/approach: The research is based on a 30-item questionnaire that was addressed to over 500 students involved in management and business undergraduate and graduate programs from two well-reputed Romanian universities. Three hundred and forty questionnaires were filled in and processed using SPSS, version 19. Additionally, a factorial analysis was performed, with a view to extract the most important factors that are involved in developing generic skills in university programs. Findings: Results demonstrate that most of the students from the undergraduate programs prefer the classical approach – less implication and responsibility in doing a harder conceptual work – while most students from the master programs are open to the new perspective of learning to learn, namely, to developing generic skills. Research limitations/implications: In the new turbulent business landscape, universities face a significant change in teaching their students. Although the research adds to the value of the extant literature on generic skills (also known as core skills), it is mainly focused on a Romanian sample, thus reflecting a context-based perspective. Originality/value: The current study provides a preliminary insight into the perception of Romanian students about developing generic skills and into their readiness to assume the role of main actors in the learning process. © 2017, © Emerald Publishing Limited.},
	number = {4},
	journal = {VINE Journal of Information and Knowledge Management Systems},
	author = {Bratianu, C. and Vatamanescu, E.-M.},
	year = {2017},
	note = {Publisher: Emerald Group Publishing Ltd.},
	keywords = {Conceptual generic skills, Core competence, Design/methodology/approach, Education, Knowledge acquisition, Knowledge based systems, Knowledge life cycles, Knowledge management, Knowledge-based approach, Learning, Life cycle, Romanian universities, Students, Surveys, Teaching, Teaching and learning},
	pages = {490--505},
}

@article{blondet_towards_2016,
	title = {Towards a knowledge based framework for numerical design of experiment optimization and management},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963614852&doi=10.1080%2f16864360.2016.1168236&partnerID=40&md5=dba3e5d426b435f264e6e479b06a128d},
	doi = {10.1080/16864360.2016.1168236},
	abstract = {Numerical Design of Experiment (DoE) is a powerful tool for product development, used to improve product quality and robustness. However, the simulation process can be highly extended by the DoE process. While methods have been developed to shorten the execution of numerical DoE, the time needed to set up the numerical DoE process is longer and longer. This paper presents a description of the objectives and first results of the SDM4DOE project (Simulation Data Management for Design of Experiment). This project aims to define a set of tools and methods to improve the simulation process involving DoE: data management, data robustness improvement and process shortening. A knowledge-based approach is proposed to solve this main issue, based on a specific knowledge representation. © 2016 CAD Solutions, LLC.},
	number = {6},
	journal = {Computer-Aided Design and Applications},
	author = {Blondet, G. and Belkadi, F. and Le Duigou, J. and Bernard, A. and Boudaoud, N.},
	year = {2016},
	note = {Publisher: Taylor and Francis Inc.},
	keywords = {Data robustness, Design of experiments, Knowledge based framework, Knowledge based systems, Knowledge management, Knowledge representation, Knowledge-based approach, Numerical design, Numerical methods, Product design, Simulation data managements, Simulation process, Specific knowledge, Tools and methods},
	pages = {872--884},
}

@article{bhuiyan_pose_2008,
	title = {On {Pose} {Estimation} for {Human}-{Robot} {Symbiosis}},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-40549085050&doi=10.5772%2f5663&partnerID=40&md5=c78fb358c60f19cac47875655777f648},
	doi = {10.5772/5663},
	abstract = {This paper presents a vision based pose estimation system using knowledge based approach for human-robot symbiosis. The system is based on visual information of the face by connected component analysis of the skin color segmentation of images in HSV color model and is commenced with the face recognition and pose classification scheme using subspace PCA based pattern-matching strategies. With the knowledge of the known user's profile, face poses are then classified by multilayer perceptron. Based on the frame-based knowledge representation approach, face poses are being interpreted using the Software Platform for Agent and Knowledge (SPAK) management. On face pose recognition, robot is then instructed to perform some specific tasks by issuing pose commands. Experimental results demonstrate that the subspace method is better than that of the standard PCA method for face pose classification. The system has been demonstrated with the implementation of the algorithm to interact with an entertainment robot named, AIBO for human-robot symbiotic relationship. © 2008 SAGE Publications Ltd.},
	number = {1},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Bhuiyan, M.A.-A. and Liu, C.H. and Ueno, H.},
	year = {2008},
	note = {Publisher: SAGE Publications Inc.},
	keywords = {AIBO, Classification (of information), Color, Color codes, Color matching, Computer software, Connected component analysis, Face pose classification, Face recognition, HSV color model, HSV color models, Human computer interaction, Human robots, Human-robot symbiosis, Image matching, Image segmentation, Knowledge based systems, Knowledge representation, Knowledge-based approach, Mathematical models, Motion estimation, Multilayer neural networks, Pattern matching, Pattern recognition systems, Pose classifications, Principal component analysis, Robots, Skin-color segmentation, Software agents, Symbiotic relationship},
	pages = {19--30},
}

@article{beydoun_automating_2010,
	title = {Automating dimensional tolerancing using {Ripple} down {Rules} ({RDR})},
	volume = {37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950188452&doi=10.1016%2fj.eswa.2009.12.086&partnerID=40&md5=a14a41e50c9965acb11578dc18ee3bb6},
	doi = {10.1016/j.eswa.2009.12.086},
	abstract = {We propose to use a knowledge based approach to assist in mechanical design focusing on dimensional tolerancing. To illustrate our approach, we capture the knowledge which human designers utilize in order to specify dimensional tolerances on shafts and mating holes in order to meet desired classes of fit as set by relevant engineering standards. The software system we developed would help mechanical designers become more effective in the time-consuming dimensioning and tolerancing process of their designs in the future. In doing this, the paper makes a twofold contribution to the field of knowledge acquisition: firstly, interface was adjusted to receive mathematical functions with their specifications prior and during the KA process to propose an approach to exploit relationships among several classes with respect to certain numerical features of the cases in order to accelerate the convergence of the RDR knowledge acquisition process by generating artificial cases which are likely to trigger the addition of exception rules. Secondly, it introduces the above problem domain of determining suitable tolerances for mechanical parts in a design as a knowledge acquisition problem. © 2010 Elsevier Ltd. All rights reserved.},
	number = {7},
	journal = {Expert Systems with Applications},
	author = {Beydoun, G. and Hoffmann, A. and Hamade, R.F.},
	year = {2010},
	keywords = {Automation design, Convergence of numerical methods, Design, Dimensional tolerance, Dimensioning and tolerancing, Engineering standards, Exception rules, Fits and tolerances, Functions, Knowledge acquisition, Knowledge based systems, Knowledge-based approach, Mathematical functions, Mechanical design, Mechanical parts, Mergers and acquisitions, Numerical features, Problem domain, Ripple down rules, Software systems, Standards, Tolerancing},
	pages = {5101--5109},
}

@article{aulinas_supporting_2011,
	title = {Supporting decision making in urban wastewater systems using a knowledge-based approach},
	volume = {26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251595622&doi=10.1016%2fj.envsoft.2010.11.009&partnerID=40&md5=86ade2f893e1674ad474ee8d4a7e4226},
	doi = {10.1016/j.envsoft.2010.11.009},
	abstract = {The use of knowledge-based systems has been shown to be a suitable approach to support decision making in environmental systems. Capturing and managing the huge quantity of data/information that has to be considered is an intrinsic factor that makes environmental systems a sophisticated domain. Organizing this data in a naive way can impact the efficacy of any knowledge-based system. Another intrinsic factor is the variety of data sources, which can result in inconsistent, uncertain or incomplete knowledge bases when different data sources are considered. Accordingly, two central issues of a successful knowledge-based system are the organization of its knowledge base and the expressiveness of its specification language. In this paper, we introduce a stratified framework for structuring any environmental knowledge base. We will argue that a declarative specification language, such as Answer Set Programming, is expressive enough to capture environmental knowledge bases that are inconsistent, uncertain and incomplete. We also present an automata-based approach to manage actions in knowledge-based systems. By solving a use case, specifically the diagnosis of the safety of a particular industrial wastewater discharge in an urban wastewater system, we illustrate how to represent relevant abstractions to model related complex processes. We show that by using them it is also possible to automate the diagnosis process (in the present case, for example, to diagnose problems at a wastewater treatment plant and afterward in the river) and hence support the decision-making task. © 2010 Elsevier Ltd.},
	number = {5},
	journal = {Environmental Modelling and Software},
	author = {Aulinas, M. and Nieves, J.C. and Cortés, U. and Poch, M.},
	year = {2011},
	keywords = {Accident prevention, Answer set programming, Complex Processes, Data source, Decision making, Decision support systems, Environmental knowledge, Environmental systems, Incomplete knowledge, Industrial wastewaters, Industry, Intrinsic factors, Knowledge base, Knowledge based systems, Knowledge management, Knowledge-based approach, Specification languages, Specifications, Urban wastewater system, Wastewater, Wastewater treatment, Wastewater treatment plants, Water treatment plants, data interpretation, decision support system, industrial waste, knowledge based system, software, urban region, waste management, wastewater},
	pages = {562--572},
}

@article{arendt_decision-making_2011,
	title = {A decision-making module for aiding ship system automation design: {A} knowledge-based approach},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956618185&doi=10.1016%2fj.eswa.2010.06.079&partnerID=40&md5=210df7ea00b268f81828263dbf64d8f7},
	doi = {10.1016/j.eswa.2010.06.079},
	abstract = {The use of elements of artificial intelligence, including knowledge-based systems, becomes more and more widespread in aiding design problem solutions. The authors have been working on problems of control systems for many years. A design process involves many decision problems connected with, for example, a choice of a subsystem structure, subunits or particular elements selection. Because of such regards, it was decided to extend knowledge-based system with a module for support of such decision making. In this paper, an elaborated module for decision-making support is considered. The basic theoretical assumptions concerning the accepted method of multiattribute decision making based on pairwise comparison in categories of hierarchical decision process (AHP) is presented. Accepted knowledge representation in AHP method and pairwise comparison method and methods of expert knowledge acquisition are discussed. The module functioning is illustrated by an example of choice of temperature sensors in a system of fuel transport to Diesel engine of a main propulsion unit of a ship. © 2010 Elsevier Ltd. All rights reserved.},
	number = {1},
	journal = {Expert Systems with Applications},
	author = {Arendt, R. and Van Uden, E.},
	year = {2011},
	keywords = {AHP method, Artificial intelligence, Decision making, Decision making support, Decision problems, Decision process, Design, Design problems, Design process, Diesel engines, Elements selection, Hierarchical systems, Knowledge acquisition, Knowledge based systems, Knowledge representation, Knowledge-based approach, Multi attribute decision making, Pair-wise comparison, Power systems, Ship propulsion, Ship systems, Ships},
	pages = {410--416},
}

@article{amato_knowledge_2008,
	title = {A knowledge based approach for a fast image retrieval system},
	volume = {26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49549110522&doi=10.1016%2fj.imavis.2008.01.005&partnerID=40&md5=70185400dcc30da8be567674e33f2b99},
	doi = {10.1016/j.imavis.2008.01.005},
	abstract = {Due to the huge increase in the amount of digital images available in the "Internet era", making efficient Content Based Image Retrieval (CBIR) systems has become one of the major endeavors. In this paper, the authors study the integration of an automatic generated knowledge base in a CBIR system based on relevance feedback method. An extensive analysis of the database structure has been carried out using fuzzy clustering algorithms to build the knowledge base. This knowledge base is used to make users aware of the overall organization of the image database during the query process. The relevance feedback method has been used to model the cluster structure as well as the correspondence between high-level user concepts and their low-level machine representation by performing retrievals according to multiple queries supplied by the user during the course of a retrieval session. The results presented in this paper demonstrate that this approach provides accurate retrieval results showing acceptable interaction speed that can be compared with existing methods. © 2008 Elsevier B.V. All rights reserved.},
	number = {11},
	journal = {Image and Vision Computing},
	author = {Amato, A. and Di Lecce, V.},
	year = {2008},
	note = {Publisher: Elsevier Ltd},
	keywords = {CBIR systems, Chlorine compounds, Cluster structures, Clustering algorithms, Content-based image retrieval, Control theory, Data base structures, Database systems, Digital imaging, Feedback, Fuzzy Clustering algorithms, Fuzzy clustering, Image databases, Image retrieval, Image retrieval systems, Information retrieval, Interaction speed, Knowledge, Knowledge base, Knowledge based systems, Knowledge-based approach, Multi-query relevance feedback, Multiple queries, Relevance feedback method},
	pages = {1466--1480},
}

@article{almeida_knowledge-based_2010,
	title = {A knowledge-based approach to the iris segmentation problem},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449635722&doi=10.1016%2fj.imavis.2009.07.003&partnerID=40&md5=4a7bb58cae087c6c0ccb6416ca072be3},
	doi = {10.1016/j.imavis.2009.07.003},
	abstract = {This paper describes a knowledge-based approach to the problem of locating and segmenting the iris in images showing close-up human eyes. This approach is inspired in the expert system's paradigm but, due the specific processing problems associated with image analysis, uses direct encoding of the "decision rules", instead of a classic, formalized, knowledge base. The algorithm involves a succession of phases that deal with image pre-processing, pupil location, iris location, combination of pupil and iris, eyelids detection, and filtering of reflections. The development was iterative, based on successive improvements tested over a set of training images. The results that were achieved indicate that this global approach can be useful to solve image analysis problems over which human "experts" have better performance than the present computer-based solutions. © 2009 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Image and Vision Computing},
	author = {Almeida, P.d.},
	year = {2010},
	note = {Publisher: Elsevier Ltd},
	keywords = {Computer-based solutions, Decision rules, Direct encoding, Expert systems, Human eye, Image analysis, Image preprocessing, Intelligent image analysis, Iris location, Iris segmentation, Knowledge base, Knowledge-based approach, Location, Processing problems, Training image},
	pages = {238--245},
}

@article{al-hunaiyyan_cognitive_2020,
	title = {A cognitive knowledge-based model for an academic adaptive e-advising system},
	volume = {15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094319395&doi=10.28945%2f4633&partnerID=40&md5=84ea64e0da84d20af0a60712eaba5170},
	doi = {10.28945/4633},
	abstract = {Aim/Purpose This study describes a conceptual model, based on the principles of concept algebra that can provide intelligent academic advice using adaptive, knowledge-based feedback. The proposed model advises students based on their traits and academic history. The system aims to deliver adaptive advice to students using historical data from previous and current students. This data-driven approach utilizes a cognitive knowledge-based (CKB) model to update the weights (values that indicate the strength of relationships between concepts) that exist between student's performances and recommended courses. Background A research study conducted at the Public Authority for Applied Education and Training (PAAET), a higher education institution in Kuwait, indicates that students' have positive perceptions of the e-Advising system. Most stu-dents believe that PAAET's e-Advising system is effective because it allows them to check their academic status, provides a clear vision of their academic timeline, and is a convenient, user-friendly, and attractive online service. Stu-dent advising can be a tedious element of academic life but is necessary to fill the gap between student performance and degree requirements. Higher edu-cation institutions have prioritized assisting undecided students with career decisions for decades. An important feature of e-Advising systems is person-alized feedback, where tailored advice is provided based on students' charac-teristics and other external parameters. Previous e-Advising systems provide students with advice without taking into consideration their different attrib-utes and goals. Methodology This research describes a model for an e-Advising system that enables stu-dents to select courses recommended based on their personalities and aca-demic performance. Three algorithms are used to provide students with adaptive course selection advice: The knowledge elicitation algorithm that rep-resents students' personalities and academic information, the knowledge bonding algorithm that combines related concepts or ideas within the knowledge base, and the adaptive e-Advising model that recommends rele-vant courses. The knowledge elicitation algorithm acquires student and aca-demic characteristics from data provided, while the knowledge bonding algo-rithm fuses the newly acquired features with existing information in the data-base. The adaptive e-Advising algorithm provides recommended courses to students based on existing cognitive knowledge to overcome the issues asso-ciated with traditional knowledge representation methods. Contribution The design and implementation of an adaptive e-Advising system are chal-lenging because it relies on both academic and student traits. A model that incorporates the conceptual interaction between the various academic and student-specific components is needed to manage these challenges. While other e-Advising systems provide students with general advice, these earlier models are too rudimentary to take student characteristics (e.g., knowledge level, learning style, performance, demographics) into consideration. For the online systems that have replaced face-to-face academic advising to be effec-tive, they need to take into consideration the dynamic nature of contempo-rary students and academic settings. Findings The proposed algorithms can accommodate a highly diverse student body by providing information tailored to each student. The academic and student el-ements are represented as an Object-Attribute-Relationship (OAR) model. Recommendations for Practitioners The model proposed here provides insight into the potential relationships be-tween students' characteristics and their academic standing. Furthermore, this novel e-Advising system provides large quantities of data and a platform through which to query students, which should enable developing more ef-fective, knowledge-based approaches to academic advising. Recommendation for Researchers The proposed model provides researches with a framework to incorporate various academic and student characteristics to determine the optimal advi-sory factors that affect a student's performance. Impact on Society The proposed model will benefit e-Advising system developers in imple-menting updateable algorithms that can be tested and improved to provide adaptive advice to students. The proposed approach can provide new insight to advisors on possible relationships between student's characteristics and current academic settings. Thus, providing a means to develop new curricu-lums and approaches to learning. Future Research In future studies, the proposed algorithms will be implemented, and the adaptive e-Advising model will be tested on real-world data and then further improved to cater to specific academic settings. The proposed model will benefit e-Advising system developers in implementing updateable algorithms that can be tested and improved to provide adaptive advisory to students. The approach proposed can provide new insight to advisors on possible rela-tionships between student's characteristics and current academic settings. Thus, providing a means to develop new curriculums and approaches to course recommendation. © 2020 Informing Science Institute. All rights reserved.},
	journal = {Interdisciplinary Journal of Information, Knowledge, and Management},
	author = {Al-Hunaiyyan, A. and Bimba, A.T. and Al-Sharhan, S.},
	year = {2020},
	note = {Publisher: Informing Science Institute},
	keywords = {Approaches to learning, Design and implementations, Education and training, Education computing, Higher education institutions, Knowledge based systems, Knowledge management, Knowledge representation, Knowledge-based approach, Online systems, Search engines, Student characteristics, Student's characteristics, Students, Traditional knowledge},
	pages = {247--263},
}

@article{bhansali_automatic_1989,
	title = {Automatic synthesis of greedy programs},
	volume = {1095},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958482537&doi=10.1117%2f12.969286&partnerID=40&md5=e3168e5bd56298c12a7d8ea2a1542f83},
	doi = {10.1117/12.969286},
	abstract = {This paper describes a knowledge based approach to automatically generate Lisp programs using the Greedy method of algorithm design. The system’s knowledge base is composed of heuristics for recognizing problems amenable to the Greedy method and knowledge about the Greedy strategy itself (i.e., rules for local optimization, constraint satisfaction, candidate ordering and candidate selection). The system has been able to generate programs for a wide variety of problems including the job-scheduling problem, the 0-1 knapsack problem, the minimal spanning tree problem, and the problem of arranging files on tape to mini- mize access time. For the special class of problems called matroids, the synthesized program provides optimal solutions, whereas for most other problems the solutions are near-optimal. © 1989 SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Bhansali, S. and Miriyala, K. and Harandi, M.T.},
	year = {1989},
	keywords = {0-1 knapsack problem, Artificial intelligence, Automatic synthesis, Candidate selection, Combinatorial optimization, Constraint Satisfaction, Constraint satisfaction problems, Heuristic methods, Job scheduling problem, Knowledge based systems, Knowledge-based approach, LISP (programming language), Local optimizations, Minimal spanning tree, Optimization},
	pages = {360--371},
}

@article{biswas_thesaurus_1987,
	title = {Thesaurus building with transitive closures for kadre},
	volume = {786},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957495859&doi=10.1117%2f12.940652&partnerID=40&md5=b99abf702a96a2bf316e4bbf5c8c9735},
	doi = {10.1117/12.940652},
	abstract = {Currently there is a big thrust towards the development of automated, user-friendly online information retrieval systems, which are software packages that allow a user population to query and receive appropriate information that is stored in a computer database. Our research adopts a knowledge based approach to the design and development of KADRE (Knowledge Assisted Document Retrieval Expert), an experimental online document retrieval system. Knowledge based techniques allow us to incorporate some of an expert librarian’s heuristic techniques into the retrieval process. We present the mathematical model of the retrieval system as a five step process and discuss the system thesaurus in some detail. Six different transitive closures of the relational matrix of term pairs are used to compute the thesaurus, and the retrieval output produced by these techniques are compared. Representation of the thesaurus as a transitive closure offers two very significant advantages: this method provides a means for completion of partial knowledge which is represented as numerical relational data; and the ensuing completion has a well defined property of (mathematical) consistency. © 1987 SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Biswas, G. and Bezdek, J.C. and Huang, L.-Y.},
	year = {1987},
	keywords = {Artificial intelligence, Design and Development, Document Retrieval, Heuristic methods, Heuristic techniques, Information retrieval, Information retrieval systems, Knowledge based systems, Knowledge-assisted, Knowledge-based approach, Numerical methods, On-line documents, Online information retrieval, Online systems, Query processing, Search engines, Thesauri, Transitive closure},
	pages = {432--438},
}

@article{ourston_arm_1987,
	title = {The {ARM} {Editor}: {A} {Knowledge}-{Based} {Approach} to {Software} {Documentation} {Support}},
	volume = {786},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957503507&doi=10.1117%2f12.940632&partnerID=40&md5=71fd40ea5f537748f6816f9bd8c04c62},
	doi = {10.1117/12.940632},
	abstract = {The Adaptive Response Model (ARM) Editor, a research prototype, is a knowledge-based system that supports the definition and manipulation of documents used in software development. This system represents a new approach to software documentation support in that knowledge about documentation is used to facilitate the automatic production of software documents. Most previous work has been on structure editor systems that allow manipulation of document parts as objects, or on user-embedded code systems, such as formatting systems, requirements traceability systems, and document generator systems. These latter systems work only if the proper codes have been embedded. © 1987 SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Ourston, D. and McBeth, R.W.},
	year = {1987},
	keywords = {ARM processors, Adaptive response, Artificial intelligence, Automatic production, Embedded systems, Generator systems, Knowledge based systems, Knowledge-based approach, Requirements traceability, Research prototype, Software design, Software documentation, Software prototyping, Structure editors},
	pages = {288--295},
}

@article{gutierrez_querying_2004,
	title = {Querying {Heterogeneous} {Spatial} {Databases}: {Combining} an {Ontology} with {Similarity} {Functions}},
	volume = {3289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048854727&doi=10.1007%2f978-3-540-30466-1_15&partnerID=40&md5=987b0ca7ee0a1d3c7b158f9fe2f78e0d},
	doi = {10.1007/978-3-540-30466-1_15},
	abstract = {This paper uses a knowledge-based approach to querying heterogeneous spatial databases based on an ontology and conceptual and attribute similarities. The ontology, which may be independent of the databases, expands and filters a user query. Then, queries are translated into a formal specification of entity classes, which are compared against definitions in databases. This process is carried out by determining the conceptual similarity between entities in a user ontology and by comparing these entities in the ontology with entities in the conceptual models of databases. In addition, the specification of a query is done not only by identifying entity classes but also by considering constraints based on attribute values. The paper describes the system architecture and presents a case study with data from a forestry information system. © Springer-Verlag 2004.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Gutiérrez, M. and Rodríguez, A.},
	year = {2004},
	note = {Publisher: Springer Verlag},
	keywords = {And filters, Attribute similarity, Attribute values, Conceptual model, Database systems, Formal specification, Knowledge based systems, Knowledge-based approach, Similarity functions, Spatial database, Specifications, System architectures},
	pages = {160--171},
}

@article{altinisik_isolating_2012,
	title = {Isolating non-predefined sensor faults by using farthest first traversal algorithm},
	volume = {51},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865118827&doi=10.1021%2fie201850k&partnerID=40&md5=295b5fb4378cc79b2850cc64e4187622},
	doi = {10.1021/ie201850k},
	abstract = {In this study, we propose a knowledge-based approach for detection and isolation of predefined and nonpredefined sensor faults in fault tolerant control (FTC) of a three-tank system. Farthest first traversal algorithm (FFTA) of data mining is used for the first time for the classification of faults in a FTC system. Predefining here means that features of a fault and its effects are known before the fault is seen on the system. Therefore, if a predefined fault is detected on the system, it is isolated into a known fault cluster and predefined action for that cluster can be taken to tolerate the fault. However, in a working system, there may be some other faults, which are not predefined. Those may be inaccurately isolated into available known clusters, since the clusters are determined according to predefined faults instead of non-predefined ones. In our work, we also propose a method for isolating the non-predefined faults that rearranges the clusters of predefined faults, online. In order to show the efficiency of proposed method, seven predefined and thirteen non-predefined fault scenarios are applied to a closed-loop FTC system. While three of the non-predefined faults are not accurately isolated without the proposed method, all of the faults are isolated correctly with the proposed method. © 2012 American Chemical Society.},
	number = {32},
	journal = {Industrial and Engineering Chemistry Research},
	author = {Altinisik, U. and Yildirim, M. and Erkan, K.},
	year = {2012},
	keywords = {Algorithms, Closed-loop, Data mining, Fault scenarios, Fault tolerant control, Knowledge based systems, Knowledge-based approach, Sensor fault, Sensors, Traversal algorithms, Working systems},
	pages = {10641--10648},
}

@article{chernyshov_knowledge_2018,
	title = {Knowledge {Based} {Identification} of {Non}-{Linear} {Systems}},
	volume = {51},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052902076&doi=10.1016%2fj.ifacol.2018.07.125&partnerID=40&md5=2bf26ca0cc489bbb5988afd93f0ff80c},
	doi = {10.1016/j.ifacol.2018.07.125},
	abstract = {In order to model uncertainty involved into a real-world process, a general approach combining both analytical and expert knowledge-based techniques to identify a multi-input multi-output non-linear system model is proposed. The approach leads to using a new type of stochastic dependence of random processes, involving corresponding expert knowledge, and which is an extension of the dispersion identification technique based on a new – expert knowledge-based – measure of stochastic dependence. The paper is preceded with a state-of-the art analysis of available knowledge-based identification and control approaches. © 2018},
	number = {6},
	journal = {IFAC-PapersOnLine},
	author = {Chernyshov, K.R.},
	year = {2018},
	note = {Publisher: Elsevier B.V.},
	keywords = {Control theory, Dispersion function, Dispersions, Expert knowledge, Knowledge based systems, Knowledge-based approach, Linear systems, MIMO systems, Maximum entropy principle, Non-parametric identification, Nonlinear systems, Random processes, Stochastic systems, Uncertainty analysis},
	pages = {30--35},
}

@article{sui_content_2016,
	title = {Content design of a micro video curriculum in the era of scattered learning},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991449454&doi=10.5013%2fIJSSST.a.17.11.04&partnerID=40&md5=701cea0a8f306146edc28ef2d76be2db},
	doi = {10.5013/IJSSST.a.17.11.04},
	abstract = {In this paper, the author investigated the content design of a micro video curriculum in the era of scattered learning. This paper puts forward a design which is combined with student-centered micro design and the idea of integral design. Finally, the paper discusses the design process of micro-video course and put forward a design framework from the macro-content based and micro-instruction based level and combines it with theories of education, psychology, technology, arts and social context. This paper also analyzed the rationales to decompose the design of a micro-video course and the knowledge-based approaches of decomposition and relation design. © 2016, UK Simulation Society. All rights reserved.},
	number = {11},
	journal = {International Journal of Simulation: Systems, Science and Technology},
	author = {Sui, Y.},
	year = {2016},
	note = {Publisher: UK Simulation Society},
	keywords = {Content design, Curricula, Design, Design frameworks, Design process, Integral designs, Knowledge based systems, Knowledge-based approach, Micro video, Relation design, Scattered learning},
	pages = {4.1--4.1},
}

@article{de_araujo_applying_2019,
	title = {Applying computational intelligence techniques to improve the decision making of business game players},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053627982&doi=10.1007%2fs00500-018-3475-4&partnerID=40&md5=64207e8b2dcfd3a5dc3114052b959135},
	doi = {10.1007/s00500-018-3475-4},
	abstract = {Business games have been widely used as differentiated pedagogical tools to provide experiential learning for business students. However, a critical problem with these tools is the issue of how to give feedback to students during the runtime of the simulation, especially in view of the high number of players involved in the game and the large amount of data generated in the simulations. In this scenario, intelligent mechanisms are desirable to make knowledge-based inferences, providing information which can assist both the players and the instructors facilitating the gaming process. In this work, we present an innovative knowledge-based approach focused on business games. Firstly, we apply data mining techniques to identify the behavioral patterns of players, based on their previous decisions stored in the database of a business game called business management simulator (BMS) that is used as a support tool for teaching concepts of production management, sales and business strategies. Secondly, based on these patterns, we develop a fuzzy inference system (FIS) to predict players’ performance based on their decisions in the game. Experimental results from a comparison of the real performance of players with the performance calculated by the proposed FIS show that this approach is very useful in the business game analyzed here, since it can help students during the simulation runtime, allowing them to improve their decisions. It is also clear that the proposed approach can be easily adapted to other business games, and particularly those with a similar structure to that of BMS. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.},
	number = {18},
	journal = {Soft Computing},
	author = {de Araújo, S.A. and de Barros, D.F., Jr. and da Silva, E.M. and Cardoso, M.V.},
	year = {2019},
	note = {Publisher: Springer Verlag},
	keywords = {Artificial intelligence, Business games, Computational intelligence techniques, Data mining, Decision making, Decision trees, Experiential learning, Feedback to students, Fuzzy Inference systems (FIS), Fuzzy inference, Fuzzy logic, Intelligent mechanisms, Knowledge based systems, Knowledge-based approach, Production management, Students, Teaching, Trees (mathematics)},
	pages = {8753--8763},
}

@article{zaretalab_extended_2019,
	title = {An {Extended} {Simulated} {Annealing} {Based} on the {Memory} {Structure} to {Solve} {Redundancy} {Allocation} {Problem}},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075350226&doi=10.1142%2fS0219686719500288&partnerID=40&md5=41b93671760a5ea55b861d494fb2ec6f},
	doi = {10.1142/S0219686719500288},
	abstract = {One of the most practical optimization problems in the reliability field is the redundancy allocation problem (RAP). This problem optimizes the reliability of a system by adding redundant components to subsystems under some constraints. In recent years, various meta-heuristic algorithms applied to find a local or global optimum solution for RAP in which redundancy strategies are chosen. Among these algorithms, simulated annealing algorithm (SA) is a capable one and makes use of a mathematical analogue to the physical annealing process to finding the global optimum. In this paper, we present a new simulated annealing algorithm named knowledge-based simulated annealing (KBSA) to solve RAP for the series-parallel system when the redundancy strategy can be chosen for individual subsystems. In the KBSA algorithm, the SA part searches the solution space to find good solutions and knowledge model saves the knowledge of good solution and feed it back to the algorithm. In this paper, this approach achieves the optimal result for some instances in the literature. In order to evaluate the performance of the proposed algorithm, it is compared with well-known algorithms in the literature for different test problems. Finally, the results illustrate that the proposed algorithm has a good proficiency in obtaining desired results. © 2019 World Scientific Publishing Company.},
	number = {4},
	journal = {Journal of Advanced Manufacturing Systems},
	author = {Zaretalab, A. and Hajipour, V.},
	year = {2019},
	note = {Publisher: World Scientific Publishing Co. Pte Ltd},
	keywords = {Extended simulated annealing, Global optimum solutions, Heuristic algorithms, Knowledge based systems, Knowledge-based approach, Meta heuristic algorithm, Redundancy, Redundancy allocation problem, Redundancy allocation problem (RAP), Reliability, Series-parallel system, Simulated annealing, Simulated annealing algorithms},
	pages = {527--548},
}

@article{martinikorena_low_2020,
	title = {Low {Cost} {Gaze} {Estimation}: {Knowledge}-{Based} {Solutions}},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078305098&doi=10.1109%2fTIP.2019.2946452&partnerID=40&md5=d7acc940f9a89d2c8b7d7e15537b1f2a},
	doi = {10.1109/TIP.2019.2946452},
	abstract = {Eye tracking technology in low resolution scenarios is not a completely solved issue to date. The possibility of using eye tracking in a mobile gadget is a challenging objective that would permit to spread this technology to non-explored fields. In this paper, a knowledge based approach is presented to solve gaze estimation in low resolution settings. The understanding of the high resolution paradigm permits to propose alternative models to solve gaze estimation. In this manner, three models are presented: a geometrical model, an interpolation model and a compound model, as solutions for gaze estimation for remote low resolution systems. Since this work considers head position essential to improve gaze accuracy, a method for head pose estimation is also proposed. The methods are validated in an optimal framework, I2Head database, which combines head and gaze data. The experimental validation of the models demonstrates their sensitivity to image processing inaccuracies, critical in the case of the geometrical model. Static and extreme movement scenarios are analyzed showing the higher robustness of compound and geometrical models in the presence of user's displacement. Accuracy values of about 3° have been obtained, increasing to values close to 5° in extreme displacement settings, results fully comparable with the state-of-the-art. © 1992-2012 IEEE.},
	journal = {IEEE Transactions on Image Processing},
	author = {Martinikorena, I. and Larumbe-Bergera, A. and Ariz, M. and Porta, S. and Cabeza, R. and Villanueva, A.},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Costs, Experimental validations, Eye tracking, Eye tracking technologies, Gaze estimation, Geometrical modeling, Geometrical models, Geometry, Head Pose Estimation, Image processing, Knowledge based systems, Knowledge-based approach, Low resolution},
	pages = {2328--2343},
}

@article{belkadi_knowledge-based_2019,
	title = {Knowledge-based platform for traceability and simulation monitoring applied to design of experiments process: an open source architecture},
	volume = {30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073490657&doi=10.1080%2f09544828.2019.1642463&partnerID=40&md5=6c95217ea30f8dc73d9aa0d89ca23a5a},
	doi = {10.1080/09544828.2019.1642463},
	abstract = {In the context of hard competitiveness, companies have to elaborate continuous innovation strategy enabling the proposition of new products with a shorter time to market. To do so, engineers have to propose several solutions at the same time and to test them as faster as possible to reduce the whole development project time. In this context, the improvement of simulation process on both technical and methodological aspects is challenging. Design of experiments (DoE) is the science to design, organise and optimise a set of simulations and experimentations for DOE helps to reach the target analysis objectives with a minimum of resource and time. This paper proposes a simulation data management framework combined with a knowledge-based approach as an open source solution for the management of DoE process. Named SDM4DoE, the aim of this framework is to improve the performance of the engineering process through traceability, decision-making assistance and automatic monitoring of the whole computation chain. © 2019, © 2019 Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {8-9},
	journal = {Journal of Engineering Design},
	author = {Belkadi, F. and Le Duigou, J. and Dall’Olio, L. and Besombes, G. and Bernard, A.},
	year = {2019},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Continuous innovation, Decision making, Design of experiments, DoE configuration, Information management, Knowledge based systems, Knowledge-based approach, Methodological aspects, Open Data, Open source architecture, Open-source solutions, Simulation data managements, Simulation platform, traceability},
	pages = {311--335},
}

@article{huang_towards_2020,
	title = {Towards {Knowledge}-{Based} {Geospatial} {Data} {Integration} and {Visualization}: {A} {Case} of {Visualizing} {Urban} {Bicycling} {Suitability}},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085092410&doi=10.1109%2fACCESS.2020.2992023&partnerID=40&md5=0d7682a43abdcd0a4778ab0fac235852},
	doi = {10.1109/ACCESS.2020.2992023},
	abstract = {Geospatial information plays an indispensable role in various interdisciplinary and spatially informed analyses. However, the use of geospatial information often entails many semantic intricacies relating to, among other issues, data integration and visualization. For the integration of data from different domains, merely using ontologies is inadequate for handling subtle and complex semantic relations raised by the multiple representations of geospatial data, as the domains have different conceptual views for modelling the geographic space. In addition, for geospatial data visualization - one of the most predominant ways of utilizing geospatial information - semantic intricacies arise as the visualization knowledge is difficult to interpret and utilize by non-geospatial experts. In this paper, we propose a knowledge-based approach using semantic technologies (coupling ontologies, semantic constraints, and semantic rules) to facilitate geospatial data integration and visualization. A traffic spatially informed study is developed as a case study: visualizing urban bicycling suitability. In the case study, we complement ontologies with semantic constraints for cross-domain data integration. In addition, we utilize ontologies and semantic rules to formalize geospatial data analysis and visualization knowledge at different abstraction levels, which enables machines to infer visualization means for geospatial data. The results demonstrate that the proposed framework can effectively handle subtle cross-domain semantic relations for data integration, and empower machines to derive satisfactory visualization results. The approach can facilitate the sharing and outreach of geospatial data and knowledge for various spatially informed studies. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Huang, W. and Kazemzadeh, K. and Mansourian, A. and Harrie, L.},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Data Sharing, Data integration, Data visualization, Geo-spatial data visualizations, Geo-spatial informations, Geospatial data integration, Knowledge based systems, Knowledge-based approach, Multiple representation, Ontology, Semantic constraints, Semantic technologies, Semantics, Visualization, Visualization results},
	pages = {85473--85489},
}

@article{rega_knowledge-based_2020,
	title = {A knowledge-based approach to the layout optimization of human–robot collaborative workplace},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093839341&doi=10.1007%2fs12008-020-00742-0&partnerID=40&md5=84d1d50e8730e2abc8c5407f304d8ef3},
	doi = {10.1007/s12008-020-00742-0},
	abstract = {Human–robot collaboration (HRC) solutions are replacing classic industrial robot due to the possibility of realizing more flexible production systems. Collaborative robot systems, named cobot, can work side by side with humans combining their strengths. However, obtaining an efficient HRC is not trivial; indeed, the potential advantages of the collaborative robotics increase as complexity increases. In this context, the main challenge is to design the layout of collaborative workplaces facing the facility layout problem and ensuring the safety of the human being. To move through the high number of safety standards could be very tiring and unproductive. Therefore, in this work a list of key elements, linked to reference norms and production needs, characterizing the collaborative workplace has been identified. Then, a graph-based approach has been used in order to organize and easily manage this information. The management by means graphs has facilitated the implementation of the acquired knowledge in a code, developed in Matlab environment. This code aims to help the designer in the layout organization of human–robot collaborative workplaces in standards compliance. The paper presents the optimization code, named Smart Positioner, and the operation is explained through a workflow diagram. © 2020, The Author(s).},
	journal = {International Journal on Interactive Design and Manufacturing},
	author = {Rega, A. and Vitolo, F. and Di Marino, C. and Patalano, S.},
	year = {2020},
	note = {Publisher: Springer-Verlag Italia s.r.l.},
	keywords = {Collaborative robots, Facility layout problems, Flexible production systems, Graphic methods, Industrial robots, Knowledge based systems, Knowledge-based approach, Layout optimization, MATLAB environment, Optimization code, Plant layout, Regulatory compliance, Social robots, Workflow diagrams},
}

@article{plaza_knowledge_2020,
	title = {Knowledge based approach to ground refuelling optimization of commercial airplanes},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090065931&doi=10.1111%2fexsy.12631&partnerID=40&md5=40173d54dad756ed83c9a11158f64245},
	doi = {10.1111/exsy.12631},
	abstract = {This work aims to establish a general and optimized procedure for the initial refuelling of commercial airplanes, as this loading process is strongly related to safety and energy saving issues. The on-ground refuelling is addressed as an optimization problem whose cost function involves expert knowledge about constraints and factors that influence the aircraft stability and performance. Several heterogeneous criteria (fuelling time, structural load, flow transfers, etc.) have been considered and weighted accordance to its importance in terms of stability. This allows us to adapt the strategy to any type and planned trip of the airplane. The priority is the positioning of the centre of mass of the civil aircraft within safety and manoeuvrability margins, and near the optimal position. Evolutive algorithms are applied, keeping feasible solutions by modifying genetic operators. As a case of study, the initial refuelling of a long range type commercial aircraft, the Airbus A330-200, is analysed. Simulation results have proved this methodology to be efficient and optimal. Even more, this heuristic and general approach improves the traditional solution that follows a set of pre-defined rules that are specific for each type of aircraft. © 2020 John Wiley \& Sons Ltd},
	journal = {Expert Systems},
	author = {Plaza, E. and Santos, M.},
	year = {2020},
	note = {Publisher: Blackwell Publishing Ltd},
	keywords = {Aircraft, Aircraft stability, Commercial aircraft, Commercial airplane, Cost functions, Energy conservation, Evolutive algorithms, Feasible solution, Genetic algorithms, Genetic operators, Knowledge based systems, Knowledge-based approach, Optimization problems, Stability criteria},
}

@article{zarnoufi_machine_2020,
	title = {Machine {Normalization}: {Bringing} {Social} {Media} {Text} from {Non}-{Standard} to {Standard} {Form}},
	volume = {19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091824969&doi=10.1145%2f3378414&partnerID=40&md5=befab00bedbf4006096d2daae9ca5b58},
	doi = {10.1145/3378414},
	abstract = {User-generated text in social media communication (SMC) is mainly characterized by non-standard form. It may contain code switching (CS) text, a widespread phenomenon in SMC, in addition to noisy elements used, especially in written conversations (use of abbreviations, symbols, emoticons) or misspelled words. All of these factors constitute a wall in front of text mining applications. Common text mining tools are dedicated to standard use of standard languages but cannot deal with other forms, especially written text in social media. To overcome these problems, in this work we present our solution for the normalization of non-standard use of standard and non-standard languages (dialects) in SMC text with the use of existent resources and tools. The main processing in our solution consists of CS normalization from multiple to one language by the use of a machine translation - like approach. This processing relies on a linguistic approach of CS, which aims at identifying automatically the translation source and target languages (without human intervention). The remaining processing operations concern the normalization of SMC special expressions and spelling correction of out-of-vocabulary words. To preserve the coded-switched sentence meaning across translation, we adopt a knowledge-based approach for word sense translation disambiguation reinforced with a multi-lingual vertical context. All of these processes are embedded in what we refer to as the machine normalization system. Our solution can be used as a front-end of text mining processing, enabling the analysis of SMC noisy text. The conducted experiments show that our system performs better than considered baselines. © 2020 ACM.},
	number = {4},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	author = {Zarnoufi, R. and Jaafar, H. and Abik, M.},
	year = {2020},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Computer aided language translation, Human intervention, Knowledge based systems, Knowledge-based approach, Linguistic approach, Linguistics, Machine translations, Out of vocabulary words, Processing operations, Social networking (online), Spelling correction, Text mining, Translation disambiguation, Turing machines},
}

@article{zhu_ipnhot_2020,
	title = {{IPNHOT}: {A} knowledge-based approach for identifying protein-nucleic acid interaction hot spots},
	volume = {21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087678415&doi=10.1186%2fs12859-020-03636-w&partnerID=40&md5=75d1e7a41c709b3c52ed82f42202201d},
	doi = {10.1186/s12859-020-03636-w},
	abstract = {Background: The interaction between proteins and nucleic acids plays pivotal roles in various biological processes such as transcription, translation, and gene regulation. Hot spots are a small set of residues that contribute most to the binding affinity of a protein-nucleic acid interaction. Compared to the extensive studies of the hot spots on protein-protein interfaces, the hot spot residues within protein-nucleic acids interfaces remain less well-studied, in part because mutagenesis data for protein-nucleic acids interaction are not as abundant as that for protein-protein interactions. Results: In this study, we built a new computational model, iPNHOT, to effectively predict hot spot residues on protein-nucleic acids interfaces. One training data set and an independent test set were collected from dbAMEPNI and some recent literature, respectively. To build our model, we generated 97 different sequential and structural features and used a two-step strategy to select the relevant features. The final model was built based only on 7 features using a support vector machine (SVM). The features include two unique features such as ΔSASsa1/2 and esp3, which are newly proposed in this study. Based on the cross validation results, our model gave F1 score and AUROC as 0.725 and 0.807 on the subset collected from ProNIT, respectively, compared to 0.407 and 0.670 of mCSM-NA, a state-of-the art model to predict the thermodynamic effects of protein-nucleic acid interaction. The iPNHOT model was further tested on the independent test set, which showed that our model outperformed other methods. Conclusion: In this study, by collecting data from a recently published database dbAMEPNI, we proposed a new model, iPNHOT, to predict hotspots on both protein-DNA and protein-RNA interfaces. The results show that our model outperforms the existing state-of-art models. Our model is available for users through a webserver: http://zhulab.ahu.edu.cn/iPNHOT/. © 2020 The Author(s).},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Zhu, X. and Liu, L. and He, J. and Fang, T. and Xiong, Y. and Mitchell, J.C.},
	year = {2020},
	note = {Publisher: BioMed Central},
	keywords = {Binding energy, Biomolecules, Computational model, Forecasting, Humans, Knowledge based systems, Knowledge-based approach, Nucleic acids, Protein Interaction Mapping, Protein-nucleic acid interaction, Protein-nucleic acids, Protein-protein interactions, Protein-protein interface, Proteins, Statistical tests, Support vector machines, Thermodynamic effect, Training data sets, Transcription, chemistry, human, procedures, protein, protein analysis},
}

@article{rana_multi-level_2020,
	title = {Multi-level knowledge-based approach for implicit aspect identification},
	volume = {50},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088824502&doi=10.1007%2fs10489-020-01817-x&partnerID=40&md5=a8af2898270d0b249c22b9d12c143b31},
	doi = {10.1007/s10489-020-01817-x},
	abstract = {Sentiment analysis or opinion mining is the area of research in Natural Language Processing (NLP) and text mining which deals with the systematic identification of subjective information from user generated text. At a more fine-grained level, aspect-based sentiment analysis focuses on the targets of users’ opinions and determining sentiment orientation of these opinions. Among different tasks of aspect-based sentiment analysis, aspect extraction is the key task which includes extraction of both explicit and implicit aspects. Due to the complexity of implicit aspects, not much effort has been put forward to solve the problem while explicit aspects have been studied extensively in the recent past. Existing approaches for implicit aspect extraction have focused on specific type of aspects and have neglected the actual problem. Therefore, in this paper, we have proposed a multi-level approach which identifies implicit aspects using co-occurrence and similarity-based techniques. This research focuses on the extraction of clues for implicit targets of users’ opinions and identification of true targets of users’ opinions with the help of implicit aspect clues. The proposed approach is divided into two phases: first, several rules are crafted to identify clues for implicit aspects in a review sentence. Secondly, aspects are assigned on the basis of extracted clues using proposed multi-level approach. The proposed model can extract not only implicit aspect clues associated with opinion words but also allocate clues to opinion words where no association is identified. This helps to identify implicit aspects with or without co-occurrences of opinion words with explicit aspects. Experimental evaluation elaborates the importance of implicit aspect clues in the identification of targets of users’ opinions. The proposed approach shows better results as compared with the state-of-the-art approaches for the identification of implicit targets of users’ opinions on a dataset of different product reviews. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {12},
	journal = {Applied Intelligence},
	author = {Rana, T.A. and Cheah, Y.-N. and Rana, T.},
	year = {2020},
	note = {Publisher: Springer},
	keywords = {Aspect identifications, Experimental evaluation, Extraction, Knowledge based systems, Knowledge-based approach, NAtural language processing, Product reviews, Sentiment analysis, State-of-the-art approach, Subjective information, Systematic identification, Text mining},
	pages = {4616--4630},
}

@article{li_process_2020,
	title = {Process fault diagnosis with model- and knowledge-based approaches: {Advances} and opportunities},
	volume = {105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091737031&doi=10.1016%2fj.conengprac.2020.104637&partnerID=40&md5=43b7818838e858f50b2d8af4343469b0},
	doi = {10.1016/j.conengprac.2020.104637},
	abstract = {Fault diagnosis plays a vital role in ensuring safe and efficient operation of modern process plants. Despite the encouraging progress in its research, developing a reliable and interpretable diagnostic system remains a challenge. There is a consensus among many researchers that an appropriate modelling, representation and use of fundamental process knowledge might be the key to addressing this problem. Over the past four decades, different techniques have been proposed for this purpose. They use process knowledge from different sources, in different forms and on different details, and are also named model-based methods in some literature. This paper first briefly introduces the problem of fault detection and diagnosis, its research status and challenges. It then gives a review of widely used model- and knowledge-based diagnostic methods, including their general ideas, properties, and important developments. Afterwards, it summarises studies that evaluate their performance in real processes in process industry, including the process types, scales, considered faults, and performance. Finally, perspectives on challenges and potential opportunities are highlighted for future work. © 2020 Elsevier Ltd},
	journal = {Control Engineering Practice},
	author = {Li, W. and Li, H. and Gu, S. and Chen, T.},
	year = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Diagnostic methods, Diagnostic systems, Failure analysis, Fault detection, Fault detection and diagnosis, Knowledge based, Knowledge based systems, Knowledge-based approach, Model-based method, Process knowledge, Research status},
}

@article{zhao_knowledge-based_2020,
	title = {A knowledge-based approach for automatic quantification of epileptiform activity in children with electrical status epilepticus during sleep},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089407310&doi=10.1088%2f1741-2552%2faba6dd&partnerID=40&md5=3df0a5e2dd212742e0c8554900038d8e},
	doi = {10.1088/1741-2552/aba6dd},
	abstract = {Objective. Electrical status epilepticus during sleep (ESES), as electroencephalographic disturbances, is characterized by strong activation of epileptiform activity in the electroencephalogram during sleep. Quantitative descriptors of such epileptiform activity can support the diagnose and the prognosis of children with ESES. To quantify the epileptiform activity of ESES, a knowledge-based approach to mimic the clinical decision-making process is proposed. Approach. Firstly, a morphological operations-based scheme is designed to quickly locate the positive peaks/negative pits and roughly estimate the onset/offset of spike and slow-wave abnormalities. Then, to provide the accurate duration of ESES patterns, a set of rules for further adjusting these onsets/offsets are proposed by merging medical knowledge with a generalized threshold obtained from statistics. As such, the quantification is accomplished by evaluating the obtained spike and slow-wave abnormalities and their various durations. Main results. The effectiveness and feasibility of the proposed method were evaluated on a clinical dataset that collected at Children's Hospital of Fudan University, Shanghai, China. We demonstrate that the proposed method can recognize different types of spike and slow-wave abnormalities. The sensitivity, precision, and false positive rate achieved 91.96\%, 97.09\%, and 1.88 min-1, respectively. The estimation error for the spike-wave index was 2.32\%. Comparison results showed that our method outperforms the state-of-the-art. Significance. The quantification of spike and slow-waves provides information about ESES activity. The detection of variations types of spike and slow-waves improves the performance in the quantification of ESES. Experimental results suggest that the proposed method has great potential in automatic ESES quantification and can help improve the diagnosis and researches of epileptic encephalopathy with ESES. © 2020 IOP Publishing Ltd.},
	number = {4},
	journal = {Journal of Neural Engineering},
	author = {Zhao, X. and Wang, X. and Chen, C. and Fan, J. and Yu, X. and Wang, Z. and Akbarzadeh, S. and Li, Q. and Zhou, S. and Chen, W.},
	year = {2020},
	note = {Publisher: IOP Publishing Ltd},
	keywords = {Article, Automatic quantification, China, Clinical decision making, Decision making, Electroencephalography, Epileptiform activity, Estimation errors, False positive rates, Knowledge based systems, Knowledge-based approach, Mathematical morphology, Morphological operations, Sleep research, Status epilepticus, automation, clinical decision making, controlled study, electroencephalography, epileptic state, feature extraction, human, priority journal, sleep, slow wave sleep, spike wave},
}

@article{negri_high-resolution_2021,
	title = {High-resolution {GPR} survey for masonry wall diagnostics},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092132045&doi=10.1016%2fj.jobe.2020.101817&partnerID=40&md5=c5a56a529aec9bcfd5d7a8d642b59a70},
	doi = {10.1016/j.jobe.2020.101817},
	abstract = {Ground-penetrating Radar (GPR) is a non-invasive technique with increasing focus on civil-engineering applications. Referring to existing constructions, the most recent International and National Codes suggest a knowledge-based approach, which is mandatory before any structural assessment or design of interventions. The knowledge-based process includes experimental investigations aimed at the detailed geometrical and structural relief as well as to the evaluation of the materials mechanical properties. However, when dealing with built Cultural Heritage, destructive techniques should be limited or even forbidden. In this context, non-destructive investigations play a key role and their effectiveness should be further studied in relation to specific applications. To this end, Ground-penetrating Radar surveys were performed in the test site of the Laboratory of Applied Geophysics, the University of Salento, Lecce (Italy), with the aim of simulating various real-life practical applications. In particular, several objects of different materials and geometries were buried in the subsurface and a small building was built in the area using different construction techniques, among them, multi-leaf masonry walls, also called sack masonry walls, were constructed. The GPR method can detect both the presence of hidden bodies and different leafs of the walls with a relative efficiency depending on the field context, the dielectric properties of the host material, and the nature and size of the bodies. In this work, a test was planned to verify 900 and 2000 MHz antenna resolutions. The data acquired with the 2000 MHz antenna were used to estimate the mean electromagnetic wave propagation velocity in the sack masonry layers of known thickness and to understand if the inner core was filled with material other than air. The authors propose a high-resolution method to improve the velocity estimation using geometrical optics laws and the sign of the reflection coefficient in order to properly select the arrivals from different interfaces. © 2020 Elsevier Ltd},
	journal = {Journal of Building Engineering},
	author = {Negri, S. and Aiello, M.A.},
	year = {2021},
	note = {Publisher: Elsevier Ltd},
	keywords = {Air, Antennas, Civil engineering applications, Destructive techniques, Dielectric properties, Electromagnetic wave propagation, Experimental investigations, Geological surveys, Geometrical optics, Geometry, Geophysical prospecting, Ground penetrating radar (GPR), Ground penetrating radar survey, Ground penetrating radar systems, High-resolution methods, Knowledge based systems, Knowledge-based approach, Knowledge-based process, Masonry construction, Masonry materials, Petroleum reservoir evaluation, Retaining walls, Walls (structural partitions)},
}

@article{zhang_hybrid_2001,
	title = {Hybrid neural method for locating eyes in facial images},
	volume = {40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035485103&doi=10.1117%2f1.1391258&partnerID=40&md5=a973f10dd7d63d17ca063e720bdafa80},
	doi = {10.1117/1.1391258},
	abstract = {Locating facial features automatically in a scene is an essential but relatively unsolved problem. We develop a novel hybrid neural method for human eye location by the following stages: (1) facial images are preprocessed for normalization; (2) the candidate regions of eyes are detected using an improved version of radial basis function (RBF) networks, which is capable of dealing with those facial images containing some changes of eye size or face orientation; and (3) a hierarchical knowledge-based approach is presented to evaluate these candidates and locate the positions of both eyes. The proposed method is tested on various face images. The experimental results illustrate the effectiveness of our method. © 2001 Society of Photo-Optical Instrumentation Engineers.},
	number = {10},
	journal = {Optical Engineering},
	author = {Zhang, D. and Peng, H. and Wang, K.},
	year = {2001},
	keywords = {Backpropagation, Face recognition, Facial images, Feature extraction, Image reconstruction, Multilayer neural networks, Radial basis function networks},
	pages = {2151--2158},
}

@article{fogli_knowledge-based_2020,
	title = {A knowledge-based approach to hierarchical classification: {A} voting metaphor},
	volume = {161},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089375801&doi=10.1016%2fj.eswa.2020.113737&partnerID=40&md5=7cd6857ccc4d99f6feef1641aa7a3815},
	doi = {10.1016/j.eswa.2020.113737},
	abstract = {The paper proposes a new approach to hierarchical classification based on condition-action rules that represent expert knowledge in a given domain. The approach adopts a voting metaphor: each rule is regarded as a voter that expresses a preference for a given category to be assigned to an item to be classified; the category that receives more votes wins. Novel performance measures of hierarchical classifiers are also introduced that aim at overcoming the limitations of the current concepts of precision and recall. The proposed approach can be applied to any hierarchical classification task, for which expert knowledge is available. The viability of the approach and its performance are shown through a real-size application concerning the e-mail dispatching task inside a large public administration. The results obtained demonstrate that the proposed knowledge-based approach to hierarchical classification can reach a performance level comparable to that of human experts, if not even better. © 2020 Elsevier Ltd},
	journal = {Expert Systems with Applications},
	author = {Fogli, D. and Guida, G. and Redolfi, M. and Tonoli, R.},
	year = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Classification (of information), Condition-action rules, Expert knowledge, Hierarchical classification, Hierarchical classifiers, Knowledge based systems, Knowledge-based approach, Performance level, Performance measure, Precision and recall, Public administration},
}

@article{dalton_medical_1988,
	title = {Medical image matching},
	volume = {914},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0043000183&doi=10.1117%2f12.968667&partnerID=40&md5=a2a6612db4cb8baac31bac5d0d7cbe5b},
	doi = {10.1117/12.968667},
	abstract = {Improvements in the accuracy of repositioning patients in medical imaging systems during repeat examinations will allow a more precise measurement of the progress of the disease or treatment. The alignment task is normally carried out using a number of inadequate techniques varying from stereotatic frames, to a rigorous anatomical study of the organs shown in the different views. Present techniques are either unreliable or need total patient collaboration for the surgical implantation of a localising device. The paper describes a knowledge based approach, which will enable optimal matching to be achieved between the two data sets and will be able, at least in the case of MRI images, to provide the appropriate co-ordinates for an optimised new slice angle. An extension of the use of accurate repositioning would be the ability to cross match the different types of information from other imaging systems. The system will eventually be able to quantify the absolute difference between images subject to morphological change and temporal distortion. The method of approach uses an anatomical knowledge base to guide the segmentation of the scene into a number of clearly identified invariant key objects. The matching will proceed by iterating to progressively smaller features. Matching is carried out using symbolic feature spaced descriptions of the objects. Key words: Image matching, high level reasoning, stereotatic frames, MRI. © 1988, SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Dalton, B.L.},
	year = {1988},
	keywords = {Absolute difference, Anatomical studies, High-level reasoning, Image matching, Imaging systems, Knowledge based systems, Knowledge-based approach, Magnetic resonance imaging, Medical imaging, Morphological changes, Patient treatment, Precise measurements, Stereotatic frames, Temporal distortions},
	pages = {456--465},
}

@article{giusto_multisensorial_1989,
	title = {Multisensorial vision for autonomous vehicle driving},
	volume = {1095},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957502998&doi=10.1117%2f12.969360&partnerID=40&md5=584245e50bbd25b6eb26bdd5ee5b4aff},
	doi = {10.1117/12.969360},
	abstract = {A multisensorial vision system for autonomous vehicle driving is presented, that operates in outdoor natural environments. The system, currently under development in our laboratories, will be able to integrate data provided by different sensors in order to achieve a more reliable description of a scene and to meet safety requirements. We chose to perform a high-level symbolic fusion of the data to better accomplish the recognition task. A knowledge-based approach is followed, which provides a more accurate solution; in particular, it will be possible to integrate both physical data, furnished by each channel, and different fusion strategies, by using an appropriate control structure. The high complexity of data integration is reduced by acquiring, filtering, segmenting and extracting features from each sensor channel. Production rules, divided into groups according to specific goals, drive the fusion process, linking to a symbolic frame all the segmented regions characterized by similar properties. As a first application, road and obstacle detection is performed. A particular fusion strategy is tested that integrates results separately obtained by applying the recognition module to each different sensor according to the related model description. Preliminary results are very promising and confirm the validity of the proposed approach. Applications of Artificial Intelligence VII (1989). © 1989 SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Giusto, D.D. and Vernazza, G.},
	year = {1989},
	keywords = {Artificial intelligence, Autonomous vehicles, Control structure, Data integration, Digital storage, Extracting features, Knowledge based systems, Knowledge-based approach, Model description, Natural environments, Obstacle detection, Obstacle detectors, Safety requirements, Segmented regions},
	pages = {1118--1125},
}

@article{todorovski_using_2003,
	title = {Using domain specific knowledge for automated modeling},
	volume = {2810},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248820779&doi=10.1007%2f978-3-540-45231-7_5&partnerID=40&md5=40b06cc6a4734186bbf133b912bacd24},
	doi = {10.1007/978-3-540-45231-7_5},
	abstract = {In this paper, we present a knowledge based approach to automated modeling of dynamic systems based on equation discovery. The approach allows for integration of domain specific modeling knowledge in the process of modeling. A formalism for encoding knowledge is proposed that is accessible to mathematical modelers from the domain of use. Given a specification of an observed system, the encoded knowledge can be easily transformed into an operational form of grammar that specifies the space of candidate models of the observed system. Then, equation discovery method LAGRAMGE is used to search the space of candidate models and find the one that fits measured data best. The use of the automated modeling framework is illustrated on the example domain of population dynamics. The performance and robustness of the framework is evaluated on the task of reconstructing known models of a simple aquatic ecosystem. © Springer-Verlag Berlin Heidelberg 2003.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Todorovski, L. and Džeroski, S.},
	year = {2003},
	note = {Publisher: Springer Verlag},
	keywords = {Aquatic ecosystems, Automated modeling, Candidate models, Domain specific modeling, Domain-specific knowledge, Equation discovery, Knowledge based systems, Knowledge-based approach, Modeling of dynamics, Observed systems},
	pages = {48--59},
}

@article{pfennig_knowledge-based_2011,
	title = {A knowledge-based approach for design and modelling of high lift actuation systems},
	volume = {225},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79751517238&doi=10.1243%2f09544100JAERO857&partnerID=40&md5=61fa0e68ae7ea15e8f3aae6cd9d8b712},
	doi = {10.1243/09544100JAERO857},
	abstract = {High lift actuation system design is a highly iterative process. Especially the preliminary design phase, which sets the course for the following design steps, is characterized by uncertain and changing data input. Despite the importance of this early design phase, a specialized and integrated computational support is not available. The objective of the research project WissBaSys is to offer knowledge-based design assistance focused on the preliminary design of high lift actuation systems. Moreover, information concerning a specific drive system architecture and component data can be used for an automated model generation leading to a significant time reduction and enhanced traceability. A capable approach for order reduction of shaft transmission models, as well as the concept for computer-aided modelling, is presented in this article.},
	number = {3},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
	author = {Pfennig, M. and Thielecke, F.},
	year = {2011},
	keywords = {Actuation systems, Curricula, Data input, Design, Design steps, Drive systems, Early design phase, Flight control, Flight control systems, Flight simulators, Information concerning, Iterative process, Knowledge based systems, Knowledge-based approach, Model generation, Order reduction, Preliminary design, Preliminary design phase, Systems analysis, Systems engineering, Time reduction, Transmission model, high lift actuation system, modelling and simulation, system design},
	pages = {302--311},
}

@article{seifi_expressive_2011,
	title = {Expressive animated character sequences using knowledge-based painterly rendering},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855184783&doi=10.1155%2f2011%2f164949&partnerID=40&md5=647ac938e47d5e2a3178068bf002d822},
	doi = {10.1155/2011/164949},
	abstract = {We propose a technique to enhance emotional expressiveness in games and animations. Artists have used colors and painting techniques to convey emotions in their paintings for many years. Moreover, researchers have found that colors and line properties affect users' emotions. We propose using painterly rendering for character sequences in games and animations with a knowledge-based approach. This technique is especially useful for parametric facial sequences. We introduce two parametric authoring tools for animation and painterly rendering and a method to integrate them into a knowledge-based painterly rendering system. Furthermore, we present the results of a preliminary study on using this technique for facial expressions in still images. The results of the study show the effect of different color palettes on the intensity perceived for an emotion by users. The proposed technique can provide the animator with a depiction tool to enhance the emotional content of a character sequence in games and animations. Copyright © 2011 Hasti Seifi et al.},
	journal = {International Journal of Computer Games Technology},
	author = {Seifi, H. and Dipaola, S. and Arya, A.},
	year = {2011},
	keywords = {Animated characters, Animation, Authoring tool, Behavioral research, Color, Color palette, Facial Expressions, Facial sequences, Knowledge based systems, Knowledge-based approach, Paint, Painterly Rendering, Painting techniques, Still images},
}

@article{koch_simulating_2012,
	title = {Simulating the production of future marine products},
	volume = {59},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875340779&doi=10.1179%2fstr.2012.59.1.005&partnerID=40&md5=ad0e06a5f0173c568d07d687071fffff},
	doi = {10.1179/str.2012.59.1.005},
	abstract = {The paper describes a domain oriented and knowledge based approach that creates discrete event simulation models for ship production in a straightforward, descriptive way. It uses real-world objects recognised by engineers and planners. The approach is applied to production facilities as well as to products. Another challenge addressed is the simulation of incompletely designed products for future new-building programs. © 2012 Ship Technology Research Schiffstechnik.},
	number = {1},
	journal = {Ship Technology Research},
	author = {Koch, T.},
	year = {2012},
	note = {Publisher: University of Duisburg},
	keywords = {Building projects, Discrete event simulation, Domain-oriented, Knowledge based systems, Knowledge-based approach, Marine products, Models, Production facility, Real-world objects, Ship production, Ships, Shipyards, Simulation},
	pages = {42--48},
}

@article{javid_designer-assisted_2013,
	title = {A designer-assisted analog synthesis flow},
	volume = {233},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884578639&doi=10.1007%2f978-3-642-36329-0-6&partnerID=40&md5=90080e6bbcf9a6538724b9394a11aa41},
	doi = {10.1007/978-3-642-36329-0-6},
	abstract = {This chapter presents a designer-assisted analog synthesis flow that is fully controlled by the designer and offers an intuitive design approach. The designer knowledge to conceive an analog IP is the key element of the synthesis flow, it is taken into account to automatically generate the analog IP design procedure and the physical view. Thus both consistency and accuracy of the final design are ensured. The presented flow bridges the gap between the two traditional approaches related to analog synthesis, namely the simulation-based and the knowledge-based approaches. It combines accuracy from simulation-based approaches with speed of computation from knowledge-based approaches. The proposed analog synthesis flow is composed of an accurate hierarchical sizing and biasing tool and a parameterizable layout generation tool. To demonstrate the effectiveness of the proposed flow, a fully differential transconductor was completely synthesized in 130nm CMOS technology to respect some performance specifications set by the designer. The obtained very low runtime is due to the introduction of design knowledge during both sizing and layout generation. © Springer-Verlag Berlin Heidelberg 2013.},
	journal = {Lecture Notes in Electrical Engineering},
	author = {Javid, F. and Youssef, S. and Iskander, R. and Louërat, M.-M.},
	year = {2013},
	keywords = {Analog Synthesis, CMOS integrated circuits, Design, Design approaches, Design knowledge, Fully differential, Knowledge based systems, Knowledge-based approach, Layout generations, Performance specifications, Tools, Traditional approaches},
	pages = {123--148},
}

@article{dorbath_flexible_2014,
	title = {A {Flexible} wing modeling and physical mass estimation system for early aircraft design stages},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902815024&partnerID=40&md5=959f3ebe7a8486516c1f6d51440d1e3e},
	abstract = {A new developed extended physics-based wing mass estimation system is presented in this study. The tool chain is highly flexible in terms of geometries and therefore applicable for conventional and unconventional wings and novel structural layouts. Compared to state-off-the-art wing mass estimation tools, the physical modeling and therefore the physics-based mass estimation is extended beyond the wing primary structure. The developed mass estimation system includes the masses of the heaviest secondary structures, which are the flaps including flap tracks and carriages as well as the ailerons including their wing attachment. Additionally, the load-carrying structure of the engine pylon and the landing gear is included in the structural model, to take their influence on the wing structure into account. Besides the structural analysis and sizing capability, the tool chain consist of the load calculation modules for the aerodynamic loads, the distributed fuel pressure loads, the landing gear loads and the engine thrust loads. The tool chain includes a static aeroelastic loop, which updates the aerodynamic, fuel and engine loads with respect to the wing deformation. The structural wing model is based on finite element shell models. The load carrying structures of the flap tracks is modeled in detail. This enables the deflection of the flaps in the FE model and ensures realistic load paths in the tracks, which is mandatory for a realistic sizing. The aerodynamic load estimation is based on the vortex-lattice theory. The vortex-lattice model includes a representation of the slats and flaps that takes the fowler motion of the flaps into account. The validation of the vortex-lattice results is performed by comparing the vortex-lattice results with RANS results and shows a high accuracy. The estimation of the fuel loads on the wing is performed by a detailed modeling of the fuel in the wing, which allows the accurate computation of the distributed fuel pressure on the wing skins, ribs and spars. The ground forces on the landing gear and the engine thrust are considered as force vectors in the finite element model, while the engine itself is modeled as a mass point that is attached to the engine pylon. Beside the analysis tools, the developed multi-model generator ELWIS is the core element of the mass estimation system. The generation of complex analysis models, as in the WING mass tool chain, usually requires a huge amount of user-defined input parameters. This makes such models infeasible for preliminary aircraft design. Therefore, a knowledge-based approach is chosen for the ELWIS model generator. This means, that a large amount of engineering rules are implemented in ELWIS, wherefore the model generation process is performed automatically based on few user-friendly input parameters. The data model CPACS is chosen as input format. Therefore, the WINGmass tool chain is easy to integrate in a wider aircraft design environment, which is favorable for analysis tools of the preliminary design phase. The mass output of the WINGmass tool chain is calibrated with respect to two reference aircraft: the Airbus A320 and the Airbus A340-200. The calibration shows very small differences between the calibrated results and the reference aircraft, which shows the high accuracy of the tool chain.},
	number = {5},
	journal = {DLR Deutsches Zentrum fur Luft- und Raumfahrt e.V. - Forschungsberichte},
	author = {Dorbath, F.},
	year = {2014},
	note = {Publisher: Deutschen Forschungsanstalt fur Luft-und Raumfahrt},
	keywords = {Accurate computations, Aerodynamic loads, Aerodynamics, Aeroelasticity, Aircraft, Calibration, Chains, Engines, Finite element method, Flaps, Flexible wings, Fuels, Knowledge based systems, Knowledge-based approach, Landing gear (aircraft), Load-carrying structure, Loads (forces), Multi-model generators, Preliminary design phase, Reference aircraft, Secondary structures, Structural design, Structural modeling},
	pages = {1--130},
}

@article{koppula_graph-based_2019,
	title = {Graph-based word sense disambiguation in {Telugu} language},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064942953&doi=10.3233%2fKES-190399&partnerID=40&md5=c19d7f86dc49ec892234489c3f7082ac},
	doi = {10.3233/KES-190399},
	abstract = {In Natural Language Processing, word sense disambiguation (WSD) is an open challenge which improves the performance of the applications such as machine translation and information retrieval system. Many verbal languages will have many ambiguous words. The meaning of these ambiguous words differ per context. To choose the correct meaning of the word in the given context is known as WSD. In this article, the proposed work is to develop a WSD system using machine learning technique and knowledge-based approach for Telugu language. The knowledge resource used to develop the WSD system is Lexical Knowledge Base (LKB). The efficiency of WSD system is good when compared with other unsupervised approaches. © 2019-IOS Press and the authors. All rights reserved.},
	number = {1},
	journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
	author = {Koppula, N. and Rani, B.P. and Srinivas Rao, K.},
	year = {2019},
	note = {Publisher: IOS Press},
	keywords = {Graphic methods, Knowledge based systems, Knowledge-based approach, Learning algorithms, Learning systems, Lexical knowledge base, Machine learning techniques, Machine translations, NAtural language processing, Natural language processing systems, Search engines, Telugu language, Unsupervised approaches, Word Sense Disambiguation},
	pages = {55--60},
}

@article{ilic_design_2019,
	title = {Design methodology for graphene tunable filters at the sub-millimeter-wave frequencies},
	volume = {157},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065260695&doi=10.1016%2fj.sse.2019.04.003&partnerID=40&md5=e2cf8c9dc680719a7e6d0313d5daa6c2},
	doi = {10.1016/j.sse.2019.04.003},
	abstract = {Tunable components and circuits, allowing for the fast switching between the states of operation, are among the basic building blocks for future communications and other emerging applications. Based on the previous thorough study of graphene based resonators, the design methodology for graphene tunable filters has been devised, outlined, as well as explained through an example of the fifth order filter. The desired filtering responses can be achieved with the material loss not higher than the loss corresponding to the previously studied single resonators, depending mostly on the quantity of graphene per resonator. The proposed design method relies on the detailed design space mapping; obtained data gives an immediate assessment of the feasibility of specifications with a particular filter order, maximal passband ripple level, desired bandwidth, and acceptable losses. The design process could be further automated by the knowledge based approach using the collected design space data. © 2019 Elsevier Ltd},
	journal = {Solid-State Electronics},
	author = {Ilić, A.Ž. and Bukvić, B.M. and Budimir, D. and Ilić, M.M.},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Bandpass filters, Basic building block, Design, Design method, Emerging applications, Equivalent circuits, Full waves, Graphene, Knowledge based systems, Knowledge-based approach, Microwave filters, Millimeter waves, Numerical methods, Resonators, Sub-millimeter wave frequencies, Submillimeter waves, Tunable band-pass filters, Tunable components},
	pages = {34--41},
}

@article{methawachananont_software_2020,
	title = {Software process capability self-assessment support system based on task and work product characteristics: {A} case study of {ISO}/{IEC} 29110 standard},
	volume = {E103D},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081661657&doi=10.1587%2ftransinf.2018EDP7303&partnerID=40&md5=236eca07318c4b35e923ff4b70be5606},
	doi = {10.1587/transinf.2018EDP7303},
	abstract = {A key driver of software business growth in developing countries is the survival of software small and medium-sized enterprises (SMEs). Quality of products is a critical factor that can indicate the future of the business by building customer confidence. Software development agencies need to be aware of meeting international standards in software development process. In practice, consultants and assessors are usually employed as the primary solution, which can impact the budget in case of small businesses. Self-assessment tools for software development process can potentially reduce time and cost of formal assessment for software SMEs. However, the existing support methods and tools are largely insufficient in terms of process coverage and semi-automated evaluation. This paper proposes to apply a knowledge-based approach in development of a self-assessment and gap analysis support system for the ISO/IEC 29110 standard. The approach has an advantage that insights from domain experts and the standard are captured in the knowledge base in form of decision tables that can be flexibly managed. Our knowledge base is unique in that task lists and work products defined in the standard are broken down into task and work product characteristics, respectively. Their relation provides the links between Task List and Work Product which make users more understand and influence self-assessment. A prototype support system was developed to assess the level of software development capability of the agencies based on the ISO/IEC 29110 standard. A preliminary evaluation study showed that the system can improve performance of users who are inexperienced in applying ISO/IEC 29110 standard in terms of task coverage and user's time and effort compared to the traditional self-assessment method. © 2020 Institute of Electronics, Information and Communication, Engineers, IEICE. All rights reserved.},
	number = {2},
	journal = {IEICE Transactions on Information and Systems},
	author = {Methawachananont, A. and Buranarach, M. and Amsuriya, P. and Chaimongkhon, S. and Krairaksa, K. and Supnithi, T.},
	year = {2020},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, IEICE},
	keywords = {Budget control, Decision tables, Developing countries, Formal methods, Gap analysis, ISO Standards, International standards, Knowledge based systems, Knowledge-based approach, Self-assessment, Self-assessment tools, Small and medium-sized enterprise, Software design, Software development process, Software process capability, Software prototyping},
	pages = {339--347},
}

@article{dowdeswell_finding_2020,
	title = {Finding faults: {A} scoping study of fault diagnostics for {Industrial} {Cyber}–{Physical} {Systems}},
	volume = {168},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084943715&doi=10.1016%2fj.jss.2020.110638&partnerID=40&md5=b077bf5f4aae340fb9e379de4a5b086e},
	doi = {10.1016/j.jss.2020.110638},
	abstract = {Context: As Industrial Cyber–Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. Each of these sectors has adopted particular methods to meet their differing diagnostic needs. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. We created categories for the different diagnostic approaches via a pilot study and present an analysis of the trends that emerged. We then compared the maturity of these approaches by adapting and using the NASA Technology Readiness Level (TRL) scale. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Hybrid techniques that blend aspects of these three broad categories were also encountered. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems. Connecting ICPS together to share sufficient telemetry to diagnose and manage faults is difficult when the physical environment places demands on ICPS. Despite these challenges, the most mature papers present robust fault diagnosis and analysis techniques which have moved beyond the laboratory and are proving valuable in real-world environments. © 2020 Elsevier Inc.},
	journal = {Journal of Systems and Software},
	author = {Dowdeswell, B. and Sinha, R. and MacDonell, S.G.},
	year = {2020},
	note = {Publisher: Elsevier Inc.},
	keywords = {Accident prevention, Artificial intelligence, Electric fault currents, Fault detection, Fault detection and diagnosis, Hybrid configurations, Knowledge based systems, Knowledge-based approach, Learning systems, NASA, Petri nets, Predictive diagnostics, Production environments, Real world environments, Robust fault diagnosis, Space applications, Technology readiness levels},
}

@article{jia_integrated_2020,
	title = {Integrated data and knowledge driven methodology for human activity recognition},
	volume = {536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085927756&doi=10.1016%2fj.ins.2020.03.081&partnerID=40&md5=760f02da321fd6489d045b170c525f04},
	doi = {10.1016/j.ins.2020.03.081},
	abstract = {Human activity recognition has been a popular research area concerned with identifying the specific movement or action of a person based on variety of sensor data. Conventional human activity recognition approaches are mainly data driven, which are not working well for composite activity recognition due to the complexity and uncertainty of real scenarios. We propose in this paper a hierarchical structure-based framework and methodology for human activity recognition by an integration of data-driven approach and knowledge-based approach, which provides an interesting framework capable of bridging lower-level pattern recognition and higher-level knowledge for reasoning and explanation. More specifically, this approach constructs a hierarchical structure for representing the composite activity by a composition of lower-level actions and gestures according to its semantic meaning. This hierarchical structure is then transformed into formal syntactical logical formulas and rules, based on which the resolution based automated reasoning is applied to recognize the composite activity given the recognized lower-level actions by using data driven machine learning methods. The work is the validated using some open-source data about video based human activity recognition. The present work provides a promising framework and application illustration of integration of machine learning and symbolic reasoning. © 2020 Elsevier Inc.},
	journal = {Information Sciences},
	author = {Jia, H. and Chen, S.},
	year = {2020},
	note = {Publisher: Elsevier Inc.},
	keywords = {Activity recognition, Data-driven approach, Hierarchical structures, Human activity recognition, Knowledge based systems, Knowledge-based approach, Machine learning, Machine learning methods, Pattern recognition, Resolution-based automated reasonings, Semantics, Symbolic reasoning},
	pages = {409--430},
}

@article{tascini_understanding_1986,
	title = {Understanding images using knowledge based approach},
	volume = {593},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958513678&doi=10.1117%2f12.952166&partnerID=40&md5=d5195cf7b78dc11773f10ec8eb322195},
	doi = {10.1117/12.952166},
	abstract = {This paper presents an approach to image understanding focusing on low level image processing and proposes a rule-based approach as part of larger knowledge-based system. The general system has a yerarchical structure that comprises several knowledge-based layers. The main idea is to confine at the lower level the domain independent knowledge and to reserve the higher levels for the domain dependent knowledge, that is for the interpretation. © 1986 SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Tascini, G.},
	year = {1986},
	keywords = {Domain independents, General systems, Knowledge based, Knowledge based systems, Knowledge-based approach, Low level image processing, Medical image processing, Rule-based approach},
	pages = {138--141},
}

@article{gilmore_knowledge-based_1989,
	title = {A knowledge-based approach to planning and scheduling},
	volume = {1095},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958495331&doi=10.1117%2f12.969341&partnerID=40&md5=b8ed95f2c82a5cc2e029e3a822c102ea},
	doi = {10.1117/12.969341},
	abstract = {Analyses of the shop scheduling domain indicate the objective of scheduling is the determination and satisfaction of a large number of diverse constraints. Many researchers have explored the possibilities of scheduling with the assistance of dispatching rules, algorithms, heuristics and knowledge-based systems. This paper describes the development of an experimental knowledge-based planning and scheduling system which marries traditional planning and scheduling algorithms with a knowledge-based problem solving methodology in an integrated blackboard architecture. This system embodies scheduling methods and techniques which attempt to minimize one or a combination of scheduling parameters including completion time, average completion time, lateness, tardiness, and flow time. Preliminary results utilizing a test case factory involved in part production are presented. © 1989, SPIE.},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	author = {Gilmore, J.F. and Williams, D.L. and Thornton, S.},
	year = {1989},
	keywords = {Blackboard architecture, Dispatching rules, Experimental knowledge, Knowledge based systems, Knowledge-based approach, Planning and scheduling, Problem solving, Scheduling, Scheduling algorithms, Scheduling methods, Scheduling parameters, Traditional planning},
	pages = {906--919},
}

@article{waikar_knowledge-based_1992,
	title = {Knowledge-based approach to simulation model validation},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026944124&doi=10.1016%2f0360-8352%2892%2990109-W&partnerID=40&md5=9b67fb24acd6a49499fdbeb14623cb4b},
	doi = {10.1016/0360-8352(92)90109-W},
	abstract = {Validation is an important part of simulation modeling. The shortcoming of traditional validation is lack of sufficient knowledge and lack of organization of the existing knowledge. This research explored the possibility of using knowledge from fields other than simulation, such as functional design and database design, for conceptual validation. Two simulation models were tested using the knowledge. An organization of all the knowledge into a knowledge-base was suggested. © 1992.},
	number = {1-4},
	journal = {Computers and Industrial Engineering},
	author = {Waikar, A. and Pattanaik, M.},
	year = {1992},
	keywords = {Computer aided design, Computer simulation, Database systems, Expert systems, Functional design, Knowledge-based approach, Simulation model validation, Simulation modeling paradigms},
	pages = {245--248},
}

@article{mosavi_knowledge-based_2010,
	title = {Knowledge-based methods for optimum approximation of geometric dilution of precision},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953485072&doi=10.1142%2fS1469026810002835&partnerID=40&md5=6797c138c049a08372b910741da6b8b4},
	doi = {10.1142/S1469026810002835},
	abstract = {Global Positioning System (GPS) satellites signal processing to obtain all in view satellite measurements and to use them to find a solution and to do integrity monitoring forms a major component of the load on the receiver's processing element. If processing capability is limited there is restriction on the number of measurements which can be obtained and processed. Alternatively, the number of measurements can be restricted and the resulting saving in load on the processor can be used to offer more spare processing time which can be used for other user specific requirements. Thus if m visible satellites can provide measurements only n measurements can be used (n {\textless} m). The arrangement and the number of GPS satellites influence measurement accuracy. Dilution of Precision (DOP) is an index evaluating the arrangement of satellites. Geometric DOP (GDOP) is, in effect, the amplification factor of pseudo-range measurement errors into user errors due to the effect of satellite geometry. The GDOP approximation is an essential feature in determining the performance of a positioning system. In this paper, knowledge-based methods such as neural networks and evolutionary adaptive filters are presented for optimum approximation of GDOP. Without matrix inversion required, the knowledge-based approaches are capable of evaluating all subsets of satellites and hence reduce the computational burden. This would enable the use of a high-integrity navigation solution without the delay required for many matrix inversions. Models validity is verified with test data. The results are highly effective techniques for GDOP approximation. © 2010 Imperial College Press.},
	number = {2},
	journal = {International Journal of Computational Intelligence and Applications},
	author = {Mosavi, M.R.},
	year = {2010},
	keywords = {Adaptive filtering, Adaptive filters, Amplification, Amplification factors, Computational burden, Dilution of precision, Electric filters, Evolutionary algorithms, GDOP, GPS satellites, Geometric dilution of precision, Geometry, Global positioning system, Integrity monitoring, Knowledge based systems, Knowledge-based approach, Knowledge-based methods, Matrix inversions, Measurement accuracy, Measurement errors, Measurements, Navigation solution, Neural networks, Optimum approximation, Position control, Positioning system, Processing Time, Processing capability, Processing elements, Range measurements, Satellite geometry, Satellite measurements, Satellites, Signal processing, Signal receivers, Test data},
	pages = {153--170},
}

@article{kim_criteria_2004,
	title = {Criteria of good project network generator and its fulfillment using a dynamic {CBR} approach},
	volume = {3155},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048836684&doi=10.1007%2f978-3-540-28631-8_46&partnerID=40&md5=52decb36e0f81f2f8147637c0ba36ec4},
	doi = {10.1007/978-3-540-28631-8_46},
	abstract = {Most project-based industries such as construction, shipbuilding, and software development etc. should generate and manage project network for successful project planning. We suggest a set of criteria of good project network generator such as network generation efficiency, quality of network, and economics of system development. For the efficiency of the planning, the first criterion, we decided to take a CBR approach. However, using only previous cases is insufficient to generate a proper network for a new project. By embedding rules and constraints in the case-based system, we could improve the quality of the project network: the second criterion. The integration of CBR approach and the knowledge-based approach makes feasible the development of the project network generator and improves the quality of the network by mutual enhancement through crosschecking the knowledge and cases in the development and maintenance stages. For some complex project network planning, a single-case assumed project network generation methodology is refined into Dynamic Leveled Multiple Case approach. The methodology contributes again the efficiency and effectiveness of project network generation and reduces the efforts of the system development. © Springer-Verlag 2004.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Kim, H.W. and Lee, K.J.},
	year = {2004},
	note = {Publisher: Springer Verlag},
	keywords = {Case based reasoning, Case based systems, Complex networks, Complex projects, Efficiency, Knowledge based systems, Knowledge-based approach, Network generation, Project networks, Project planning, Project-based, Software design, System development},
	pages = {630--644},
}

@article{zheng_knowledge-based_2007,
	title = {Knowledge-based support for object-oriented software design and synthesis: {A} category theoretic approach},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350673897&partnerID=40&md5=de1990a30fe9efe685a0df6e39e37b82},
	abstract = {To reuse previous knowledge of objectoriented design and adapt them to solve new problems, the collaboration relationships and the responsibility distribution among software objects need to be thoroughly understood and precisely formulated. The paper proposes a knowledge-based approach that employs category theoretic models to formalize and mechanize objectoriented software design and synthesis by focusing concern on reasoning about the interdependency relationships at different levels of abstraction and granularity. The major benefit of our approach is twofold: First, it provides an explicit semantics for formal object-oriented specifications, and therefore enables a high-level of reusability and dynamic adaptability. Second, it utilizes the ability of categorical computations to support automated software composition and refinement. A prototype tool that demonstrates the feasibility and effectiveness of our approach is also presented.},
	number = {3},
	journal = {Journal of Digital Information Management},
	author = {Zheng, Y. and Xue, J. and Hu, Q.},
	year = {2007},
	keywords = {Automated software, Automation, Category-theoretic approach, Computer software reusability, Computer software selection and evaluation, Design, Dynamic adaptability, Explicit semantics, Formal objects, Knowledge based systems, Knowledge-based approach, Levels of abstraction, Object oriented design, Object oriented programming, Object-oriented software designs, Prototype tools, Reusability, Software composition, Software design, Software metrics, Software quality, Theoretic model},
	pages = {115--122},
}

@article{bouche_improving_2011,
	title = {Improving the performance of production lines with an expert system using a stochastic approach},
	volume = {87},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954503654&doi=10.1177%2f0037549710378019&partnerID=40&md5=00c58864921d5c41ee7742db59e80cf9},
	doi = {10.1177/0037549710378019},
	abstract = {In our increasingly competitive world, nowadays companies implement improvement strategies in every department and, in particular, in their manufacturing systems. This paper discusses the use of a global method, based on a knowledge-based approach, aiming at the development of a software tool for modeling and analysis of production flows. The main goal is the improvement of the performance of the production line. This method is based on data-processing and data-mining techniques and will help the acquisition of the meta-knowledge that is needed for finding correlations among different events in the line. Different techniques will be used: A graphical representation of the production, identification of specific behavior and research of correlations among events in the production line. Most of these techniques are based on statistical and probabilistic analyses. Events are expressed in the form of phenomena. To carry out high-level analyses, a stochastic approach will be used to identify breakdown models, which are the expression of specific correlations between phenomena. Breakdowns models will be the basis for, finally, defining action plans to improve the performance of the manufacturing lines. © 2010, SAGE Publications. All rights reserved.},
	number = {5},
	journal = {SIMULATION},
	author = {Bouché, P. and Zanni-Merk, C.},
	year = {2011},
	keywords = {Abstracting, Action plan, Behavioral research, Breakdown model, Computer simulation, Data handling, Discrete events, Expert systems, Global methods, Graphical representations, High-level analysis, Knowledge engineering, Knowledge-based approach, Manufacture, Manufacturing lines, Manufacturing system, Meta-knowledge, Mining techniques, Modeling and analysis, Probabilistic analysis, Production engineering, Production flows, Production line, Productivity, Software tool, Stochastic approach, Stochastic models, Stochastic systems, modeling and simulation of production systems},
	pages = {363--383},
}

@article{yi_knowledge-based_2011,
	title = {Knowledge-based approach to improving detailing plan in multiple product situations using {PDE} weights},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650710288&doi=10.1016%2fj.eswa.2010.09.044&partnerID=40&md5=f9c0b0b5631b4a8c8e3790d77f79665b},
	doi = {10.1016/j.eswa.2010.09.044},
	abstract = {Any pharmaceutical company relying heavily on its sales force to detail multiple products knows the importance of optimizing a short time window to detail its products to physicians effectively, in the right sequence. With the trend toward decreasing detailing time that is now averaging less than a minute, the optimization of this period is critical to success, especially in today's challenging selling environment. This paper develops a knowledge-based approach that integrates domain experts' knowledge of the definition of promotional responsiveness with a hybrid model of neural networks and a nonlinear program to accurately determine the physician detail equivalent (PDE) weights that reflect the weighted sequence of detail and portfolio size while identifying the physicians who are responsive to details. The output from this approach drives physician detailing planning, as well as planning for market share of detailing volume, which is known as share of voice (SOV) planning. Results based on six months of implementation indicate that the knowledge-based approach performs significantly better than the traditional approach by more than 12\% in profit. © 2010 Elsevier Ltd. All rights reserved.},
	number = {4},
	journal = {Expert Systems with Applications},
	author = {Yi, J.C.},
	year = {2011},
	keywords = {Competition, Domain experts, Drug products, Hybrid model, Knowledge based systems, Knowledge-based approach, Market share, Multiple products, Neural networks, Nonlinear programming, Nonlinear programs, Optimization, Pharmaceutical company, Physician detailing equivalent, Profitability, Response functions, Sales force, Share of voice, Time windows},
	pages = {3835--3843},
}

@article{uszok_knowledge-based_2013,
	title = {Knowledge-based approaches to information management in coalition environments},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874621833&doi=10.1109%2fMIS.2012.89&partnerID=40&md5=2d0c95710f189f8209020de0cadd98a2},
	doi = {10.1109/MIS.2012.89},
	abstract = {The community of interest information-sharing model lets coalition partners publish and disseminate data in a controlled fashion. In this vein, the authors have extended the Phoenix information management system to improve document selection and filtering. © 2001-2011 IEEE.},
	number = {1},
	journal = {IEEE Intelligent Systems},
	author = {Uszok, A. and Bunch, L. and Bradshaw, J.M. and Reichherzer, T. and Hanna, J. and Frantz, A.},
	year = {2013},
	keywords = {Coalition partners, Community of interest, Document selection, Information management, Information management systems, Information retrieval systems, Information-sharing model, Knowledge based systems, Knowledge-based approach, OWL, Ontology, Public policy},
	pages = {34--41},
}

@article{tyagi_hybrid_2014,
	title = {A hybrid knowledge-based approach to collaborative filtering for improved recommendations},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900415333&doi=10.3233%2fKES-140292&partnerID=40&md5=62c8e433ae16b84bf144b28c0713e14e},
	doi = {10.3233/KES-140292},
	abstract = {Collaborative filtering (CF) is one of the most successful and effective recommendation techniques for personalized information access. This method makes recommendations based on past transactions and feedback from users sharing similar interests. However, many commercial recommender systems are widely adopting the CF algorithms; these methods are required to have the ability to deal with sparsity in data and to scale with the increasing number of users and items. The proposed approach addresses the problems of sparsity and scalability by first clustering users based on their rating patterns and then inferring clusters (neighborhoods) by applying two knowledge-based techniques: rule-based reasoning (RBR) and case-based reasoning (CBR) individually. Further to improve accuracy of the system, HRC (hybridization of RBR and CBR) procedure is employed to generate an optimal neighborhood for an active user. The proposed three neighborhood generation procedures are then combined with CF to develop RBR/CF, CBR/CF, and HBR/CF schemes for recommendations. An empirical study reveals that the RBR/CF and CBR/CF perform better than other state-of-the-art CF algorithms, whereas HRC/CF clearly outperforms the rest of the schemes. © 2014 - IOS Press and the authors.},
	number = {2},
	journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
	author = {Tyagi, S. and Bharadwaj, K.K.},
	year = {2014},
	note = {Publisher: I O S Press},
	keywords = {Algorithms, Casebased reasonings (CBR), Collaborative filtering, Empirical studies, Knowledge based systems, Knowledge-based approach, Optimal neighborhoods, Personalized information, Recommendation techniques, Recommender systems, Rule based reasoning, clustering},
	pages = {121--133},
}

@article{baban_maintenance_2019,
	title = {Maintenance {Decision}-{Making} {Support} for {Textile} {Machines}: {A} {Knowledge}-{Based} {Approach} {Using} {Fuzzy} {Logic} and {Vibration} {Monitoring}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068751990&doi=10.1109%2fACCESS.2019.2923791&partnerID=40&md5=84e8ae6a762fcea5e2305cfbdb4a7377},
	doi = {10.1109/ACCESS.2019.2923791},
	abstract = {A condition-based maintenance approach may be used for planning the maintenance activities of textile machines with a satisfactory performance by developing maintenance decision-making support based on fuzzy logic and vibration monitoring. Since textile machines are systems with moving parts operating at relatively high-speed, vibration monitoring was used to indicate their failure development. At the same time, the characterization of the degradation phenomenon of textile machines involves some degree of uncertainty and vagueness. Within this context, a knowledge-based approach that employed fuzzy logic and vibration monitoring was developed. Deterioration symptoms do announce future failures of industrial machines, therefore building a maintenance decision-making support for scheduling maintenance actions of textile machines based on the estimation of their condition becomes a resourceful way to prevent their further deterioration. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Baban, M. and Baban, C.F. and Suteu, M.D.},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Computer circuits, Condition based maintenance, Condition monitoring, Decision making, Degree of uncertainty, Deterioration, Fuzzy logic, Industrial machines, Knowledge based systems, Knowledge-based approach, Maintenance, Maintenance Action, Maintenance activity, Maintenance decision making, Textile machinery, Textiles, Vibration measurement, Vibration monitoring},
	pages = {83504--83514},
}

@article{bahri_knowledge-based_2018,
	title = {Knowledge-based approaches for identity management in online social networks},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046131799&doi=10.1002%2fwidm.1260&partnerID=40&md5=57af3e5095d9e17d17956317439f95ec},
	doi = {10.1002/widm.1260},
	abstract = {When we meet a new person, we start by introducing ourselves. We share our names, and other information about our jobs, cities, family status, and so on. This is how socializing and social interactions can start: we first need to identify each other. Identification is a cornerstone in establishing social contacts. We identify ourselves and others by a set of civil (e.g., name, nationality, ID number, gender) and social (e.g., music taste, hobbies, religion) characteristics. This seamlessly carried out identification process in face-to-face interactions is challenged in the virtual realms of socializing, such as in online social network (OSN) platforms. New identities (i.e., online profiles) could be created without being subject to any level of verification, making it easy to create fake information and forge fake identities. This has led to a massive proliferation of accounts that represent fake identities (i.e., not mapping to physically existing entities), and that poison the online socializing environment with fake information and malicious behavior (e.g., child abuse, information stealing). Within this milieu, users in OSNs are left unarmed against the challenging task of identifying the real person behind the screen. OSN providers and research bodies have dedicated considerable effort to the study of the behavior and features of fake OSN identities, trying to find ways to detect them. Some other research initiatives have explored possible techniques to enable identity validation in OSNs. Both kinds of approach rely on extracting knowledge from the OSN, and exploiting it to achieve identification management in their realms. We provide a review of the most prominent works in the literature. We define the problem, provide a taxonomy of related attacks, and discuss the available solutions and approaches for knowledge-based identity management in OSNs. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textgreater} Human Centricity and User Interaction Application Areas{\textgreater} Internet and Web-Based Applications Application Areas{\textgreater} Society and Culture. © 2018 Wiley Periodicals, Inc.},
	number = {5},
	journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	author = {Bahri, L. and Carminati, B. and Ferrari, E.},
	year = {2018},
	note = {Publisher: Wiley-Blackwell},
	keywords = {Face-to-face interaction, Identification process, Identity management, Knowledge based systems, Knowledge-based approach, On-line social networks, Online social networks (OSN), Online systems, Research initiatives, Social networking (online), Web-based applications},
}

@article{korn_general_2018,
	title = {General principles of systems},
	volume = {47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043320886&doi=10.1108%2fK-09-2017-0348&partnerID=40&md5=2a9640f3ac9b4e2ee2aacbf42933e5bb},
	doi = {10.1108/K-09-2017-0348},
	abstract = {Purpose: Three problematic issues followed by paradigm changes over the recent history of human intellectual endeavour are identified as 1. mysticism/superstition to – conventional science (of physics), 2. predominant use of qualitative/quantitative properties for analysis and design to – structural or systemic properties, and 3. current speculative/fragmented, multiple approaches to the “systemic view” to – a firmer knowledge-based approach reflecting the empirical and universal nature of this view. This paper aims to consider the problematic issues, to conclude that conventional science is inadequate to cope with the 2nd paradigm change and to introduce a “new science of systems” which can integrate conventional science and alleviate the 3rd problematic issue by suggesting three principles implemented by linguistic modelling as operational model. Design/methodology/approach: The highly successful methodology of conventional science is followed with systemic content by suggesting three general principles of systems, namely, principle of existence (pervasiveness of structural description), principle of complexity (aggregates for emergence of outcomes) and principle of change (change by purpose or chance), and linguistic modelling of static and dynamic scenarios based on natural language as operational model. This language is processed to “elementary constituents”, of which complex structures can be constructed. These constituents are converted into reasoning schemes consisting of “ordered pairs” and “predicate logic statements” in static and dynamic states. Findings: Stories of problematic scenarios are converted into the universal scheme of “management/producers” – “products” – “users/consumers” by constructing linguistic networks of products and semantic diagrams of organizations/user/consumers for investigating the emergence of outcomes in analysis and for designing prototypes. Problematic issues of individual objects in a scenario are resolved by methods of conventional science, which is thus integrated with systems science to form the “scientific enterprise”. Research limitations/implications: Once the new approach is debated, further developments in the mathematics of ordered pairs, predicate logic and uncertainties are needed. The linguistic basis is to be further investigated. Connection with AI and “logical atomism of Bertrand Russell” is to be explored. Practical implications: Further applications to large-scale scenarios by practitioners using the “universal scheme” and development of software are needed. Social implications: The approach is rooted in accepted branches of knowledge, is highly teachable and should lead to be used by professionals and others once debated and accepted. © 2018, Emerald Publishing Limited.},
	number = {8},
	journal = {Kybernetes},
	author = {Korn, J.},
	year = {2018},
	note = {Publisher: Emerald Group Publishing Ltd.},
	keywords = {Application programs, Complex networks, Computer circuits, Design/methodology/approach, Further development, General principles, Knowledge based systems, Knowledge-based approach, Linguistic Modelling, Linguistic networks, Modeling languages, Product design, Scientific enterprise, Semantics, Structural descriptions, System theory},
	pages = {1524--1548},
}

@article{butnaru_shotgunwsd_2019,
	title = {{ShotgunWSD} 2.0: {An} {Improved} {Algorithm} for {Global} {Word} {Sense} {Disambiguation}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081105138&doi=10.1109%2fACCESS.2019.2938058&partnerID=40&md5=6eef67ff24c20a81ebe3ac8a11166150},
	doi = {10.1109/ACCESS.2019.2938058},
	abstract = {ShotgunWSD is a recent unsupervised and knowledge-based algorithm for global word sense disambiguation (WSD). The algorithm is inspired by the Shotgun sequencing technique, which is a broadly-used whole genome sequencing approach. ShotgunWSD performs WSD at the document level based on three phases. The first phase consists of applying a brute-force WSD algorithm on short context windows selected from the document, in order to generate a short list of likely sense configurations for each window. The second phase consists of assembling the local sense configurations into longer composite configurations by prefix and suffix matching. In the third phase, the resulting configurations are ranked by their length, and the sense of each word is chosen based on a majority voting scheme that considers only the top configurations in which the respective word appears. In this paper, we present an improved version (2.0) of ShotgunWSD which is based on a different approach for computing the relatedness score between two word senses, a step that stays at the core of building better local sense configurations. For each sense, we collect all the words from the corresponding WordNet synset, gloss and related synsets, into a sense bag. We embed the collected words from all the sense bags in the entire document into a vector space using a common word embedding framework. The word vectors are then clustered using k-means to form clusters of semantically related words. At this stage, we consider that clusters with fewer samples (with respect to a given threshold) represent outliers and we eliminate these clusters altogether. Words from the eliminated clusters are also removed from each and every sense bag. Finally, we compute the median of all the remaining word embeddings in a given sense bag to obtain a sense embedding for the corresponding word sense. We compare the improved ShotgunWSD algorithm (version 2.0) with its previous version (1.0) as well as several state-of-the-art unsupervised WSD algorithms on six benchmarks: SemEval 2007, Senseval-2, Senseval-3, SemEval 2013, SemEval 2015, and overall (unified). We demonstrate that ShotgunWSD 2.0 yields better performance than ShotgunWSD 1.0 and some other recent unsupervised or knowledge-based approaches. We also performed paired McNemar's significance tests, showing that the improvements of ShotgunWSD 2.0 over ShotgunWSD 1.0 are in most cases statistically significant, with a confidence interval of 0.01. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Butnaru, A.M. and Ionescu, R.T.},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Composite configurations, Confidence interval, Embeddings, K-means clustering, Knowledge based systems, Knowledge-based algorithms, Knowledge-based approach, Natural language processing systems, Semantically-related words, Significance test, Vector spaces, Whole genome sequencing, Word-sense disambiguation},
	pages = {120961--120975},
}

@article{viani_supervised_2019,
	title = {Supervised methods to extract clinical events from cardiology reports in {Italian}},
	volume = {95},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066986896&doi=10.1016%2fj.jbi.2019.103219&partnerID=40&md5=51cdb5a25e12e71ec8679e661a5c6dd3},
	doi = {10.1016/j.jbi.2019.103219},
	abstract = {Clinical narratives are a valuable source of information for both patient care and biomedical research. Given the unstructured nature of medical reports, specific automatic techniques are required to extract relevant entities from such texts. In the natural language processing (NLP) community, this task is often addressed by using supervised methods. To develop such methods, both reliably-annotated corpora and elaborately designed features are needed. Despite the recent advances on corpora collection and annotation, research on multiple domains and languages is still limited. In addition, to compute the features required for supervised classification, suitable language- and domain-specific tools are needed. In this work, we propose a novel application of recurrent neural networks (RNNs) for event extraction from medical reports written in Italian. To train and evaluate the proposed approach, we annotated a corpus of 75 cardiology reports for a total of 4365 mentions of relevant events and their attributes (e.g., the polarity). For the annotation task, we developed specific annotation guidelines, which are provided together with this paper. The RNN-based classifier was trained on a training set including 3335 events (60 documents). The resulting model was integrated into an NLP pipeline that uses a dictionary lookup approach to search for relevant concepts inside the text. A test set of 1030 events (15 documents) was used to evaluate and compare different pipeline configurations. As a main result, using the RNN-based classifier instead of the dictionary lookup approach allowed increasing recall from 52.4\% to 88.9\%, and precision from 81.1\% to 88.2\%. Further, using the two methods in combination, we obtained final recall, precision, and F1 score of 91.7\%, 88.6\%, and 90.1\%, respectively. These experiments indicate that integrating a well-performing RNN-based classifier with a standard knowledge-based approach can be a good strategy to extract information from clinical text in non-English languages. © 2019 Elsevier Inc.},
	journal = {Journal of Biomedical Informatics},
	author = {Viani, N. and Miller, T.A. and Napolitano, C. and Priori, S.G. and Savova, G.K. and Bellazzi, R. and Sacchi, L.},
	year = {2019},
	note = {Publisher: Academic Press Inc.},
	keywords = {Automatic technique, Cardiology, Classification (of information), Clinical research, Computer, Data Mining, Data mining, Electronic Health Records, Extract informations, Heart Diseases, Humans, Information retrieval, Italy, Knowledge based systems, Knowledge-based approach, Linguistics, NAtural language processing, Natural Language Processing, Natural language processing systems, Neural Networks, Neural networks, Non-English languages, Pipeline configuration, Pipelines, Recurrent neural network (RNNs), Recurrent neural networks, Semantics, Supervised classification, Supervised learning, article, cardiology, classifier, data mining, electronic health record, extraction, heart disease, human, human experiment, natural language processing, pipeline, procedures, recall, semantics},
}

@article{chang_knowledge-based_1994,
	title = {A knowledge-based operation support system for network traffic management},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028194654&doi=10.1016%2f0167-9236%2894%2990062-0&partnerID=40&md5=2d2b710398177b8499900bccec6be180},
	doi = {10.1016/0167-9236(94)90062-0},
	abstract = {The operation of a telephone network is subject to unexpected contingencies such as facilities impairments, trunk congestion, and focused overload. When contingencies happen in the network, the proper control actions must be taken promptly within a short period of time (e.g. 5 minutes); otherwise, the serious congestion will soon spread to the other parts of network. Until now, there has been a lack of an automated approach to generate a contigency routing plan to prevent network degradation. In this paper, a knowledge-based traffic control support system (TRACOSS) is proposed for network traffic management. TRACOSS not only contains four knowledge bases to accumulate the domain expertise used for contingencies detection, diagnosis, affected traffic estimation and control commands generation, but also contains a multicommodity network flow model to formulate the operation of telephone network. Based on this model, a heuristic algorithm is developed to design the contingency routing plan. TRACOSS has the following advantages: (1) The acceptable contingency plan can be obtained by the heuristic algorithm within the fixed time constraint. (2) Four knowledge bases can be used to accumulate the domain expertise to automate the network traffic management. (3) When new network components are installed, knowledge bases can be easily upgraded to adapt new technologies. © 1994.},
	number = {1},
	journal = {Decision Support Systems},
	author = {Chang, C.-Y. and Chung, C.-G.},
	year = {1994},
	keywords = {Algorithms, Computer networks, Decision support systems, Heuristic methods, Knowledge based operation support system, Knowledge based systems, Knowledge based traffic control support system, Multicommodity network flow model, Network traffic management, Operations research, Telecommunication control, Telecommunication networks, Telecommunication traffic, Telephone systems},
	pages = {25--36},
}

@article{braune_vitro_2019,
	title = {In {Vitro} {Thrombogenicity} {Testing} of {Biomaterials}},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074109888&doi=10.1002%2fadhm.201900527&partnerID=40&md5=ef67b089901ed530518f657d17fd98d4},
	doi = {10.1002/adhm.201900527},
	abstract = {The short- and long-term thrombogenicity of implant materials is still unpredictable, which is a significant challenge for the treatment of cardiovascular diseases. A knowledge-based approach for implementing biofunctions in materials requires a detailed understanding of the medical device in the biological system. In particular, the interplay between material and blood components/cells as well as standardized and commonly acknowledged in vitro test methods allowing a reproducible categorization of the material thrombogenicity requires further attention. Here, the status of in vitro thrombogenicity testing methods for biomaterials is reviewed, particularly taking in view the preparation of test materials and references, the selection and characterization of donors and blood samples, the prerequisites for reproducible approaches and applied test systems. Recent joint approaches in finding common standards for a reproducible testing are summarized and perspectives for a more disease oriented in vitro thrombogenicity testing are discussed. © 2019 Helmholtz-Zentrum Geesthacht. Published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
	number = {21},
	journal = {Advanced Healthcare Materials},
	author = {Braune, S. and Latour, R.A. and Reinthaler, M. and Landmesser, U. and Lendlein, A. and Jung, F.},
	year = {2019},
	note = {Publisher: Wiley-VCH Verlag},
	keywords = {Anticoagulants, Biocompatible Materials, Biomaterials, Blood, Blood Platelets, Blood component, Blood test, Cardio-vascular disease, Common standards, Dental prostheses, Diseases, Female, Hemolysis, Humans, Implant materials, Implants (surgical), In-vitro, Knowledge based systems, Knowledge-based approach, Male, Materials Testing, Review, Thrombogenicity, anticoagulant agent, anticoagulation, biomaterial, blood analysis, blood cell, blood donor, chemistry, complement system, donor selection, drug effect, female, fibrin, fibrinopeptide A, hemoglobin, hemoglobin blood level, hemolysis, human, in vitro study, leukocyte activation, male, materials testing, partial thromboplastin time, platelet count, priority journal, thrombin, thrombocyte, thrombogenicity, thrombosis},
}

@article{yi_knowledge-based_2008,
	title = {Knowledge-based approach to improving micromarketing decisions in a data-challenged environment},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-45049086718&doi=10.1016%2fj.eswa.2007.08.032&partnerID=40&md5=a49404931ece5e0a8902d2fc0939de5b},
	doi = {10.1016/j.eswa.2007.08.032},
	abstract = {With the growing popularity of micromarketing strategies, more companies are tailoring their marketing efforts to meet individual consumer need based on analysis of consumer-level data. However, there are two major obstacles in the way to effective micromarketing strategies: (1) data limitation at the consumer-level making it difficult to construct robust and accurate promotional response function and (2) misplaced usage of a resource allocation approach effective in a data-rich environment to data-challenged environment. These obstacles lead to suboptimal marketing resource allocation decisions. This paper presents a knowledge-based approach specifically designed for effectively allocating marketing resources in the micromarketing environment. A team of domain experts provide knowledge in refining and imputing data to overcome data limitation issues, as well as identifying consumer-level constraints to strengthen the integer programming model's performance. Results indicate that this approach is transparent to all levels of management, adaptable to changes in environment, and easy to implement. In addition, there is tremendous potential for improving not only profitability but also growing the company's intellectual capital. When this approach is applied to a random sales territory, it outperforms the traditional method by more than 19\% in profit. © 2007 Elsevier Ltd. All rights reserved.},
	number = {3},
	journal = {Expert Systems with Applications},
	author = {Yi, J.C.},
	year = {2008},
	keywords = {Data acquisition, Data limitation, Decision making, Knowledge based systems, Marketing, Micromarketing, Resource allocation, Sales resource allocation},
	pages = {1379--1385},
}

@article{rahim_enhanced_2010,
	title = {Enhanced relevance-based approach for network control},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953158651&partnerID=40&md5=ae6637bf268319274c59ddbf3dc42eae},
	abstract = {Simple flooding, probabilistic approach, area-based scheme, knowledge-based approach and Multi hop Vehicular broadcast is not suitable for VANETs scenario because of its dynamic nature. Relevance scheme is proposed to disseminate the relevant message for sharing in VANETs and discards the redundant messages from the network and improves the over all performance of network. The relevance-based approach does not provide network control and it only broadcast user traffic. This paper presents an improvement in mathematical model to consider the network control. Simulations using NS-2 show that proposed mathematical model consider the network control and improve the global benefit.},
	number = {2},
	journal = {Informatica (Ljubljana)},
	author = {Rahim, A. and Muhaya, F.B. and Khan, Z.S. and Ansari, M.A. and Sher, M.},
	year = {2010},
	keywords = {802.11 e, Ad hoc networks, Area-based, Broadcasting, Computer simulation, Dynamic nature, Knowledge based systems, Knowledge-based approach, Mathematical models, Mobile telecommunication systems, Multihop, Network control, Probabilistic approaches, User traffics, Vehicular ad hoc networks},
	pages = {223--226},
}

@article{khanmohammadi_hybrid_2009,
	title = {A hybrid technique for thickness-map visualization of the {HIP} cartilages in {MRI}},
	volume = {E92-D},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950255687&doi=10.1587%2ftransinf.E92.D.2253&partnerID=40&md5=60a1fbe1ba3f7fc6d28f146ec39dc506},
	doi = {10.1587/transinf.E92.D.2253},
	abstract = {Quantification of the hip cartilages is clinically important. In this study, we propose an automatic technique for segmentation and visualization of the acetabular and femoral head cartilages based on clinically obtained multi-slice T1-weighted MR data and a hybrid approach. We follow a knowledge based approach by employing several features such as the anatomical shapes of the hip femoral and acetabular cartilages and corresponding image intensities. We estimate the center of the femoral head by a Hough transform and then automatically select the volume of interest. We then automatically segment the hip bones by a self-adaptive vector quantization technique. Next, we localize the articular central line by a modified canny edge detector based on the first and second derivative filters along the radial lines originated from the femoral head center and anatomical constraint. We then roughly segment the acetabular and femoral head cartilages using derivative images obtained in the previous step and a top-hat filter. Final masks of the acetabular and femoral head cartilages are automatically performed by employing the rough results, the estimated articular center line and the anatomical knowledge. Next, we generate a thickness map for each cartilage in the radial direction based on a Euclidian distance. Three dimensional pelvic bones, acetabular and femoral cartilages and corresponding thicknesses are overlaid and visualized. The techniques have been implemented in C++ and MATLAB environment. We have evaluated and clarified the usefulness of the proposed techniques in the presence of 40 clinical hips multi-slice MR images. Copyright © 2009 The Institute of Electronics, Information and Communication Engineers.},
	number = {11},
	journal = {IEICE Transactions on Information and Systems},
	author = {Khanmohammadi, M. and Zoroofi, R.A. and Nishii, T. and Tanaka, H. and Sato, Y.},
	year = {2009},
	note = {Publisher: Institute of Electronics, Information and Communication, Engineers, IEICE},
	keywords = {Adaptive Vector Quantization, Anatomical constraint, Bone, Bone segmentation, Canny edge detectors, Cartilage, Cartilage segmentation, Data visualization, Digital filters, Directional derivative, Edge detection, Hough transforms, Image segmentation, Knowledge based systems, Knowledge-based approach, Magnetic resonance imaging, Singular value decomposition, Thickness map, Vector quantization, Visualization},
	pages = {2253--2263},
}

@article{dorbath_extended_2014,
	title = {Extended physics-based wing mass estimation in early design stages applying automated model generation},
	volume = {228},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900524239&doi=10.1177%2f0954410013482657&partnerID=40&md5=1ce2935b1bff5eaa83141cbd7a852fc0},
	doi = {10.1177/0954410013482657},
	abstract = {This article introduces a tool chain for extended physics-based wing mass estimation. Compared to state-of-the-art tool chains, the physics-based structural modelling is extended beyond the wing primary structure. The structural model also includes the movable trailing edge devices including tracks, the spoilers, the engine pylons and the landing gear. The chain consists of the structural analysis model, models for aerodynamic, fuel, landing gear and engine loads as well as a sizing algorithm. To make the complexity of the model generation process feasible for preliminary aircraft design, a knowledge-based approach is chosen. This means that the analysis models are created partly automatically, which leads to a minimum set of required input parameters for the central model generator. The DLR aircraft parametrisation format Common Parametric Aircraft Configuration Scheme is used as central data model for input and output. Therefore, the chain can be easily included in a wider multidisciplinary aircraft design environment. © IMechE 2013 Reprints and permissions: sagepub.co.uk/journalsPermissions.nav.},
	number = {7},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
	author = {Dorbath, F. and Nagel, B. and Gollnick, V.},
	year = {2014},
	note = {Publisher: Professional Engineering Publishing},
	keywords = {Aircraft, Aircraft configurations, Aircraft design, Automated model generations, Chains, Design, Early design stages, Engines, Estimation, Finite element method, Flaps, Knowledge based systems, Knowledge-based approach, Landing gear (aircraft), Mass estimation, Multidisciplinary aircraft design, Structural modelling, Tools},
	pages = {1010--1019},
}

@article{fredj_knowledge-based_2014,
	title = {A knowledge-based approach to initial population generation in evolutionary algorithms: {Application} to the protein structure prediction problem},
	volume = {8001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916215800&doi=10.1007%2f978-3-642-45321-2_13&partnerID=40&md5=5ef1119619b1d1001cc13a6f72fcebe1},
	doi = {10.1007/978-3-642-45321-2_13},
	abstract = {In this study we introduce a new approach to generate the initial population of an Evolutionary Algorithm (EA), based on problem-specific knowledge. We discuss the key ingredients (knowledge and diversity) necessary to generate a good diverse initial random population, with particular application to the protein structure prediction problem. Two main components of our Initial Population Generation (IPG) algorithm are described: (a) one provides the bio-chemical problem-specific knowledge (Molecular Dynamics (MD) and Normal Mode Analysis (NMA)); (b) the second one is an algorithm which ensures population diversity by using the complete graph of the generated bio-molecular conformations. Results show that IPG is a promising algorithm for the creation of good diversity initial populations. © Springer-Verlag Berlin Heidelberg 2014.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Fredj, E. and Goldstein, M. and Goldstein, M.},
	year = {2014},
	note = {Publisher: Springer Verlag},
	keywords = {Bioinformatics, Chemical analysis, Complete graphs, Evolutionary algorithms, Graph theory, Initial population, Knowledge based systems, Knowledge-based approach, Molecular dynamics, New approaches, Normal mode analysis, Population diversity, Problem-specific knowledge, Protein structure prediction, Proteins},
	pages = {252--262},
}

@article{pantazi_application_2014,
	title = {Application of supervised self organising models for wheat yield prediction},
	volume = {436},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921325806&doi=10.1007%2f978-3-662-44654-6_55&partnerID=40&md5=016a87c135cdde80ea6d30c0d056a249},
	doi = {10.1007/978-3-662-44654-6_55},
	abstract = {The management of wheat yield behavior in agricultural areas is a very important task because it influences and specifies the wheat yield production. An efficient knowledge-based approach utilizing an efficient Machine Learning algorithm for characterizing wheat yield behavior is presented in this research work. The novelty of the method is based on the use of Supervised Self Organizing Maps to handle existent sensor information by using a supervised learning algorithm so as to assess measurement data and update initial knowledge. The advent of precision farming generates data which, because of their type and complexity, are not efficiently analyzed by traditional methods. The Supervised Self Organizing Maps have been proved from the literature efficient and flexible to analyze sensor information and by using the appropriate learning algorithms can update the initial knowledge. The Self Organizing models that are developed consisted of input nodes representing the main factors in wheat crop production such as biomass indicators, Organic Carbon (OC), pH, Mg, Total N, Ca, Cation Exchange Capacity (CEC), Moisture Content (MC) and the output weights represented the class labels corresponding to the predicted wheat yield. © IFIP International Federation for Information Processing 2014.},
	journal = {IFIP Advances in Information and Communication Technology},
	author = {Pantazi, X.E. and Moshou, D. and Mouazen, A.M. and Kuang, B. and Alexandridis, T.},
	year = {2014},
	note = {Publisher: Springer New York LLC},
	keywords = {Agricultural areas, Cation exchange capacities, Conformal mapping, Cultivation, Data mining, Information use, Knowledge based systems, Knowledge-based approach, Learning algorithms, Learning systems, Machine learning, Measurement data, Neural networks, Organic carbon, Precision agriculture, Precision farming, Self organizing maps, Self-organizing model, Sensor informations, Supervised self-organizing map},
	pages = {556--565},
}

@article{backhaus_knowledge-based_2015,
	title = {Knowledge-{Based} {Conceptual} {Synthesis} of {Industrial}-{Scale} {Downstream} {Processes} for {Biochemical} {Products}},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923312464&doi=10.1002%2fceat.201400111&partnerID=40&md5=a1881a80ec4056627ae714e495acece1},
	doi = {10.1002/ceat.201400111},
	abstract = {A generic, knowledge-based guideline assisting downstream process synthesis for biochemical products is presented. It offers process designers a structured process design methodology supporting them in capturing potentially relevant information, which might be beyond their expertise. The guideline is based on heuristic knowledge which was collected, structured in a generic way, and clearly represented. The generation of alternative downstream routes as starting points for experiments, simulation, and cost calculation is hereby accelerated. The application of the guideline is demonstrated on the example of penicillin V downstream processing from fermentation broth. © 2015 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
	number = {3},
	journal = {Chemical Engineering and Technology},
	author = {Backhaus, K. and Lochmüller, M. and Arndt, M.C. and Riechert, O. and Schembecker, G.},
	year = {2015},
	note = {Publisher: Wiley-VCH Verlag},
	keywords = {Biochemical products, Conceptual process synthesis, Design Methodology, Downstream process, Downstream-processing, Fermentation broths, Heuristic knowledge, Knowledge based systems, Knowledge-based approach},
	pages = {537--546},
}

@article{anwar_countering_2020,
	title = {Countering {Malicious} {URLs} in {Internet} of {Things} {Using} a {Knowledge}-{Based} {Approach} and a {Simulated} {Expert}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084928260&doi=10.1109%2fJIOT.2019.2954919&partnerID=40&md5=9a1d011198895c3e10cd1457dc01047d},
	doi = {10.1109/JIOT.2019.2954919},
	abstract = {This article proposes a novel methodology to detect malicious uniform resource locators (URLs) using simulated expert (SE) and knowledge-base system (KBS). The proposed study not only efficiently detects known malicious URLs but also adapts countermeasure against the newly generated malicious URLs. Moreover, this article also explored which lexical features are contributing more in final decision using a factor analysis method, and thus help in avoiding the involvement of human experts. Furthermore, we apply the following state-of-the-art machine learning (ML) algorithms, i.e., naïve Bayes (NB), decision tree (DT), gradient boosted trees (GBT), generalized linear model (GLM), logistic regression (LR), deep learning (DL), and random rest (RF), and evaluate the performance of these algorithms on a large-scale real data set of data-driven Web applications. The experimental results clearly demonstrate the efficiency of NB in the proposed model as NB outperforms when compared to the rest of the aforementioned algorithms in terms of average minimum execution time (i.e., 3 s) and is able to accurately classify the 107 586 URLs with 0.2\% error rate and 99.8\% accuracy rate. © 2014 IEEE.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Anwar, S. and Al-Obeidat, F. and Tubaishat, A. and Din, S. and Ahmad, A. and Khan, F.A. and Jeon, G. and Loo, J.},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Decision trees, Deep learning, Factor analysis method, Generalized linear model, Internet of things, Knowledge base system, Knowledge based systems, Knowledge-based approach, Lexical features, Logistic regression, Novel methodology, State of the art, Trees (mathematics), WEB application},
	pages = {4497--4504},
}

@article{zhou_ontology-driven_2017,
	title = {An ontology-driven, {SVM} approach for hyperspectral image classification},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012910150&doi=10.1080%2f19479832.2017.1288174&partnerID=40&md5=39da18030ab884b858fd322bf91b167a},
	doi = {10.1080/19479832.2017.1288174},
	abstract = {​​Although support vector machine (SVM) has shown powerful capability to support supervised classification for hyperspectral data, such model does not have the capability to define semantically the classification and reasoning rules which can potentially contribute to a more accurate supervised classification. In this article, we present an ontology-driven framework to support SVM-based hyperspectral data classification. First, we proposed a dimension reduction algorithm to allow automatic selection of prominent spectral characteristics for each land cover class included in a hyperspectral image. The result of the prominent spectral characteristics includes ranking and weights, used to signify the importance of a subset of all wavebands in distinguishing a particular land cover class from others. Then, we developed an ontology named HIC-Ontology to formally represent the extracted spectral characteristics to support the final training and classification process. The experimental results show that the proposed technique achieves better performance in classifying hyperspectral data than using the classic classification algorithm alone. We expect this work to contribute significantly in hyper-spectral image processing by introducing this knowledge-based approach. © 2017 Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {2},
	journal = {International Journal of Image and Data Fusion},
	author = {Zhou, X. and Li, W. and Wu, S. and Wang, S.},
	year = {2017},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Classification (of information), Classification algorithm, Classification process, Dimension reduction, Dimension reduction algorithm, Hyperspectral data classification, Hyperspectral imaging, Image classification, Image processing, Knowledge based systems, Knowledge-based approach, Ontology, Spectral characteristics, Spectroscopy, Supervised classification, Supervised learning, Support vector machines, algorithm, image classification, land cover, spectral analysis, support vector machine},
	pages = {112--129},
}

@article{egorova_fictive_2018,
	title = {Fictive motion extraction and classification},
	volume = {32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051929310&doi=10.1080%2f13658816.2018.1498503&partnerID=40&md5=f65caca6f13ce3d4ff61af318abc5e8b},
	doi = {10.1080/13658816.2018.1498503},
	abstract = {Fictive motion (e.g. ‘The highway runs along the coast’) is a pervasive phenomenon in language that can imply both a static and a moving observer. In a corpus of alpine narratives, it is used in three types of spatial descriptions: conveying the actual motion of the observer, describing a vista and communicating encyclopaedic spatial knowledge. This study takes a knowledge-based approach to develop rules for automated extraction and classification of these types based on an annotated corpus of fictive motion instances. In particular, we identify the differences in the set of concepts involved into the production of the three types of descriptions, followed by their linguistic operationalization. Based on that, we build a set of rules that classify fictive motion with an overall precision of 0.87 and recall of 0.71. The article highlights the importance of examining spatially rich, naturally occurring corpora for the lines of work dealing with the automated interpretation of spatial information in texts, as well as, more broadly, investigation of spatial language involved into various types of spatial discourse. © 2018, © 2018 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {11},
	journal = {International Journal of Geographical Information Science},
	author = {Egorova, E. and Moncla, L. and Gaio, M. and Claramunt, C. and Purves, R.S.},
	year = {2018},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {automation, classification, conceptual framework, knowledge based system, spatial analysis},
	pages = {2247--2271},
}

@article{mahantesh_integrated_2014,
	title = {Integrated machine health monitoring: {A} knowledge based approach},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940269400&doi=10.1007%2fs13198-013-0178-1&partnerID=40&md5=df7ba028b128e7cf24019e3677d71e33},
	doi = {10.1007/s13198-013-0178-1},
	abstract = {Machine health monitoring in today's complex plant systems has gained more prominence than ever before because of steep increase in machinery costs, plant investments and maintenance expenses. A breakdown in any one machine or a component in a plant could mean huge losses coupled with safety and environmental threats as in the case of nuclear or chemical plants. The advances in manufacturing technology and the competition in the market necessitate the continuous availability of machinery for production. This has created a need for integrating maintenance with other manufacturing activities for better plant availability and efficiency. The objective of present research work is to present one such integrated machine health monitoring (IMHM) system developed using knowledge-based systems. The proposed model can be a useful maintenance tool in majority of small and medium scale manufacturing plants. A comprehensive knowledge-based system (KBS) could be developed over a period of time for industrial machinery which can monitor the major machinery faults and provide expert maintenance solutions through measurement and analysis of machine parameters such as power, vibration, noise, temperature, wear debris, lubricant condition, etc. A fault diagnosis system with KBS is based on computer programs interlinking fault symptoms, faults and remedies. These solutions are based on published information about permissible machine parameters in handbooks, journals, conferences besides the past maintenance experiences and from machine expert's knowledge regarding specific machinery problem and its solution. The paper outlines possible sub-modules for IMHM along with their features. © 2013 The Society for Reliability Engineering, Quality and Operations Management (SREQOM), India and The Division of Operation and Maintenance, Lulea University of Technology, Sweden.},
	number = {3},
	journal = {International Journal of Systems Assurance Engineering and Management},
	author = {Mahantesh, N. and Aditya, P. and Kumar, U.},
	year = {2014},
	keywords = {Artificial intelligence, Chemical plants, Condition based maintenance, Fault diagnosis systems, Knowledge based systems, Knowledge-based approach, Machine health monitoring, Machinery, Maintenance, Manufacture, Manufacturing activities, Manufacturing technologies, Measurement and analysis, World class},
	pages = {371--382},
}

@article{hemmati_systematic_2015,
	title = {Systematic approaches for designing {Rogowski} coils},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928186139&doi=10.1049%2fiet-smt.2013.0266&partnerID=40&md5=e91cbd501321376897cb231479e519bf},
	doi = {10.1049/iet-smt.2013.0266},
	abstract = {Rogowski coil is one of the popular current transducers for measuring currents either in the sinusoidal form or in the form of high-frequency pulses. In this study, first, the performance of the Rogowski coil is investigated through comprehensive experiments to be formed as a design methodology. Then, the results create a base for proposing two systematic approaches for designing Rogowski coils. The first approach is a knowledge-based approach and the second one involves a heuristic search method which results in optimum design according to the selected objective function, considering the practical limits in the physical parameters. The proposed design procedures cover Rogowski coils which measure large sinusoidal currents for power system protection purposes and also those for measuring impulsive currents with large as well as low peaks. © The Institution of Engineering and Technology 2015.},
	number = {3},
	journal = {IET Science, Measurement and Technology},
	author = {Hemmati, E. and Shahrtash, S.M.},
	year = {2015},
	note = {Publisher: Institution of Engineering and Technology},
	keywords = {Current transducer, Design, Electric coils, Heuristic algorithms, Heuristic methods, Heuristic search methods, High-frequency pulse, Knowledge based systems, Knowledge-based approach, Objective functions, Physical parameters, Power system protection, Sinusoidal currents},
	pages = {259--267},
}

@article{laurent_improving_2014,
	title = {Improving recognition of proper nouns in {ASR} through generating and filtering phonetic transcriptions},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900533159&doi=10.1016%2fj.csl.2014.02.006&partnerID=40&md5=89232bb768bdd61bc30473a4f33d2ee7},
	doi = {10.1016/j.csl.2014.02.006},
	abstract = {Accurate phonetic transcription of proper nouns can be an important resource for commercial applications that embed speech technologies, such as audio indexing and vocal phone directory lookup. However, an accurate phonetic transcription is more difficult to obtain for proper nouns than for regular words. Indeed, phonetic transcription of a proper noun depends on both the origin of the speaker pronouncing it and the origin of the proper noun itself. This work proposes a method that allows the extraction of phonetic transcriptions of proper nouns using actual utterances of those proper nouns, thus yielding transcriptions based on practical use instead of mere pronunciation rules. The proposed method consists in a process that first extracts phonetic transcriptions, and then iteratively filters them. In order to initialize the process, an alignment dictionary is used to detect word boundaries. A rule-based grapheme-to-phoneme generator (LIA-PHON), a knowledge-based approach (JSM), and a Statistical Machine Translation based system were evaluated for this alignment. As a result, compared to our reference dictionary (BDLEX supplemented by LIA-PHON for missing words) on the ESTER 1 French broadcast news corpus, we were able to significantly decrease the Word Error Rate (WER) on segments of speech with proper nouns, without negatively affecting the WER on the rest of the corpus. © 2014 Elsevier Ltd.},
	number = {4},
	journal = {Computer Speech and Language},
	author = {Laurent, A. and Meignier, S. and Deléglise, P.},
	year = {2014},
	note = {Publisher: Academic Press},
	keywords = {Commercial applications, G2P, Grapheme to phonemes, Indexing (of information), Iterative methods, Knowledge based systems, Knowledge-based approach, Linguistics, Moses, Phonetic transcriptions, Proper nouns, Speech recognition, Statistical machine translation, Surface mount technology, Transcription},
	pages = {979--996},
}

@article{das_fuzzy_2018,
	title = {A fuzzy logic based transport mode detection framework in urban environment},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042928258&doi=10.1080%2f15472450.2018.1436968&partnerID=40&md5=1584733902be979225c003bcc4340e17},
	doi = {10.1080/15472450.2018.1436968},
	abstract = {Transport mode detection is an emerging research area in different domains such as urban planning, context-aware mobile computing, and intelligent transportation systems. Current approaches are mostly data-driven, based on machine learning approaches. However, machine learning models require substantial training data and cannot explain the reasoning procedure. Data-driven approaches also fall short while interpreting trajectories where ground truth information is limited. Therefore, this paper develops a novel knowledge-based approach for interpreting smartphone global positioning system trajectories by detecting various transport modes used during travel. The proposed model is based on an expert system that can work without any training, based solely on expert knowledge. Core is a fuzzy multiple-input multiple-output expert system using kinematic and spatial information with a well explained fuzzy reasoning scheme through a fuzzy rule base. The model can provide alternate predictions with varied certainty factors. Different membership function combinations have been evaluated in terms of accuracy and ambiguity, and the result demonstrates that the model performs best using a Gaussian–Gaussian combination, comparable to the existing machine learning approaches. © 2018, © 2018 Taylor \& Francis.},
	number = {6},
	journal = {Journal of Intelligent Transportation Systems: Technology, Planning, and Operations},
	author = {Das, R.D. and Winter, S.},
	year = {2018},
	note = {Publisher: Taylor and Francis Inc.},
	keywords = {Carrier mobility, Computer circuits, Expert systems, Fuzzy inference, Fuzzy logic, Fuzzy rules, GPS, Global positioning system, Intelligent systems, Intelligent transportation systems, Knowledge-based approach, Learning systems, MIMO systems, Machine learning approaches, Machine learning models, Membership functions, Reasoning procedures, Smartphones, Thermodynamic properties, Trajectories, Transport modes, Trellis codes, Urban transportation, accuracy assessment, detection method, expert system, fuzzy mathematics, machine learning, mobile phone, travel, uncertainty, uncertainty analysis, urban planning},
	pages = {478--489},
}

@article{thomas_innovative_2019,
	title = {An innovative hybrid approach for extracting named entities from unstructured text data},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065029748&doi=10.1111%2fcoin.12214&partnerID=40&md5=0a37553bb46a9f84f078fdef3095a910},
	doi = {10.1111/coin.12214},
	abstract = {Named entity recognition (NER) is the core part of information extraction that facilitates the automatic detection and classification of entities in natural language text into predefined categories, such as the names of persons, organizations, locations, and so on. The output of the NER task is crucial for many applications, including relation extraction, textual entailment, machine translation, information retrieval, etc. Literature shows that machine learning and deep learning approaches are the most widely used techniques for NER. However, for entity extraction, the abovementioned approaches demand the availability of a domain-specific annotated data set. Our goal is to develop a hybrid NER system composed of rule-based deep learning as well as clustering-based approaches, which facilitates the extraction of generic entities (such as person, location, and organization) out of natural language texts of domains that lack generic named entities labeled domain data sets. The proposed approach takes the advantages of both deep learning and clustering approaches but separately, in combination with a knowledge-based approach by using a postprocessing module. We evaluated the proposed methodology on court cases (judgments) as a use case since it contains generic named entities of different forms that are poorly or not present in open-source NER data sets. We also evaluated our hybrid models on two benchmark data sets, namely, Computational Natural Language Learning (CoNLL) 2003 and Open Knowledge Extraction (OKE) 2016. The experimental results obtained from benchmark data sets show that our hybrid models achieved substantially better performance in terms of the F-score in comparison to other competitive systems. © 2019 Wiley Periodicals, Inc.},
	number = {4},
	journal = {Computational Intelligence},
	author = {Thomas, A. and Sangeetha, S.},
	year = {2019},
	note = {Publisher: Blackwell Publishing Inc.},
	keywords = {Artificial intelligence, Benchmarking, Character recognition, Classification (of information), Clustering approach, Data mining, Deep learning, Knowledge based systems, Knowledge extraction, Knowledge-based approach, Machine translations, Named entity recognition, Natural language learning, Natural language processing systems, Natural language text, Text processing, judicial domain},
	pages = {799--826},
}

@article{baskaran_knowledge-based_2009,
	title = {A knowledge-based primary care approach to increase breast screening attendance},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651540100&doi=10.1504%2fIJBET.2009.022914&partnerID=40&md5=8446c084f6135a431b7853c17011bc35},
	doi = {10.1504/IJBET.2009.022914},
	abstract = {The static rate of breast screening attendance in the UK has been of concern in the fight against breast cancer mortality. This paper highlights how primary care can play a vital role in addressing this issue. Knowledge created through prediction mechanisms and sharing them with care deliverers forms the core of this discussion. Knowledge-based alerts are employed to initiate interventions to increase the breast screening attendance. This paper highlights the various factors that are to be considered while deploying such initiatives in primary care setting and validates them through a questionnaire-based survey. Copyright © 2009, Inderscience Publishers.},
	number = {2},
	journal = {International Journal of Biomedical Engineering and Technology},
	author = {Baskaran, V. and Bali, R.K. and Arochena, H. and Naguib, R.N.G. and Wheaton, M. and Wallis, M. and Wickramasinghe, N.},
	year = {2009},
	note = {Publisher: Inderscience Publishers},
	keywords = {Artificial intelligence, Breast Cancer, Breast screening, Information technology, Knowledge based, Knowledge based systems, Knowledge-based approach, Prediction mechanisms, Primary care, Surveys},
	pages = {172--188},
}

@article{elger_application_2019,
	title = {Application of raman spectroscopy to working gas sensors: {From} in situ to operando studies},
	volume = {19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075278817&doi=10.3390%2fs19235075&partnerID=40&md5=3a8419bb75ad9de90cd1fbb69a4412bc},
	doi = {10.3390/s19235075},
	abstract = {Understanding the mode of operation of gas sensors is of great scientific and economic interest. A knowledge-based approach requires the development and application of spectroscopic tools to monitor the relevant surface and bulk processes under working conditions (operando approach). In this review we trace the development of vibrational Raman spectroscopy applied to metal-oxide gas sensors, starting from initial applications to very recent operando spectroscopic approaches. We highlight the potential of Raman spectroscopy for molecular-level characterization of metal-oxide gas sensors to reveal important mechanistic information, as well as its versatility regarding the design of in situ/operando cells and the combination with other techniques. We conclude with an outlook on potential future developments. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.},
	number = {23},
	journal = {Sensors (Switzerland)},
	author = {Elger, A.-K. and Hess, C.},
	year = {2019},
	note = {Publisher: MDPI AG},
	keywords = {Chemical sensors, Development and applications, Gas detectors, Gases, In-situ spectroscopy, Knowledge based systems, Knowledge-based approach, Mechanisms, Metal oxide gas sensors, Metallic compounds, Metals, Mode of operations, Operando approaches, Operando spectroscopy, Raman spectroscopy, Spectroscopic tool, Trace elements},
}

@article{quinzo_computational_2019,
	title = {Computational assembly of a human {Cytomegalovirus} vaccine upon experimental epitope legacy},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076339583&doi=10.1186%2fs12859-019-3052-6&partnerID=40&md5=a35d7dd532d702187db1742825f17d1a},
	doi = {10.1186/s12859-019-3052-6},
	abstract = {Background: Human Cytomegalovirus (HCMV) is a ubiquitous herpesvirus affecting approximately 90\% of the world population. HCMV causes disease in immunologically naive and immunosuppressed patients. The prevention, diagnosis and therapy of HCMV infection are thus crucial to public health. The availability of effective prophylactic and therapeutic treatments remain a significant challenge and no vaccine is currently available. Here, we sought to define an epitope-based vaccine against HCMV, eliciting B and T cell responses, from experimentally defined HCMV-specific epitopes. Results: We selected 398 and 790 experimentally validated HCMV-specific B and T cell epitopes, respectively, from available epitope resources and apply a knowledge-based approach in combination with immunoinformatic predictions to ensemble a universal vaccine against HCMV. The T cell component consists of 6 CD8 and 6 CD4 T cell epitopes that are conserved among HCMV strains. All CD8 T cell epitopes were reported to induce cytotoxic activity, are derived from early expressed genes and are predicted to provide population protection coverage over 97\%. The CD4 T cell epitopes are derived from HCMV structural proteins and provide a population protection coverage over 92\%. The B cell component consists of just 3 B cell epitopes from the ectodomain of glycoproteins L and H that are highly flexible and exposed to the solvent. Conclusions: We have defined a multiantigenic epitope vaccine ensemble against the HCMV that should elicit T and B cell responses in the entire population. Importantly, although we arrived to this epitope ensemble with the help of computational predictions, the actual epitopes are not predicted but are known to be immunogenic. © 2019 The Author(s).},
	journal = {BMC Bioinformatics},
	author = {Quinzo, M.J. and Lafuente, E.M. and Zuluaga, P. and Flower, D.R. and Reche, P.A.},
	year = {2019},
	note = {Publisher: BioMed Central},
	keywords = {Computational Biology, Computational predictions, Cytology, Cytomegalovirus, Cytomegalovirus Vaccines, Cytomegalovirus vaccine, Cytotoxic activities, Diagnosis, Epitopes, Forecasting, HCMV, Human cytomegalovirus, Humans, Knowledge based systems, Knowledge-based approach, Population protection, Structural proteins, T-cells, Therapeutic treatments, Vaccines, biology, epitope, human, immunology, procedures},
}

@article{tzouveli_intelligent_2004,
	title = {Intelligent visual descriptor extraction from video sequences},
	volume = {3094},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846076935&doi=10.1007%2f978-3-540-25981-7_9&partnerID=40&md5=b957ce2852c85f97fc951180cc0872f6},
	doi = {10.1007/978-3-540-25981-7_9},
	abstract = {Extraction of visual descriptors is a crucial problem for state-of-the-art visual information analysis. In this paper, we present a knowledge-based approach for detection of visual objects in video sequences, extraction of visual descriptors and matching with pre-defined objects. The proposed approach models objects through their visual descriptors defined in MPEG7. It first extracts moving regions using an efficient active contours technique. It then computes visual descriptions of the moving regions including color, motion and shape features that are invariant to affine transformations. The extracted features are matched to a-priori knowledge about the objects' descriptions, using appropriately defined matching functions. Results are presented which illustrate the theoretical developments. © Springer-Verlag 2004.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Tzouveli, P. and Andreou, G. and Tsechpenakis, G. and Avrithis, Y. and Kollias, S.},
	year = {2004},
	note = {Publisher: Springer Verlag},
	keywords = {Affine transformations, Descriptor extractions, Extraction, Image segmentation, Knowledge based systems, Knowledge-based approach, Matching functions, Priori knowledge, State of the art, Theoretical development, Video recording, Visual information analysis},
	pages = {132--146},
}

@article{kabbaj_analytical_2010,
	title = {Analytical and knowledge based approaches for a bioprocess supervision},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-75849124921&doi=10.1016%2fj.knosys.2009.06.014&partnerID=40&md5=bf881619a2430d314746698710f2205e},
	doi = {10.1016/j.knosys.2009.06.014},
	abstract = {The paper deals with supervision of biotechnological processes. Two main tasks are considered in this supervision scheme. The first one deals with Fault Detection and Isolation (FDI). The objective is to detect changes in the process dynamics using a model-based method. The second task concerns process physiological state recognition. A behavioral model is built using the expert knowledge along with clustering techniques applied to the measured data. The behavioral model under automata form is an heuristic model of the process under supervision. © 2009 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Knowledge-Based Systems},
	author = {Kabbaj, N. and Nakkabi, Y. and Doncescu, A.},
	year = {2010},
	keywords = {Behavioral model, Bioprocess, Bioprocesses, Biotechnological process, Classification, Classification (of information), Clustering techniques, Expert knowledge, Fault detection, Fault detection and isolation, Heuristic model, Knowledge based systems, Knowledge-based approach, Main tasks, Measured data, Model-based method, Physiological state, Process dynamics},
	pages = {116--124},
}

@article{kalathiya_identification_2016,
	title = {Identification of {1H}-indene-(1,3,5,6)-tetrol derivatives as potent pancreatic lipase inhibitors using molecular docking and molecular dynamics approach},
	volume = {63},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945369430&doi=10.1002%2fbab.1432&partnerID=40&md5=c5bba4d9267815967d5c6cedee0da6bd},
	doi = {10.1002/bab.1432},
	abstract = {Pancreatic lipase is a potential therapeutic target to treat diet-induced obesity in humans, as obesity-related diseases continue to be a global problem. Despite intensive research on finding potential inhibitors, very few compounds have been introduced to clinical studies. In this work, new chemical scaffold 1H-indene-(1,3,5,6)-tetrol was proposed using knowledge-based approach, and 36 inhibitors were derived by modifying its functional groups at different positions in scaffold. To explore binding affinity and interactions of ligands with protein, CDOCKER and AutoDock programs were used for molecular docking studies. Analyzing results of rigid and flexible docking algorithms, inhibitors C\_12, C\_24, and C\_36 were selected based on different properties and high predicted binding affinities for further analysis. These three inhibitors have different moieties placed at different functional groups in scaffold, and to characterize structural rationales for inhibitory activities of compounds, molecular dynamics simulations were performed (500 nSec). It has been shown through simulations that two structural fragments (indene and indole) in inhibitor can be treated as isosteric structures and their position at binding cleft can be replaced by each other. Taking into account these information, two lines of inhibitors can further be developed, each line based on a different core scaffold, that is, indene/indole. © 2015 International Union of Biochemistry and Molecular Biology, Inc.},
	number = {6},
	journal = {Biotechnology and Applied Biochemistry},
	author = {Kalathiya, U. and Padariya, M. and Baginski, M.},
	year = {2016},
	note = {Publisher: Wiley-Blackwell Publishing Ltd},
	keywords = {1h indene (1, 3, 5, 6) tetrol derivative, Article, Binding energy, Bins, Corrosion inhibitors, Diet-induced obesity, Drug Design, Enzyme Inhibitors, Indenes, Knowledge based systems, Knowledge-based approach, Lipase, Molecular Docking Simulation, Molecular Dynamics Simulation, Molecular docking, Molecular dynamics, Molecular dynamics simulations, Molecular modeling, Nutrition, Pancreas, Pancreatic lipase, Polycyclic aromatic hydrocarbons, Potential inhibitors, Protein Conformation, Structural fragments, Therapeutic targets, Thermodynamics, algorithm, animal experiment, antagonists and inhibitors, antiobesity agent, binding affinity, blood brain barrier, chemistry, controlled study, diet induced obesity, drug design, enzyme inhibitor, enzymology, esterase inhibitor, human, hydrogen bond, indene derivative, intestine absorption, metabolism, molecular docking, molecular dynamics, molecular weight, mouse, nonhuman, pancreas, protein conformation, protein structure, static electricity, thermodynamics, triacylglycerol lipase, unclassified drug},
	pages = {765--778},
}

@article{binns_integrated_2011,
	title = {An integrated knowledge-based approach for modelling biochemical reaction networks},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053913257&doi=10.1016%2fj.compchemeng.2011.03.030&partnerID=40&md5=8ac5e301ddc92ce97bfd927f94faf4b0},
	doi = {10.1016/j.compchemeng.2011.03.030},
	abstract = {This paper presents a new methodology for constructing cellular network topologies by searching for new binding species and new reactions catalysed by the enzymes present. Our technique is knowledge-based and integrates several steps. Starting from a pre-determined list of enzymes in the system it (i) generates lists of binding species, (ii) constructs a reaction network using these species and (iii) finds pathways through this network, which link different substrates (raw materials) with target metabolites (pathway products). Graph-theory-based analysis of the two-dimensional structures of known binding species is used to compute pharmacophores, the structures and functional groups binding at the corresponding enzymes' sites. New binding species are obtained by searching in appropriate databases for existing compounds, which contain these pharmacophores. Reactions are constructed by generating all possible combinations of the binding species identified and by testing the feasibility (i.e. the ability to conserve atomic/molecular mass) of each constructed reaction. Generated reactions are required to be linearly independent in order to minimise the complexity of subsequent steps. Finally, pathways through the reaction network are computed to assess important reactions and metabolites for a given process. Our integrated procedure has been applied to two illustrative systems, the glycolysis and the citric acid cycle in Homo sapiens and Saccharomyces cerevisiae, respectively. New binding species and reactions were found for the enzymes involved. It was observed that some enzymes are very specific and only catalyse a small number of very similar reactions. Pathways were also constructed and analysed to demonstrate the relative importance of the metabolites involved. © 2011 Elsevier Ltd.},
	number = {12},
	journal = {Computers and Chemical Engineering},
	author = {Binns, M. and Theodoropoulos, C.},
	year = {2011},
	keywords = {Ability testing, Binding sites, Biochemical reaction network, Biomolecules, Catalyse, Cellular network, Cellular neural networks, Citric acid, Different substrates, Electric network topology, Enzymes, Functional groups, Graph theory, Homo sapiens, Knowledge based systems, Knowledge-based approach, Metabolic network, Metabolism, Metabolites, Pharmacodynamics, Pharmacophores, Reaction generation, Reaction network, Reaction pathways, Relative importance, Two-dimensional structures, Yeast},
	pages = {3025--3043},
}

@article{nejad_automatic_2016,
	title = {Automatic image acquisition with knowledge-based approach for multi-directional determination of skid resistance of pavements},
	volume = {71},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990869212&doi=10.1016%2fj.autcon.2016.08.003&partnerID=40&md5=cc3fd70bfd1a4e774dc5671c3667d663},
	doi = {10.1016/j.autcon.2016.08.003},
	abstract = {Evaluation of pavement skid resistance (SR), as a crucial index for assessing the degree of pavement safety, is essential for pavement management. Considering the difficulties involved in estimation of an overall index of extent and direction of SR, a new method is proposed in the present work for estimation of the SR using an automated image-based system. The images were first captured by an automated image acquisition system (IAS) and then an image processing expert system and a knowledge-based decision support system (DSS) were designed. The central part of the system proposed in the current work runs based on Wavelet Transform (WT), which consists of distinct modules including pre-processing, feature extraction, approximate indexes in three different directions (horizontal, vertical, and diagonal), and the overall index. The method was verified on a database of pavement images collected in dry and wet conditions. A comparison of the obtained results with those of British pendulum tester (BPT) indicates the validity and high speed of the proposed method. © 2016 Elsevier B.V.},
	number = {Part 2},
	journal = {Automation in Construction},
	author = {Nejad, F.M. and Karimi, N. and Zakeri, H.},
	year = {2016},
	note = {Publisher: Elsevier B.V.},
	keywords = {Artificial intelligence, Automated image acquisition systems, Decision support systems, Dry and wet conditions, Expert systems, Feature extraction, Image acquisition, Image based system, Image processing, Knowledge based decision support systems, Knowledge based systems, Knowledge-based approach, Multidirectional judgment, Pavement management, Pavements, Skid resistance, Wavelet, Wavelet transforms},
	pages = {414--429},
}

@article{bordogna_interoperable_2017,
	title = {An interoperable open data framework for discovering popular tours based on geo-tagged tweets},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031739855&doi=10.1504%2fIJIIDS.2017.087255&partnerID=40&md5=3a18523740e0f6ed8ab37ab5c6dcbbf4},
	doi = {10.1504/IJIIDS.2017.087255},
	abstract = {In this paper, we introduce an original approach that exploits timestamped geo-tagged messages posted by Twitter users through their smartphones when they travel to trace their trips. A clustering approach is applied to group similar trips to identify tours, and an interoperable framework is used to share the popular tours on the web, in order to analyse them in relation with local geo-located territorial resources. Tools developed to reconstruct and mine the tours of tourists within a region are described, which identify, track, and group the tourists' trips through a knowledge-based approach, exploiting timestamped geo-tagged information associated with Twitter messages sent by tourists while travelling. The collected tracks are managed and shared on the web in compliance with OGC standards so as to be able to analyse the characteristic of localities visited by the tourists by spatial overlaying with other open geo-spatial data, such as maps of points of interest (POIs) of distinct type. The result is a novel interoperable framework, based on web-service technology. © 2017 Inderscience Enterprises Ltd.},
	number = {3-4},
	journal = {International Journal of Intelligent Information and Database Systems},
	author = {Bordogna, G. and Cuzzocrea, A. and Frigerio, L. and Psaila, G. and Toccu, M.},
	year = {2017},
	note = {Publisher: Inderscience Publishers},
	keywords = {Big data, Clustering approach, Data Analytics, Data framework, Geo-spatial data, Intelligent systems, Interoperability, Knowledge based systems, Knowledge-based approach, OGC Standards, Open Data, Points of interest, Regulatory compliance, Social networking (online), Web service technology, Web services},
	pages = {246--268},
}

@article{qi_crop_2017,
	title = {A crop phenology knowledge-based approach for monthly monitoring of construction land expansion using polarimetric synthetic aperture radar imagery},
	volume = {133},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029585565&doi=10.1016%2fj.isprsjprs.2017.09.009&partnerID=40&md5=e032fd3445cb5c7ee0931e09037b3ee0},
	doi = {10.1016/j.isprsjprs.2017.09.009},
	abstract = {Synthetic aperture radar (SAR) remote sensing, which is independent of weather conditions, can monitor construction land expansion at short intervals for early prevention of unauthorized land use. However, seasonal crop growth creates land cover changes that are hardly distinguishable from land developments by using the traditional approach that employs two SAR images for detection. This study proposes a knowledge-based approach based on crop phenology to detect monthly construction land expansion by using consecutive polarimetric SAR imagery. The innovation of the proposed approach is the utilization of crop phenology knowledge to remove errors introduced by seasonal crop growth. In this approach, using crop phenology knowledge as a basis, a knowledge-based system is built to automatically determine when seasonal crop growth yields considerable errors. Monthly land developments are normally detected by comparing two consecutive images, but in the periods when the errors from crop growth are considerable, monthly detection results are calibrated using an additional third consecutive image, which is utilized to identify the errors based on the difference in temporal land cover change between land development and crop growth. A comparison was made between the proposed approach and the traditional approach for the monthly monitoring of construction land expansion. We found that seasonal paddy growth created many errors by using the traditional approach. The proposed approach substantially reduced these errors. Compared with the traditional approach, the proposed approach reduced errors by up to 87.33\% with an average overall error rate of only 0.24\%. The results indicated that the proposed approach outperforms the traditional approach in monitoring monthly construction land expansion and suppressing the disturbance from seasonal crop growth. © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Qi, Z. and Yeh, A.G.-O. and Li, X.},
	year = {2017},
	note = {Publisher: Elsevier B.V.},
	keywords = {Biology, Consecutive images, Construction land, Crop phenology, Crops, Errors, Expansion, Knowledge based systems, Knowledge-based approach, Land use, Land-cover change, Polarimeters, Polarimetric SAR, Polarimetric synthetic aperture radars, Radar, Radar imaging, Remote sensing, Synthetic aperture radar, Tracking radar, Traditional approaches},
	pages = {1--17},
}

@article{lesage_computational_2018,
	title = {Computational modeling and reverse engineering to reveal dominant regulatory interactions controlling osteochondral differentiation: {Potential} for regenerative medicine},
	volume = {6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058690582&doi=10.3389%2ffbioe.2018.00165&partnerID=40&md5=9673cf490b03449554eb4e7d065a1c0d},
	doi = {10.3389/fbioe.2018.00165},
	abstract = {The specialization of cartilage cells, or chondrogenic differentiation, is an intricate and meticulously regulated process that plays a vital role in both bone formation and cartilage regeneration. Understanding the molecular regulation of this process might help to identify key regulatory factors that can serve as potential therapeutic targets, or that might improve the development of qualitative and robust skeletal tissue engineering approaches. However, each gene involved in this process is influenced by a myriad of feedback mechanisms that keep its expression in a desirable range, making the prediction of what will happen if one of these genes defaults or is targeted with drugs, challenging. Computer modeling provides a tool to simulate this intricate interplay from a network perspective. This paper aims to give an overview of the current methodologies employed to analyze cell differentiation in the context of skeletal tissue engineering in general and osteochondral differentiation in particular. In network modeling, a network can either be derived from mechanisms and pathways that have been reported in the literature (knowledge-based approach) or it can be inferred directly from the data (data-driven approach). Combinatory approaches allow further optimization of the network. Once a network is established, several modeling technologies are available to interpret dynamically the relationships that have been put forward in the network graph (implication of the activation or inhibition of certain pathways on the evolution of the system over time) and to simulate the possible outcomes of the established network such as a given cell state. This review provides for each of the aforementioned steps (building, optimizing, and modeling the network) a brief theoretical perspective, followed by a concise overview of published works, focusing solely on applications related to cell fate decisions, cartilage differentiation and growth plate biology. Particular attention is paid to an in-house developed example of gene regulatory network modeling of growth plate chondrocyte differentiation as all the aforementioned steps can be illustrated. In summary, this paper discusses and explores a series of tools that form a first step toward a rigorous and systems-level modeling of osteochondral differentiation in the context of regenerative medicine. © 2018 Lesage, Kerkhofs and Geris.},
	number = {NOV},
	journal = {Frontiers in Bioengineering and Biotechnology},
	author = {Lesage, R. and Kerkhofs, J. and Geris, L.},
	year = {2018},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {Body fluids, Cartilage, Cell engineering, Chondrocyte differentiation, Chondrocytes, Chondrogenic differentiation, Cytology, Differentiation (calculus), Gene expression, Gene regulatory network model, Gene regulatory networks, In-silico models, Knowledge based systems, Knowledge-based approach, Network inference, Regenerative Medicine, Reverse engineering, Tissue, Tissue engineering},
}

@article{goldberg_knowledge-based_1988,
	title = {A knowledge-based approach for evaluating forestry-map congruency with remotely sensed imagery},
	volume = {324},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024224844&partnerID=40&md5=572b2bebce6f527856cb7a78042bde89},
	abstract = {Simple algorithmic methods for integrating remotely sensed data with existing cartographic databases do not provide satisfactory results. One of the main difficulties is in reconciling spatial differences between the two data sources. The differences are due to such diverse factors as temporal variations, spatial errors in the map data, and topographic effects in the remotely sensed data. To investigate the extent of this data-integration problem, a map-image congruency evaluation (MICE) knowledge-based system was developed. -Authors},
	number = {1579},
	journal = {Philosophical Transactions - Royal Society of London, A},
	author = {Goldberg, M. and Goodenough, D.G. and Plunkett, G.},
	year = {1988},
	keywords = {British Columbia, Canada, data integration, forest cover, knowledge-based approach, map-image congruency, spatial difference},
	pages = {447--456},
}

@article{jannach_knowledge-based_2004,
	title = {A knowledge-based framework for the rapid development of conversational recommenders},
	volume = {3306},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048856573&doi=10.1007%2f978-3-540-30480-7_40&partnerID=40&md5=8ed5071b1c1e9085b47c34d671e77fd3},
	doi = {10.1007/978-3-540-30480-7_40},
	abstract = {Web-based sales assistance systems are a valuable means to guide online customers in the decision-making and product selection process. Conversational recommenders simulate the behavior of an experienced sales expert, which is a knowledge-intensive task and requires personalized user interaction according to the customers' needs and skills. In this paper, we present the ADVISOR SUITE framework for rapid development of conversational recommenders for arbitrary domains. In the system, both the recommendation logic and the knowledge required for constructing the personalized dialog and adaptive web pages is contained in a declarative knowledge-base. The advisory application can be completely modeled using graphical tools based on a conceptual model of online sales dialogs. A template mechanism supports the automatic construction of maintainable dynamic web pages. At run-time, a controller component generically steers the interaction flow. Practical experiences from several commercial installations of the system show that development times and costs for online sales advisory systems can be significantly reduced when following the described knowledge-based approach. © Springer-Verlag 2004.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Jannach, D. and Kreutler, G.},
	year = {2004},
	note = {Publisher: Springer Verlag},
	keywords = {Automatic construction, Commercial installation, Decision making, Declarative knowledge, Dynamic Web-pages, Electronic commerce, Knowledge based framework, Knowledge based systems, Knowledge intensive tasks, Knowledge-based approach, Online systems, Practical experience, Sales, Social networking (online), Websites},
	pages = {390--402},
}

@article{montali_knowledge-rich_2019,
	title = {Knowledge-rich optimisation of prefabricated façades to support conceptual design},
	volume = {97},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056721294&doi=10.1016%2fj.autcon.2018.11.002&partnerID=40&md5=0f732d3ffcea4b46089c9d0a86389fd1},
	doi = {10.1016/j.autcon.2018.11.002},
	abstract = {One of the principal challenges in façade design is to support the architectural intent by devising technically viable (i.e. standard-compliant and manufacturable) solutions from as early as possible in the design stage. This is increasingly relevant as prefabricated façades increase in complexity and bespokedness in response to current societal, financial and environmental challenges. In this paper a process that addresses this challenge is presented. The process consists of two sub-processes 1) to build product-oriented knowledge bases and digital tools for supporting design on a project-by-project basis and 2) to help designers identify a set of optimal solutions that consider the façade-specific trade-off between architectural intent and performance requirements, while meeting the largest number of production-related constraints. This process was applied to a case study and the results were compared with those obtained from a recently-developed façade. It was found that, although the proposed process produces optimal solutions that are approximated, designers can benefit from more control over the product's manufacturability, performance and architectural intent in less time. © 2018 Elsevier B.V.},
	journal = {Automation in Construction},
	author = {Montali, J. and Sauchelli, M. and Jin, Q. and Overend, M.},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Architectural design, Conceptual design, Design for manufacture and assemblies, Design optimisation, Digital devices, Economic and social effects, Environmental challenges, Facade design, Knowledge based systems, Knowledge-based approach, Machine design, Manufacture, Optimal solutions, Optimal systems, Performance requirements, Product design, Support conceptual designs},
	pages = {192--204},
}

@article{ansari_knowledge-based_2020,
	title = {A knowledge-based approach for representing jobholder profile toward optimal human–machine collaboration in cyber physical production systems},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078882967&doi=10.1016%2fj.cirpj.2019.11.005&partnerID=40&md5=dde58612317e0fa95ea93d7eaa84b38d},
	doi = {10.1016/j.cirpj.2019.11.005},
	abstract = {Increasing number of AI-enhanced approaches provide helpful “know-how” for reproducing and imitating skills and finally substituting human jobs with algorithms and intelligent machines. However, complementarity of human and machine, especially in hybrid man-machine settings is still not sufficiently explored. The main objective of this paper is to establish a twofold qualitative and quantitative methodology for optimal selection of a competent jobholder(s) to perform a certain task by semantic modeling and analysis of jobholder (human and machine) profiles corresponding to the task characteristics and learning requirements including knowledge, skills and competences (KSCs). The proposed knowledge-based approach comprises semantic modeling and quantitative methods focusing on measuring and correlating the level of human competences and machine autonomy, and identifying the extent of human–machine complementarity in performing an assigned task. The Vector of Competence and Autonomy (VCA) is built for identifying the extent of human–machine collaboration. The quantitative analysis involves several human factors in association to various combinations of technological components of Digital Assistance Systems with different automation degrees, under certain aspects of complexity of products and workplaces. Applying a set of rules and considering jobholder profiles, VCA values are interpreted and the current state of complementarity is inferred. Furthermore, feasible radical or incremental managerial transition pathways are identified as an initial step to reach the optimal (desired) level of human–machine collaboration in the example of TU Wien Pilot Factory Industry 4.0. © 2019 CIRP},
	journal = {CIRP Journal of Manufacturing Science and Technology},
	author = {Ansari, F. and Hold, P. and Khobreh, M.},
	year = {2020},
	note = {Publisher: Elsevier Ltd},
	keywords = {Automation, Autonomy, CPPS, Competence, Cyber Physical System, Human, Industry 4.0, Knowledge based systems, Knowledge-based approach, Quantitative methodology, Semantics, Skills and competences, Technological components, Technology transfer},
	pages = {87--106},
}

@article{altinisik_new_2012,
	title = {A new fault tolerant control approach for the three-tank system using data mining},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870057329&doi=10.1016%2fj.compeleceng.2012.06.011&partnerID=40&md5=33eea7c571cb3acae098978d05bfbfed},
	doi = {10.1016/j.compeleceng.2012.06.011},
	abstract = {In this study, we propose a knowledge-based approach for detection and isolation of sensor faults in fault tolerant control (FTC) of the three-tank system. Farthest first traversal algorithm (FFTA) of data mining is used first-time for the classification of faults in an FTC system. The sliding window is used to detect signal changes, which contain possible transients due to faults. The variance-changing ratio is calculated to extract the features of the sensor signal in each window. Then, FFTA is utilized for the isolation of sensor faults. In order to demonstrate the efficiency of the proposed method, seven types of artificial faults were applied to closed-loop fault tolerant control system in certain periods. All faults were detected and isolated immediately after they occurred. Moreover, fault isolation was achieved when multiple faults occurred simultaneously. © 2012 Elsevier Ltd. All rights reserved.},
	number = {6},
	journal = {Computers and Electrical Engineering},
	author = {Altinisik, U. and Yildirim, M.},
	year = {2012},
	keywords = {Closed-loop, Data mining, Fault isolation, Fault tolerant control, Fault tolerant control systems, Knowledge based systems, Knowledge-based approach, Multiple faults, Sensor fault, Sensor signals, Sensors, Signal changes, Sliding Window, Tanks (containers), Traversal algorithms},
	pages = {1627--1635},
}

@article{ren_kernel_2015,
	title = {Kernel fuzzy {C}-means clustering for word sense disambiguation in biomedical texts},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959203977&partnerID=40&md5=9bc27e007ec898b6bf5ebe7335ceb612},
	abstract = {Word sense disambiguation (WSD) in biomedical texts is important. The majority of existing research primarily focuses on supervised learning methods and knowledge-based approaches. Implementing these methods requires significant human-annotated corpus, which is not easily obtained. In this paper, we developed an unsupervised system for WSD in biomedical texts. First, we predefine the number of senses for an ambiguous word. Kernel fuzzy C-means clustering is used to group the same sense terms into a set. Each set is mapped to a certain sense to achieve disambiguation. Experimental results on all 50 ambiguous terms from NLM-WSD corpus demonstrate that our proposed system outperforms other unsupervised methods. Meanwhile, the kernel fuzzy C-means system is 5\% more precise than the state-of-art knowledge-based WSD system on the full NLM-WSD dataset. Our system is highly efficient and accurate for word sense disambiguation in biomedical texts and does not require human-annotated corpus.},
	number = {6},
	journal = {Journal of Digital Information Management},
	author = {Ren, K. and Ren, Y.F.},
	year = {2015},
	note = {Publisher: Digital Information Research Foundation},
	keywords = {Clustering, Fuzzy C mean, Fuzzy C means clustering, Fuzzy systems, Knowledge based, Knowledge based systems, Knowledge-based approach, Natural language processing systems, Supervised learning methods, Unsupervised learning, Unsupervised method, Word Sense Disambiguation},
	pages = {411--420},
}

@article{jimenez_word2set_2019,
	title = {Word2set: {WordNet}-{Based} {Word} {Representation} {Rivaling} {Neural} {Word} {Embedding} for {Lexical} {Similarity} and {Sentiment} {Analysis}},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064665562&doi=10.1109%2fMCI.2019.2901085&partnerID=40&md5=575ab4aeebddaa674cc066cf077be941},
	doi = {10.1109/MCI.2019.2901085},
	abstract = {Measuring lexical similarity using WordNet has a long tradition. In the last decade, it has been challenged by distributional methods, and more recently by neural word embedding. In recent years, several larger lexical similarity benchmarks have been introduced, on which word embedding has achieved state-of-the-art results. The success of such methods has eclipsed the use of WordNet for predicting human judgments of lexical similarity. We propose a new set cardinality-based method for measuring lexical similarity, which exploits the WordNet graph, obtaining a word representation, which we called word2set, based on related neighboring words. We show that the features extracted from set cardinalities computed using this word representation, when fed into a support vector regression classifier trained on a dataset of common synonyms and antonyms, produce results competitive with those of word-embedding approaches. On the task of predicting the lexical sentiment polarity, our WordNet set-based representation significantly outperforms the classical measures and achieves the performance of neural embeddings. Although word embedding is still the best approach for these tasks, our method significantly reduces the gap between the results shown by knowledge-based approaches and by distributional representations, without requiring a large training corpus. It is also more effective for less-frequent words. © 2005-2012 IEEE.},
	number = {2},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Jimenez, S. and Gonzalez, F.A. and Gelbukh, A. and Duenas, G.},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Classification (of information), Embeddings, Human judgments, Knowledge based systems, Knowledge-based approach, Lexical similarity, Ontology, Semantics, Sentiment analysis, Set cardinality, State of the art, Support vector regression (SVR), Training corpus, Word representations},
	pages = {41--53},
}

@article{chen_knowledge-based_2013,
	title = {A knowledge-based approach for carpal tunnel segmentation from magnetic resonance images},
	volume = {26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877786866&doi=10.1007%2fs10278-012-9530-2&partnerID=40&md5=96af3a7f2e51689ddb912c3b863ebb93},
	doi = {10.1007/s10278-012-9530-2},
	abstract = {Carpal tunnel syndrome (CTS) has been reported as one of the most common peripheral neuropathies. Carpal tunnel segmentation from magnetic resonance (MR) images is important for the evaluation of CTS. To date, manual segmentation, which is time-consuming and operator dependent, remains the most common approach for the analysis of the carpal tunnel structure. Therefore, we propose a new knowledge-based method for automatic segmentation of the carpal tunnel from MR images. The proposed method first requires the segmentation of the carpal tunnel from the most proximally cross-sectional image. Three anatomical features of the carpal tunnel are detected by watershed and polygonal curve fitting algorithms to automatically initialize a deformable model as close to the carpal tunnel in the given image as possible. The model subsequently deforms toward the tunnel boundary based on image intensity information, shape bending degree, and the geometry constraints of the carpal tunnel. After the deformation process, the carpal tunnel in the most proximal image is segmented and subsequently applied to a contour propagation step to extract the tunnel contours sequentially from the remaining cross-sectional images. MR volumes from 15 subjects were included in the validation experiments. Compared with the ground truth of two experts, our method showed good agreement on tunnel segmentations by an average margin of error within 1 mm and dice similarity coefficient above 0.9. © 2012 Society for Imaging Informatics in Medicine.},
	number = {3},
	journal = {Journal of Digital Imaging},
	author = {Chen, H.-C. and Wang, Y.-Y. and Lin, C.-H. and Wang, C.-K. and Jou, I.-M. and Su, F.-C. and Sun, Y.-N.},
	year = {2013},
	keywords = {Algorithms, Automated, Automatic segmentations, Carpal Tunnel Syndrome, Carpal tunnel, Computer-Assisted, Curve fitting, Deformable models, Deformation, Humans, Image Enhancement, Image Interpretation, Image intensity information, Image segmentation, Knowledge based systems, Knowledge-based approach, MR, Magnetic Resonance Imaging, Magnetic resonance, Magnetic resonance images, Magnetic resonance imaging, Pattern Recognition, Polygonal curve, Watersheds, algorithm, article, automated pattern recognition, carpal tunnel syndrome, computer assisted diagnosis, human, image enhancement, methodology, nuclear magnetic resonance imaging, procedures},
	pages = {510--520},
}

@article{khalastchi_sensor-based_2018,
	title = {A sensor-based approach for fault detection and diagnosis for robotic systems},
	volume = {42},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037365171&doi=10.1007%2fs10514-017-9688-z&partnerID=40&md5=112ec7da11a2df18b9358a14fe164049},
	doi = {10.1007/s10514-017-9688-z},
	abstract = {As we rely more on robots, thus it becomes important that a continuous successful operation is maintained. Unfortunately, these sophisticated, machines are susceptible to different faults. Some faults might quickly deteriorate into a catastrophe. Thus, it becomes important to apply a fault detection and diagnosis (FDD) mechanism such that faults will be diagnosed in time, allowing a recovery process. Yet, some types of robots require an FDD approach to be accurate, online, quick, able to detect unknown faults, computationally light, and practical to construct. Having all these features together challenges typical model-based, data-driven, and knowledge-based approaches. In this paper we present the SFDD approach that meets these requirements by combining model-based and data-driven techniques. The SFDD utilizes correlation detection, pattern recognition, and a model of structural dependencies. We present two different implementations of the SFDD. In addition, we introduce a new data set, to be used as a public benchmark for FDD, which is challenging due to the contextual nature of injected faults. We show the SFDD implementations are significantly more accurate than three competing approaches, on the benchmark, a physical robot, and a commercial UAV domains. Finally, we show the contribution of each feature of the SFDD. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {6},
	journal = {Autonomous Robots},
	author = {Khalastchi, E. and Kalech, M.},
	year = {2018},
	note = {Publisher: Springer New York LLC},
	keywords = {Correlation detection, Data driven, Data driven technique, Failure analysis, Fault detection, Fault detection and diagnosis, Knowledge based systems, Knowledge-based approach, Model-based OPC, Pattern recognition, Recovery process, Robotic systems, Robotics, Robots, Sensors},
	pages = {1231--1248},
}

@article{mehta_approach_2013,
	title = {An approach to determine important attributes for engineering change evaluation},
	volume = {135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877604616&doi=10.1115%2f1.4023551&partnerID=40&md5=5964ad115ae247b1cfc349c6b1ccdfec},
	doi = {10.1115/1.4023551},
	abstract = {Enterprises plan detailed evaluation of only those engineering change (EC) effects that might have a significant impact. Using past EC knowledge can prove effective in determining whether a proposed EC effect has significant impact. In order to utilize past EC knowledge, it is essential to identify important attributes that should be compared to compute similarity between ECs. This paper presents a knowledge-based approach for determining important EC attributes that should be compared to retrieve similar past ECs so that the impact of proposed EC effect can be evaluated. The problem of determining important EC attributes is formulated as the multi-objective optimization problem. Measures are defined to quantify importance of an attribute set. The knowledge in change database and the domain rules among attribute values are combined for computing the measures. An ant colony optimization (ACO)-based search approach is used for efficiently locating the set of important attributes. An example EC knowledge-base is created and used for evaluating the measures and the overall approach. The evaluation results show that our measures perform better than state-of-the-art evaluation criteria. Our overall approach is evaluated based on manual observations. The results show that our approach correctly evaluates the value of proposed change impact with a success rate of 83.33\%. © 2013 by ASME.},
	number = {4},
	journal = {Journal of Mechanical Design, Transactions of the ASME},
	author = {Mehta, C. and Patil, L. and Dutta, D.},
	year = {2013},
	keywords = {Ant Colony Optimization (ACO), Artificial intelligence, Attribute values, Engineering change evaluations, Engineering changes, Evaluation criteria, Evaluation results, Knowledge based systems, Knowledge-based approach, Multi-objective optimization problem},
}

@article{naganathan_coarse-grained_2013,
	title = {Coarse-grained models of protein folding as detailed tools to connect with experiments},
	volume = {3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881635500&doi=10.1002%2fwcms.1133&partnerID=40&md5=9f98f1945b834d4e294ff962dd61bc90},
	doi = {10.1002/wcms.1133},
	abstract = {Protein folding, a process that spans a wide range of timescales and that involves complex conformational dynamics, is an extremely challenging problem to decode at the atomic level. Over the past decade, coarse-grained (CG) models that rely on a reduced representation of the polymer chain as dictated by the native structure have been quite successful in characterizing and predicting a variety of aspects of the folding mechanism of single-domain proteins. The ever-increasing ability of this minimalist treatment is a primarily due to the rapid and efficient sampling afforded by coarse-graining that smoothens the folding landscape and the simple nature of the constituent physical energy functions that can be easily cast in various forms or parameterized using experimental or knowledge-based approaches. With the advances in computational power we have now reached a stage where CG simulations can be routinely performed to test various mechanistic hypothesis, to interpret intricate experimental observables and even suggest new experimental avenues. Here, we provide an overview of recent CG developments that have predicted experiments quantitatively and others that have sought to use experimental information as constraints to tune the energetics and answer fundamental questions in folding and conformational behavior of proteins. We further discuss open issues and point to new directions that can drive the CG models toward better agreement with experiments and to a better understanding of folding mechanisms in general. © 2012 John Wiley \& Sons, Ltd.},
	number = {5},
	journal = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
	author = {Naganathan, A.N.},
	year = {2013},
	keywords = {Coarse grained models, Computational power, Conformational behavior, Conformational dynamics, Efficient sampling, Experiments, Knowledge based systems, Knowledge-based approach, Protein folding, Proteins, Reduced representation, Single-domain proteins},
	pages = {504--514},
}

@article{pedrinaci_strategy-driven_2009,
	title = {Strategy-driven business process analysis},
	volume = {21 LNBIP},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-66249133699&doi=10.1007%2f978-3-642-01190-0_15&partnerID=40&md5=3b8d4855be6a747ae9bad1557f76025e},
	doi = {10.1007/978-3-642-01190-0_15},
	abstract = {Business Process Analysis (BPA) aims to verify, validate, and identify potential improvements for business processes. Despite the wide range of technologies developed so far, the large amount of information that needs to be integrated and processed, as well as the quantity of data that has to be produced and presented still poses important challenges both from a processing and presentation perspectives. We argue that to enhance BPA, semantics have to be the core backbone in order to better support the application of analysis techniques on the first hand, and to guide the computation and presentation of the results on the other hand. We propose a knowledge-based approach to supporting strategy-driven BPA by making use of a comprehensive and extensible ontological framework capturing from high-level strategic concerns down to lower-level monitoring information. We describe how corporate strategies can be operationalized into concrete analysis that can guide the evaluation of organisational processes, structure the presentation of results obtained and better help assess the well-being of corporate business processes. © 2009 Springer Berlin Heidelberg.},
	journal = {Lecture Notes in Business Information Processing},
	author = {Pedrinaci, C. and Markovic, I. and Hasibether, F. and Domingue, J.},
	year = {2009},
	note = {Publisher: Springer Verlag},
	keywords = {Amount of information, Analysis techniques, Business Process, Business Process Analysis, Concrete analysis, Corporate strategies, Enterprise resource management, Knowledge based systems, Knowledge-based approach, Monitoring information, Ontological frameworks, Semantics, Strategic Analysis},
	pages = {169--180},
}

@article{yau_customized_2011,
	title = {A customized smart {CAM} system for digital dentistry},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960995362&doi=10.3722%2fcadaps.2011.395-405&partnerID=40&md5=116fd78b55f62841c1d428b64c0ba3a8},
	doi = {10.3722/cadaps.2011.395-405},
	abstract = {This paper presents an automated system for NC tool-path planning and generation of digital dental restoration. The process planning of an automated system has to do with well-ordered approaches, and accurate, efficient manufacture of work-pieces or parts, from initial to finished stages. Traditionally, dental restoration making is a totally manual and labor-intensive work, and the computer aided design (CAD) /computer aided manufacturing (CAM) technology has rarely been utilized. In recent years, however, with the advancement of intra-oral scanning technology, more and more CAD/CAM technology is being applied to the design and production of dental restorations. Since the operators are dental technicians who are mostly unfamiliar with CAD/CAM, it is important to have a customized and highly automated dental CAD/CAM system for the digital dental industry. General-purpose CAM system cannot be used in this case because long hours of tool-path programming and editing are required. In order to reduce processing and editing time, a highly customized and "one-button" CAM system is developed in this work to reach automation and increase efficiency. The key is to capture and incorporate the domain knowledge of dental design and production into the tool-path programming process. In other words, the complicated process planning problem can be overcome by a knowledge-based approach, and the resulting machining plan can be realized by a 5-axis milling machine to directly produce dental restorations. In this paper, we show that machining sequences of several different dental restorations, from simple to complex, can all be automatically planned and produced by the proposed system. A comparison between the automated system and the traditional manual approach is also presented and discussed. © 2011 CAD Solutions, LLC.},
	number = {3},
	journal = {Computer-Aided Design and Applications},
	author = {Yau, H.-T. and Chen, H.-C. and Yu, P.-J.},
	year = {2011},
	keywords = {5-axis milling machine, Automated systems, Automation, CAM systems, Cad/cams, Cams, Computer aided design, Computer aided manufacturing, Dental CAD/CAM, Dental restorations, Dentistry, Digital dentistry, Domain knowledge, Filling, Knowledge based systems, Knowledge-based approach, Machining, Manufacture, Milling machines, Process control, Restoration, Tool path generation, Tool-path programming, Toolpaths, Work pieces},
	pages = {395--405},
}

@article{pasciuto_comparison_2014,
	title = {A comparison between optimization-based human motion prediction methods: {Data}-based, knowledge-based and hybrid approaches},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892797324&doi=10.1007%2fs00158-013-0960-3&partnerID=40&md5=d98bad045888249b172f3ed27bbe262a},
	doi = {10.1007/s00158-013-0960-3},
	abstract = {In this paper an optimization-based hybrid dynamic motion prediction method is presented. The method is hybrid as the prediction relies both on actually performed motions for reference (following a data-based approach) and on the definition of appropriate performance measures (following a knowledge-based approach). The prediction is carried out through the definition of a constrained non-linear optimization problem, in which the objective function is composed of a weighted combination of data-based and knowledge-based contributions. The weights of each contribution are varied in order to generate a battery of hybrid predictions, which range from purely data-based to purely knowledge-based. The results of the predictions are analyzed and compared against actually performed motions both qualitatively and quantitatively, using a measure of realism defined as the distance of the predicted motions from the mean of the actually performed motions. The method is applied to clutch pedal depression motions and the comparison between the different approaches favors the hybrid solution, which seems to combine the strengths of both data- and knowledge-based approaches, enhancing the realism of the predicted motion. © 2013 Springer-Verlag Berlin Heidelberg.},
	number = {1},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Pasciuto, I. and Ausejo, S. and Celigüeta, J.T. and Suescun, A. and Cazón, A.},
	year = {2014},
	keywords = {Constrained non-linear optimizations, Forecasting, Human motions, Hybrid predictions, Hybrid solution, Knowledge based systems, Knowledge-based approach, Motion prediction, Objective functions, Optimization, Performance measure},
	pages = {169--183},
}

@article{basu_situational_2016,
	title = {Situational awareness for the electrical power grid},
	volume = {60},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978946761&doi=10.1147%2fJRD.2015.2498818&partnerID=40&md5=72a51f9e9cb21c318530000b823ed0e3},
	doi = {10.1147/JRD.2015.2498818},
	abstract = {With deployment of high-Throughput, low-latency sensors such as PMUs (phasor measurement units), utilities have an opportunity to achieve high-resolution 'visibility' into the state of the electrical power grid at any time. In this paper, we revisit situational awareness-a term with origins in reconnaissance and mission planning-and develop a knowledge-based approach for monitoring the electrical power grid that combines both static and dynamic sources of information to enable better comprehension and decision support. At the core of this approach is an abstraction layer for representing and interpreting the granular sensor data reported by PMUs as power system events. We describe how this abstraction layer can be used to develop a cognitive model of the grid operator, engineer, or analyst, and, ultimately, to filter, interpret, and efficiently summarize grid behaviors. We also describe interfaces that we developed and discuss some actual utility use cases implemented in partnership with Hydro-Québec, Canada's largest electricity generator by a utility and one of the world's largest producers of clean energy with the largest transmission system in North America. © 1957-2012 IBM.},
	number = {1},
	journal = {IBM Journal of Research and Development},
	author = {Basu, C. and Padmanaban, M. and Guillon, S. and Cauchon, L. and De Montigny, M. and Kamwa, I.},
	year = {2016},
	note = {Publisher: IBM Corporation},
	keywords = {Abstracting, Abstraction layer, Decision support systems, Decision supports, Electric power transmission, Electric power transmission networks, Electricity generators, Knowledge based systems, Knowledge-based approach, Mission planning, Phasor measurement units, Situational awareness, Sources of informations, Transmission systems, Units of measurement},
}

@article{donalds_toward_2019,
	title = {Toward a cybercrime classification ontology: {A} knowledge-based approach},
	volume = {92},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059550621&doi=10.1016%2fj.chb.2018.11.039&partnerID=40&md5=56783d0523faaf1b4662834a83ca0016},
	doi = {10.1016/j.chb.2018.11.039},
	abstract = {In recent years there has been an increase in cybercrimes and its negative impacts on the lives of individuals, organizations, and governments. It has been argued that a better understanding of cybercrime is a necessary condition to develop appropriate legal and policy responses to cybercrime. While a universally agreed-upon classification scheme would facilitate the development of such understanding and also collaborations, current classification schemes are insufficient, fragmented and often incompatible since each focuses on different perspectives (e.g., role of the computer, attack, attacker's or defender's viewpoint), or uses varying terminologies to refer to the same thing, making consistent cybercrime classifications improbable. In this paper we present and illustrate a new cybercrime ontology that incorporates multiple perspectives and offers a more holistic viewpoint for cybercrime classification than prior works. It should therefore prove to be a more useful tool for cybercrime stakeholders. © 2018 Elsevier Ltd},
	journal = {Computers in Human Behavior},
	author = {Donalds, C. and Osei-Bryson, K.-M.},
	year = {2019},
	note = {Publisher: Elsevier Ltd},
	keywords = {Classification (of information), Classification scheme, Computer crime, Cyber-crimes, Cybercrime, Design science, Knowledge based systems, Knowledge-based approach, Ontology, article, human, ontology},
	pages = {403--418},
}

@article{jin_intelligent_2009,
	title = {Intelligent assembly time analysis using a digital knowledge-based approach},
	volume = {6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-69649103521&doi=10.2514%2f1.41231&partnerID=40&md5=a9a33d14ee29a2f52cc1eacf7cc37ba5},
	doi = {10.2514/1.41231},
	abstract = {The implementation of effective time analysis methods which are both fast and accurate, in the era of digital manufacturing, has become a significant challenge for manufacturers hoping to build and maintain a competitive advantage. This paper proposes a structure oriented, knowledge-based approach for intelligent time analysis for complex, engineering assembly processes within a digital manufacturing framework. The approach combines three important aspects: hierarchical object oriented structure, knowledge modeling, and implementation within a digital manufacturing framework. The hierarchical structure facilitates the capture of work breakdown structure, automated time analysis, and effective information management. Knowledge modeling enables the intelligent generation of manufacturing cycle times from design parameters. The implementation of a digital design and manufacturing platform with integrated time analysis capability through intuitive graphical user interfaces and configured functionalities, provides a truly collaborative methodology and concurrent engineering tool for productivity improvement. An exemplar study using an aircraft panel assembly from a regional jet is presented. Although the method currently focuses on aerospace assembly, it can also be applied to many other industry sectors including automotive, railway carriage, and large scale marine manufacturing. The main contribution of the work is to present a methodology that facilitates the integration of time analysis with product design and assembly process definition using a digital manufacturing solution. © 2009.},
	number = {8},
	journal = {Journal of Aerospace Computing, Information and Communication},
	author = {Jin, Y. and Curran, R. and Butterfield, J. and Burke, R. and Welch, B.},
	year = {2009},
	keywords = {Aircraft, Aircraft manufacture, Aircraft panels, Assembly, Assembly process, Assembly time, Competition, Competitive advantage, Concurrent engineering, Design parameters, Digital design and manufacturing, Digital manufacturing, Digital manufacturing solutions, Effective time, Graphical user interfaces, Hierarchical structures, Industrial railroads, Industry sectors, Information management, Knowledge based systems, Knowledge engineering, Knowledge modeling, Knowledge-based approach, Manufacturing cycle, Object oriented, Product design, Productivity, Productivity improvements, Railway carriages, Time analysis, Work breakdown structure},
	pages = {506--522},
}

@article{kravari_cross-community_2012,
	title = {Cross-community interoperation between knowledge-based multi-agent systems: {A} study on {EMERALD} and {Rule} {Responder}},
	volume = {39},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859218303&doi=10.1016%2fj.eswa.2012.02.160&partnerID=40&md5=e027dc80e036344bdeaf461f33fc1302},
	doi = {10.1016/j.eswa.2012.02.160},
	abstract = {The ultimate vision of the Semantic Web (SW) is to provide users with the capability of delegating complex tasks to intelligent agents. The latter, acting in an interoperable and information-rich Web environment, will efficiently satisfy their users' requests in a variety of real-life applications. Much work has been done on SW information agents for Web-based query answering; a variety of multi-agent platforms and Web language standards has been proposed. However, the platform- and language-bridging interoperation across multi-agent systems has been neglected so far, although it will be vital for large-scale agent deployment and wide-spread adoption of agent technology by human users. This article defines the space of possible interoperation methods for heterogeneous multi-agent systems based on the communication type, namely symmetric or asymmetric, and the MASs status, namely open or closed systems. It presents how heterogeneous multi-agent systems can use one of these methods to interoperate and, eventually, automate collaboration across communities. The method is exemplified with two SW-enabled multi-agent systems, EMERALD and Rule Responder, which assist communities of users based on declarative SW and multi-agent standards such as RDF, OWL, RuleML, and FIPA. This interoperation employs a declarative, knowledge-based approach, which enables information agents to make smart and consistent decisions, relying on high-quality facts and rules. Multi-step interaction use cases between agents from both communities are presented, demonstrating the added value of interoperation. © 2012 Elsevier Ltd. All rights reserved.},
	number = {10},
	journal = {Expert Systems with Applications},
	author = {Kravari, K. and Bassiliades, N. and Boley, H.},
	year = {2012},
	keywords = {Added values, Agent technology, Closed systems, Complex task, Computer programming languages, Cross-community, EMERALD, Gems, High quality, Human users, Information agents, Intelligent agents, Intelligent multi agent systems, Interoperations, Knowledge based systems, Knowledge-based approach, Multi agent system (MAS), Multi agent systems, Multi-agent platforms, Multi-step, Query answering, Real-life applications, Rule Responder, Semantic Web, Web environment, Web languages},
	pages = {9571--9587},
}

@article{hullermeier_knowledge-based_2015,
	title = {From knowledge-based to data-driven fuzzy modeling: {Development}, criticism, and alternative directions},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949113037&doi=10.1007%2fs00287-015-0931-8&partnerID=40&md5=c588d118d1ce7f17a4a7aa6258e0c5fd},
	doi = {10.1007/s00287-015-0931-8},
	abstract = {This paper elaborates on a development in (applied) fuzzy logic that has taken place in the last couple of decades, namely, the complementation or even replacement of the traditional knowledge-based approach to fuzzy rule-based systems design by a data-driven one. It is argued that the classical rule-based modeling paradigm is actually more amenable to the knowledge-based approach, for which it was originally conceived, and less so to data-driven model design. An important reason that prevents fuzzy (rule-based) systems from being leveraged in large-scale applications is the flat structure of rule bases, along with the local nature of fuzzy rules and their limited ability to express complex dependencies between variables. As an alternative approach to fuzzy systems modeling, we advocate so-called fuzzy pattern trees. Because of its hierarchical, modular structure and the use of different types of (nonlinear) aggregation operators, a fuzzy pattern tree has the ability to represent functional dependencies in a more flexible and more compact way. © 2015, Springer-Verlag Berlin Heidelberg.},
	number = {6},
	journal = {Informatik-Spektrum},
	author = {Hüllermeier, E.},
	year = {2015},
	note = {Publisher: Springer Verlag},
	keywords = {Aggregation operator, Data driven fuzzy modeling, Data-driven model, Functional dependency, Fuzzy inference, Fuzzy logic, Fuzzy rules, Knowledge based systems, Knowledge-based approach, Large-scale applications, Mathematical operators, Modular structures, Structural design, Traditional knowledge},
	pages = {500--509},
}

@article{torricelli_-house_2016,
	title = {In-house {Validation} of a {DNA} {Extraction} {Protocol} from {Honey} and {Bee} {Pollen} and {Analysis} in {Fast} {Real}-{Time} {PCR} of {Commercial} {Honey} {Samples} {Using} a {Knowledge}-{Based} {Approach}},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969790761&doi=10.1007%2fs12161-016-0539-x&partnerID=40&md5=0b2fae1d93dae5d940ff9275672cc1df},
	doi = {10.1007/s12161-016-0539-x},
	abstract = {For consumers, honey is a natural product that should not be subjected to treatment or alteration. Since the question of the presence of genetically modified organisms concerned pollen in honey, the aim of this research was to find an alternative, however, practical and efficient method of honey and bee pollen DNA extraction for routine analysis application. Furthermore, to evaluate the extracted DNA, a real-time PCR system based on the actin gene was optimized and validated in fast mode for the first time to reduce analysis time. To develop an alternative DNA extraction protocol, two already published procedures were combined and tested with some variations, in particular without beads/filters used to grind/enrich pollen. The best approach found in terms of quantity and quality of extracted DNA was a combination of the pretreatment and the extraction method, described in the German guideline, with some modifications and the addition of a DNA purification kit. This protocol was validated with DNA extracts from honey and bee pollen and it was applied to 18 commercial honey samples. Furthermore, a sample proved positive in transgenic screening elements analysis and for transgenic event identification, a knowledge-based approach was adopted. Since the DNA extraction protocol proved suitable, it could be applied for other analysis such as molecular species characterization, the study of traceability, and environmental monitoring, considering honey as a vector of authorized and not authorized genetically modified organisms. © 2016, Springer Science+Business Media New York.},
	number = {12},
	journal = {Food Analytical Methods},
	author = {Torricelli, M. and Pierboni, E. and Tovo, G.R. and Curcio, L. and Rondini, C.},
	year = {2016},
	note = {Publisher: Springer New York LLC},
	keywords = {Biology, DNA, DNA extraction, DNA sequences, Extraction, Fast real-time pcr, Food products, Genes, Genetically modified organisms, Honey, Knowledge based systems, Knowledge-based approach, Polymerase chain reaction, Proteins, Screening},
	pages = {3439--3450},
}

@article{sathyanarayana_pool_2012,
	title = {Pool boiling of {HFE} 7200-{C4H4F6O} mixture on hybrid micro-nanostructured surface},
	volume = {3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881153426&doi=10.1115%2f1.4023245&partnerID=40&md5=b32c0d91489711e41b3d75e01c51cefe},
	doi = {10.1115/1.4023245},
	abstract = {Steadily increasing heat dissipation in electronic devices has generated renewed interest in direct immersion cooling. The ideal heat transfer fluid for direct immersion cooling applications should be chemically and thermally stable, and compatible with the electronic components. These constraints have led to the use of Novec fluids and fluroinerts as coolants. Although these fluids are chemically stable and have low dielectric constants, they are plagued by poor thermal properties like low thermal conductivity (about twice that of air) and low specific heat (same as that of air). These factors necessitate the development of new heat transfer fluids with improved heat transfer properties and applicability. C4H4F6O is a new heat transfer fluid which has been identified using computer-aided molecular design (CAMD) and knowledge-based approaches. A mixture of Novec fluid (HFE 7200) with C4H4F6O is evaluated in this study. Pool boiling experiments are performed at saturated condition on a 10mm × 10mm silicon test chip with CuO nanostructures on a microgrooved surface, to investigate the thermal performance of this new fluid mixture. The mixture increased the critical heat flux moderately by 8.4\% over pure HFE 7200. Additional investigation is necessary before C4H4F 6O can be considered for immersion cooling applications. Copyright © 2013 by ASME.},
	number = {4},
	journal = {Journal of Nanotechnology in Engineering and Medicine},
	author = {Sathyanarayana, A. and Warrier, P. and Im, Y. and Joshi, Y. and Teja, A.S.},
	year = {2012},
	keywords = {1, 1 trifluoroethyl)ether fluid, Air, Computer aided molecular design, Cooling, HFE 7200 bis(1, HFE 7200 fluid, Heat transfer, Heat transfer properties, Knowledge based systems, Knowledge-based approach, Low dielectric constants, Low thermal conductivity, Microgrooved surface, Mixtures, Nanostructures, Pool boiling, Saturated conditions, article, calibration, comparative study, computer aided design, computer aided molecular design, concentration (parameters), contact angle, controlled study, cooling, copper oxide nanomaterial, critical heat flux, density, direct immersion cooling, experimental study, heat tolerance, heat transfer, investigative procedures, laboratory device, miscellaneous drugs and agents, nanomaterial, pool boiling, predictive value, silicon test chip, surface property, surface tension, temperature related phenomena, thermal conductivity, unclassified drug, viscosity},
}

@article{opritescu_automated_2015,
	title = {Automated driving for individualized sheet metal part production - {A} neural network approach},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937760753&doi=10.1016%2fj.rcim.2015.03.006&partnerID=40&md5=3976dc7591fcb98e6dd07cb16b97130b},
	doi = {10.1016/j.rcim.2015.03.006},
	abstract = {The manufacturing of individualized sheet metal components is one of the most important issues in industrial sheet metal working. Incremental forming methods, in particular driving, offer the opportunity for achieving this objective. However, these manual processes are very difficult to automate, as a result of their complexity and user interactivity. To resolve this problem, a knowledge-based approach is presented, which utilizes a special type of driving process. Initially, a neural network architecture is established which delivers manufacturing strategies allowing part production for simple component shapes. After providing a method for training data generation, training sessions are carried out. Strategies, computed by trained networks, are adopted for processing sheet blanks which are used for evaluating the framework. Finally, the developed procedure is generalized, and a concept is designed which allows a transfer, in order to facilitate the production of arbitrary individualized sheet metal parts. © 2015 Elsevier Ltd.},
	journal = {Robotics and Computer-Integrated Manufacturing},
	author = {Opritescu, D. and Volk, W.},
	year = {2015},
	note = {Publisher: Elsevier Ltd},
	keywords = {Automated driving, Complex networks, Flexible manufacturing systems, Incremental forming, Incremental sheet forming, Knowledge based systems, Knowledge-based approach, Manufacture, Manufacturing strategy, Metal working, Metals, Network architecture, Neural networks, Sheet metal, Sheet metal components, Sheet metal parts, Toolpaths},
	pages = {144--150},
}

@article{loitsch_knowledge-based_2017,
	title = {A knowledge-based approach to user interface adaptation from preferences and for special needs},
	volume = {27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032464546&doi=10.1007%2fs11257-017-9196-z&partnerID=40&md5=e849996ba24ccecc8da8db8db1870227},
	doi = {10.1007/s11257-017-9196-z},
	abstract = {Moving between devices is omnipresent, but not for people with disabilities or those who require specific accessibility options. Setting up assistive technologies or finding settings to overcome a certain barrier can be a demanding task for people without technical skills. Context-sensitive adaptive user interfaces are advancing, although migrating access features from one device to another is very rarely addressed. In this paper, we describe the knowledge-based component of the Global Public Inclusive Infrastructure that infers how a device shall be best configured at the operating system layer, the application layer and the web layer to meet the requirements of a user including possible special needs or disabilities. In this regard, a mechanism to detect and resolve conflicting accessibility policies as well as recommending preference substitutes is a main requirement, as elaborated in this paper. As the proposed system emulates decision-making of accessibility experts, we validated the automatic deduced configurations against manual configurations of ten accessibility experts. The assessment result shows that the average matching score of the developed system is high. Thus, the proposed system can be considered capable of making precise decisions towards personalizing user interfaces based on user needs and preferences. © 2017, Springer Science+Business Media B.V.},
	number = {3-5},
	journal = {User Modeling and User-Adapted Interaction},
	author = {Loitsch, C. and Weber, G. and Kaklanis, N. and Votis, K. and Tzovaras, D.},
	year = {2017},
	note = {Publisher: Springer Netherlands},
	keywords = {Accessibility, Adaptive user interface, Application layers, Assistive technology, Behavioral research, Context sensitive, Decision making, Expert systems, Interface adaptation, Knowledge based systems, Knowledge-based approach, People with disabilities, Transportation, User interfaces},
	pages = {445--491},
}

@article{preece_sentinel_2018,
	title = {Sentinel: {A} {Codesigned} {Platform} for {Semantic} {Enrichment} of {Social} {Media} {Streams}},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039792545&doi=10.1109%2fTCSS.2017.2763684&partnerID=40&md5=6353a5ebe0eae018e5a0b6b4a9ac7d33},
	doi = {10.1109/TCSS.2017.2763684},
	abstract = {We introduce the Sentinel platform that supports semantic enrichment of streamed social media data for the purposes of situational understanding. The platform is the result of a codesign effort between computing and social scientists, iteratively developed through a series of pilot studies. The platform is founded upon a knowledge-based approach, in which input streams (channels) are characterized by spatial and terminological parameters, collected media is preprocessed to identify significant terms (signals), and data are tagged (framed) in relation to an ontology. Interpretation of processed media is framed in terms of the 5W framework (who, what, when, where, and why). The platform is designed to be open to the incorporation of new processing modules, building on the knowledge-based elements (channels, signals, and framing ontology) and accessible via a set of user-facing apps. We present the conceptual architecture for the platform, discuss the design and implementation challenges of the underlying stream-processing system, and present a number of apps developed in the context of the pilot studies, highlighting the strengths and importance of the codesign approach and indicating promising areas for future research. © 2014 IEEE.},
	number = {1},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Preece, A. and Spasic, I. and Evans, K. and Rogers, D. and Webberley, W. and Roberts, C. and Innes, M.},
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Conceptual architecture, Data structures, Design and implementations, Interactive computer systems, Knowledge based systems, Knowledge-based approach, Media, Media streaming, Natural language processing systems, Ontology, Real time systems, Semantics, Social computing, Social networking (online), Stream processing systems, Streaming media, Twitter, User centered design},
	pages = {118--131},
}

@article{kao_knowledge-based_1990,
	title = {Knowledge-based approach to the optimal dock arrangement},
	volume = {21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-5544277984&doi=10.1080%2f00207729008910542&partnerID=40&md5=3026d46cceacee601ecc7058a27095cc},
	doi = {10.1080/00207729008910542},
	abstract = {A problem frequently encountered in ports is the sequencing of waiting ships for berthing to load or unload so that the total demurrage cost incurred is minimized. Here a knowledge-based approach is applied to solve this problem. The constraints of the port and the working rules adopted by the port are expressed as knowledge rules and embedded into the framework of the logic of dock arrangement. With the inference mechanism of a knowledge engineering language OPS5, the best arrangement of the docks can be inferred. Since the operation rules differ from port to port, the problem is confined to the material docks of the China Steel Corporation. A system entitled DOCK is designed to find the best three available alternatives. The idea can be applied and the DOCK system modified for use by other ports. © 1990 Taylor \& Francis Group, LLC.},
	number = {11},
	journal = {International Journal of Systems Science},
	author = {Kao, C. and Li, D.-C. and Wu, C. and Tsai, C.-C.},
	year = {1990},
	keywords = {China steel corporations, Docks, Hydraulic structures, Inference mechanism, Knowledge based systems, Knowledge rules, Knowledge-based approach, Operation rules, Port-to-port, Problem solving},
	pages = {2209--2215},
}

@article{wu_knowledge-based_2013,
	title = {Knowledge-based approach to assembly sequence planning for wind-driven generator},
	volume = {2013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884826615&doi=10.1155%2f2013%2f908316&partnerID=40&md5=066633c30062ad3c5f8d5eafc084f757},
	doi = {10.1155/2013/908316},
	abstract = {Assembly sequence planning plays an essential role in the manufacturing industry. However, there still exist some challenges for the research of assembly planning, one of which is the weakness in effective description of assembly knowledge and information. In order to reduce the computational task, this paper presents a novel approach based on engineering assembly knowledge to the assembly sequence planning problem and provides an appropriate way to express both geometric information and nongeometric knowledge. In order to increase the sequence planning efficiency, the assembly connection graph is built according to the knowledge in engineering, design, and manufacturing fields. Product semantic information model could offer much useful information for the designer to finish the assembly (process) design and make the right decision in that process. Therefore, complex and low-efficient computation in the assembly design process could be avoided. Finally, a product assembly planning example is presented to illustrate the effectiveness of the proposed approach. Initial experience with the approach indicates the potential to reduce lead times and thereby can help in completing new product launch projects on time. © 2013 Meiping Wu et al.},
	journal = {Mathematical Problems in Engineering},
	author = {Wu, M. and Zhao, Y. and Wang, C.},
	year = {2013},
	keywords = {Assembly, Assembly sequence planning, Assembly sequence planning problem, Computational task, Connection graphs, Geometric information, Knowledge based systems, Knowledge-based approach, Manufacturing fields, Manufacturing industries, Product design},
}

@article{fufezan_p3d_2009,
	title = {P3d - {Python} module for structural bioinformatics},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349731764&doi=10.1186%2f1471-2105-10-258&partnerID=40&md5=893689ca861ef1d0eee1e05c8002cd7f},
	doi = {10.1186/1471-2105-10-258},
	abstract = {Background: High-throughput bioinformatic analysis tools are needed to mine the large amount of structural data via knowledge based approaches. The development of such tools requires a robust interface to access the structural data in an easy way. For this the Python scripting language is the optimal choice since its philosophy is to write an understandable source code. Results: p3d is an object oriented Python module that adds a simple yet powerful interface to the Python interpreter to process and analyse three dimensional protein structure files (PDB files). p3d's strength arises from the combination of a) very fast spatial access to the structural data due to the implementation of a binary space partitioning (BSP) tree, b) set theory and c) functions that allow to combine a and b and that use human readable language in the search queries rather than complex computer language. All these factors combined facilitate the rapid development of bioinformatic tools that can perform quick and complex analyses of protein structures. Conclusion: p3d is the perfect tool to quickly develop tools for structural bioinformatics using the Python scripting language. © 2009 Fufezan and Specht; licensee BioMed Central Ltd.},
	journal = {BMC Bioinformatics},
	author = {Fufezan, C. and Specht, M.},
	year = {2009},
	note = {Publisher: BioMed Central},
	keywords = {Binary space partitioning trees, Binary trees, Bioinformatic analysis, Bioinformatic tools, Bioinformatics, C (programming language), Knowledge based systems, Knowledge-based approach, Optimal systems, Protein structures, Proteins, Python scripting languages, Structural bioinformatics, Three dimensional protein structures, Trees (mathematics), access to information, article, computer interface, computer language, computer program, data analysis, data mining, high throughput screening, information processing, protein structure, structural bioinformatics, theory},
	pages = {258},
}

@article{durupt_knowledge_2010,
	title = {Knowledge based reverse engineering - {An} approach for reverse engineering of a mechanical part},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650563635&doi=10.1115%2f1.3482059&partnerID=40&md5=0152741023176d0a46be561cfecdf69d},
	doi = {10.1115/1.3482059},
	abstract = {Reverse engineering (RE) is a domain of current interest where physical models are measured or digitized in order to obtain a virtual model. Currently, these virtual models are rebuilt using approaches that consider only a geometric point of view. These models are generally frozen (i.e., poorly parameterized and not easy to modify). These kinds of models are good for many applications but people need more for redesign operations. This paper proposes a knowledge-based approach for reverse engineering that enables a RE user to rebuild nonfrozen models that are close to an original CAD model. © 2010 American Society of Mechanical Engineers.},
	number = {4},
	journal = {Journal of Computing and Information Science in Engineering},
	author = {Durupt, A. and Remy, S. and Ducellier, G.},
	year = {2010},
	keywords = {CAD models, Computer aided design, Feature extraction, KBE, Knowledge based systems, Knowledge-based approach, Mechanical parts, Models, Nonfrozen, Parameterized, Physical model, Product design, Reverse engineering, Virtual models, segmentation},
}

@article{faust_dynamic_2019,
	title = {Dynamic optimization of a two-stage emulsion polymerization to obtain desired particle morphologies},
	volume = {359},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057721518&doi=10.1016%2fj.cej.2018.11.081&partnerID=40&md5=5232cd7e983bec0093cd24d7065de3bd},
	doi = {10.1016/j.cej.2018.11.081},
	abstract = {Particle morphology is a key property of hybrid waterborne polymer dispersions that are used in high-end applications. However, evolving these product-by-process material properties is still mainly achieved by trial-and-error guided semi-empirical knowledge. In this work, a recently published mathematical model for the development of the particle morphology and a dynamic optimization method based on the direct single shooting approach are used to demonstrate in silico the feasibility of process optimization aimed at controlling particle morphology of waterborne polymer dispersions. This knowledge-based approach is assessed for existing and new materials with distinct particle morphologies. The batch times can be reduced by up to 13\%. © 2018},
	journal = {Chemical Engineering Journal},
	author = {Faust, J.M.M. and Hamzehlou, S. and Leiza, J.R. and Asua, J.M. and Mhamdi, A. and Mitsos, A.},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Dispersions, Dynamic optimization, Emulsification, Emulsion polymerization, Knowledge based systems, Knowledge-based approach, Mathematical morphology, Morphology, Optimization, Particle morphologies, Process control, Process materials, Semi-empirical, Single-shooting approach, Trial and error, Waterborne polymers},
	pages = {1035--1045},
}

@article{walker_knowledge-based_1995,
	title = {A knowledge-based systems approach to agroforestry research and extension},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028979026&partnerID=40&md5=a191b129e1d6570ed54b65783ce38138},
	number = {3},
	journal = {AI Applications},
	author = {Walker, D.H. and Sinclair, F.L. and Kendon, G.},
	year = {1995},
	keywords = {agroforestry research, agroforestry research and extension, computer program, decision-support system, forest management, knowledge based system, knowledge-based approach, planning tool},
	pages = {61--72},
}

@article{prats_framework_2010,
	title = {A framework for compliant physical interaction: {The} grasp meets the task},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549086053&doi=10.1007%2fs10514-009-9145-8&partnerID=40&md5=b6b6cb5e1ee0716c6ed47aeb412d4fd7},
	doi = {10.1007/s10514-009-9145-8},
	abstract = {Although the grasp-task interplay in our daily life is unquestionable, very little research has addressed this problem in robotics. In order to fill the gap between the grasp and the task, we adopt the most successful approaches to grasp and task specification, and extend them with additional elements that allow to define a grasp-task link. We propose a global sensor-based framework for the specification and robust control of physical interaction tasks, where the grasp and the task are jointly considered on the basis of the task frame formalism and the knowledge-based approach to grasping. A physical interaction task planner is also presented, based on the new concept of task-oriented hand preshapes. The planner focuses on manipulation of articulated parts in home environments, and is able to specify automatically all the elements of a physical interaction task required by the proposed framework. Finally, several applications are described, showing the versatility of the proposed approach, and its suitability for the fast implementation of robust physical interaction tasks in very different robotic systems. © 2009 Springer Science+Business Media, LLC.},
	number = {1},
	journal = {Autonomous Robots},
	author = {Prats, M. and Sanz, P.J. and Del Pobil, A.P.},
	year = {2010},
	keywords = {Daily lives, Fast implementation, Home environment, Knowledge based systems, Knowledge-based approach, Little research, New concept, Physical interactions, Robotic systems, Robotics, Robust control, Sensor-based control, Sensors, Specifications, Task frame formalisms, Task planner, Task specifications},
	pages = {89--111},
}

@article{gong_study_2013,
	title = {Study on routing protocols for delay tolerant mobile networks},
	volume = {2013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873372502&doi=10.1155%2f2013%2f145727&partnerID=40&md5=b35c5f169b25af21c4e6ef25f35a3db7},
	doi = {10.1155/2013/145727},
	abstract = {Delay tolerant mobile networks feature with intermittent connectivity, huge transmission delay, nodal mobility, and so forth. There is usually no end-to-end path in the networks and it poses great challenges for routing in DTMNs. In this paper, the architecture of DTMNs is introduced at first, including the characteristics of DTMNs, routing challenges, and metric and mobility models. And then, the state-of-the-art routing protocols for DTMNs are discussed and analyzed. Routing strategies are classified into three categories: nonknowledge-based approach, knowledge-based approach, and social-based approach. Finally, some research issues about DTMNs are presented. © 2013 Haigang Gong and Lingfei Yu.},
	journal = {International Journal of Distributed Sensor Networks},
	author = {Gong, H. and Yu, L.},
	year = {2013},
	keywords = {Delay tolerant mobile networks, End-to-end path, Intermittent connectivity, Knowledge based systems, Knowledge-based approach, Mobility model, Research issues, Routing protocols, Routing strategies, Social-based, Transmission delays},
}

@article{vlachidis_knowledge-based_2016,
	title = {A knowledge-based approach to {Information} {Extraction} for semantic interoperability in the archaeology domain},
	volume = {67},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963614867&doi=10.1002%2fasi.23485&partnerID=40&md5=b78bc6061d9ff04062b7103b2faf27f4},
	doi = {10.1002/asi.23485},
	abstract = {The article presents a method for automatic semantic indexing of archaeological grey-literature reports using empirical (rule-based) Information Extraction techniques in combination with domain-specific knowledge organization systems. The semantic annotation system (OPTIMA) performs the tasks of Named Entity Recognition, Relation Extraction, Negation Detection, and Word-Sense Disambiguation using hand-crafted rules and terminological resources for associating contextual abstractions with classes of the standard ontology CIDOC Conceptual Reference Model (CRM) for cultural heritage and its archaeological extension, CRM-EH. Relation Extraction (RE) performance benefits from a syntactic-based definition of RE patterns derived from domain oriented corpus analysis. The evaluation also shows clear benefit in the use of assistive natural language processing (NLP) modules relating to Word-Sense Disambiguation, Negation Detection, and Noun Phrase Validation, together with controlled thesaurus expansion. The semantic indexing results demonstrate the capacity of rule-based Information Extraction techniques to deliver interoperable semantic abstractions (semantic annotations) with respect to the CIDOC CRM and archaeological thesauri. Major contributions include recognition of relevant entities using shallow parsing NLP techniques driven by a complimentary use of ontological and terminological domain resources and empirical derivation of context-driven RE rules for the recognition of semantic relationships from phrases of unstructured text. © 2015 ASIS\&T.},
	number = {5},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Vlachidis, A. and Tudhope, D.},
	year = {2016},
	note = {Publisher: John Wiley and Sons Inc.},
	keywords = {Abstracting, Character recognition, Computational linguistics, Data mining, Domain-specific knowledge, Indexing (of information), Information analysis, Information extraction techniques, Information retrieval, Knowledge based systems, Knowledge-based approach, NAtural language processing, Natural language processing systems, Semantic analysis, Semantic interoperability, Semantics, Syntactics, Terminology, Text mining, Text processing, Thesauri, Word Sense Disambiguation, archeology, extraction, human, human experiment, inheritance, linguistics, model, natural language processing, ontology, recognition, validation process},
	pages = {1138--1152},
}

@article{karimi_knowledge-based_2017,
	title = {A knowledge-based approach for multi-factory production systems},
	volume = {77},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979877997&doi=10.1016%2fj.cor.2016.07.003&partnerID=40&md5=13a7c2b2f4fbe5772239ae61ec4d7b39},
	doi = {10.1016/j.cor.2016.07.003},
	abstract = {This paper investigates scheduling of jobs with deadlines across a serial multi-factory supply chain which involves minimizing sum of total tardiness and total transportation costs. Jobs can be transported among factories and can be delivered to the customer in batches which have limited capacity. The aim of this optimization problem is threefold: (1) determining the number of batches, (2) assigning jobs to batches, and (3) scheduling the batches production and delivery in each factory. The proposed problem formulated as a mixed-integer linear program. Then the model's performance is analyzed and evaluated through two examples. Moreover, a knowledge-based imperialistic competitive algorithm (KBICA) is also presented to find an approximate optimum solution for the problem. Computational experiments of the proposed problem investigate the efficiency of the method through different sizes of the test problems. © 2016 Elsevier Ltd},
	journal = {Computers and Operations Research},
	author = {Karimi, N. and Davoudpour, H.},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Batch deliveries, Imperialist competitive algorithms, Integer programming, Knowledge based systems, Knowledge-based approach, Mixed integer programming, Optimization, Production system, Scheduling, Scheduling algorithms, Supply chains},
	pages = {72--85},
}

@article{li_knowledge-based_2014,
	title = {Knowledge-based approach for reservoir system optimization},
	volume = {140},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921286536&doi=10.1061%2f%28ASCE%29WR.1943-5452.0000379&partnerID=40&md5=0b932e998bb8cc2f61c6a7064f143021},
	doi = {10.1061/(ASCE)WR.1943-5452.0000379},
	abstract = {This paper addresses a knowledge-based approach for reservoir system optimization. The approach takes a more detailed consideration of each turbine in a hydropower plant than the traditional constant output coefficient method. To use this approach, a knowledge expression and a knowledge function are defined for hydropower plant operation. The knowledge expression is extracted by dynamic programming to save all possibly optimal situations of unit commitment, and further, the knowledge function is formulated based on a two-dimensional interpolation of the knowledge expression. Through the use of the knowledge expression and the knowledge function, computer memory requirements can be reduced and unnecessary computations can be avoided in the reservoir operation optimization. To overcome the decomposition schemes in time and in space and guarantee finding the global optimum (in a discrete sense) with an extended time horizon, up to 400 CPU cores are used to run a parallel dynamic programming model, which applies the knowledge-based approach, to estimate the maximum energy production of the Three Gorges Project (TGP) and the Gezhouba Project (GZB) cascade hydropower plants in China in the year of 2010, with 1 day as the time step and 365 days as the time horizon. The case study results show that the maximum energy production of the TGP-GZB system would be 1,035.0 × 108 kWh in 2010 under the current operating rules. Thus, there is still room for improvement in energy production, with the maximum increase of 1.83\% (18.3 × 108 kWh) from optimizing hydropower plant operation and 3.46\% (34.6 × 108 kWh) from optimizing both hydropower plant operation and reservoir operation. The overall approach is effective in providing optimal assessment solutions with an acceptable computation time. Thus, it can be concluded that the approach can be applied for planning purposes or providing more reasonable boundaries for real-time operation. © 2014 American Society of Civil Engineers.},
	number = {6},
	journal = {Journal of Water Resources Planning and Management},
	author = {Li, X. and Wei, J. and Fu, X. and Li, T. and Wang, G.},
	year = {2014},
	note = {Publisher: American Society of Civil Engineers (ASCE)},
	keywords = {Cascade hydropower plants, China, Concrete dams, Dynamic programming, Gezhouba Dam, Hubei, Hydroelectric power, Hydroelectric power plants, Hydropower plants, Knowledge based systems, Knowledge-based approach, Parallel dynamic programming, Reservoir operation, Three Gorges, hydroelectric power plant, knowledge based system, optimization, reservoir},
}

@article{hager_knowledge_2010,
	title = {A knowledge based approach to loss severity assessment in financial institutions using {Bayesian} networks and loss determinants},
	volume = {207},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957709199&doi=10.1016%2fj.ejor.2010.06.020&partnerID=40&md5=9b5a6ff334d037b66e9aec87d8403e46},
	doi = {10.1016/j.ejor.2010.06.020},
	abstract = {Modelling loss severity from rare operational risk events with potentially catastrophic consequences has proved a difficult task for practitioners in the finance industry. Efforts to develop loss severity models that comply with the BASEL II Capital Accord have resulted in two principal model directions where one is based on scenario generated data and the other on scaling of pooled external data. However, lack of relevant historical data and difficulties in constructing relevant scenarios frequently raise questions regarding the credibility of the resulting loss predictions. In this paper we suggest a knowledge based approach for establishing severity distributions based on loss determinants and their causal influence. Loss determinants are key elements affecting the actual size of potential losses, e.g. market volatility, exposure and equity capital. The loss severity distribution is conditional on the state of the identified loss determinants, thus linking loss severity to underlying causal drivers. We suggest Bayesian Networks as a powerful framework for quantitative analysis of the causal mechanisms determining loss severity. Leaning on available data and expert knowledge, the approach presented in this paper provides improved credibility of the loss predictions without being dependent on extensive data volumes. © 2010 Elsevier B.V. All rights reserved.},
	number = {3},
	journal = {European Journal of Operational Research},
	author = {Häger, D. and Andersen, L.B.},
	year = {2010},
	keywords = {Advanced measurement approach, Basel II, Bayesian networks, Catastrophic consequences, Data volume, Distributed parameter networks, Equity capital, Expert knowledge, Finance, Financial institution, Forecasting, Historical data, Inference engines, Intelligent networks, Key elements, Knowledge based systems, Knowledge-based approach, Loss prediction, Market volatility, Operational risks, Potential loss, Quantitative analysis, Risk analysis, Risk management, Severity model, Societies and institutions},
	pages = {1635--1644},
}

@article{wu_cancelable_2019,
	title = {Cancelable {Biometric} {Recognition} with {ECGs}: {Subspace}-{Based} {Approaches}},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055147077&doi=10.1109%2fTIFS.2018.2876838&partnerID=40&md5=2a9771d77101723a9d417206e276dd14},
	doi = {10.1109/TIFS.2018.2876838},
	abstract = {Relying on physiological or behavioral traits for identity recognition, biometric technologies offer several advantages over conventional possession- and knowledge-based approaches and are now widely used in diverse applications. However, as most biometrics (e.g., fingerprints, irises, etc.) in use are extrinsic, susceptible to replay attacks, and could result in the disclosure of individuals' physiological and pathological conditions, security and privacy concerns must be considered. In this paper, several electrocardiogram (ECG)-based cancelable biometric schemes are proposed to mitigate such concerns. The intrinsic and dynamic nature of ECGs and their inherent indication of life make them extremely difficult to steal or counterfeit. Using the concept of 'signal subspace collapsing,' distinct biometric templates associated with an individual's ECGs can be constructed such that it is possible to revoke a compromised template like a password. By incorporating different strategies for common subspace suppression, the well-known multiple signal classification method can effectively determine the identity of any individual just via his/her ECGs. Unlike existing cancelable biometrics, the recognition can be accomplished without knowledge of the distortion transformation, which further increases the difficulty of recovering the original ECGs from their templates. Various experiments with real ECGs from 285 subjects are conducted to illustrate the efficacy of the proposed schemes. © 2005-2012 IEEE.},
	number = {5},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Wu, S.-C. and Chen, P.-T. and Swindlehurst, A.L. and Hung, P.-L.},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Biomedical signal processing, Biometric recognition, Biometrics, Cancelable biometrics, Electrocardiography, Knowledge based systems, Knowledge-based approach, MUSIC, Multiple signal classification methods, Pathological conditions, Physiology, Security and privacy, subspace},
	pages = {1323--1336},
}

@article{vijay_kumar_machine_2002,
	title = {Machine recognition of printed {Kannada} text},
	volume = {2423},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249110735&doi=10.1007%2f3-540-45869-7_4&partnerID=40&md5=8c0db99d0e85a5e871e3558a9c145e1e},
	doi = {10.1007/3-540-45869-7_4},
	abstract = {This paper presents the design of a full fledged OCR system for printed Kannada text. The machine recognition of Kannada characters is difficult due to similarity in the shapes of different characters, script complexity and non-uniqueness in the representation of diacritics. The document image is subject to line segmentation, word segmentation and zone detection. From the zonal information, base characters, vowel modifiers and consonant conjucts are separated. Knowledge based approach is employed for recognizing the base characters. Various features are employed for recognising the characters. These include the coefficients of the Discrete Cosine Transform, Discrete Wavelet Transform and Karhunen-Louve Transform. These features are fed to different classifiers. Structural features are used in the subsequent levels to discriminate confused characters. Use of structural features, increases recognition rate from 93\% to 98\%. Apart from the classical pattern classification technique of nearest neighbour, Artificial Neural Network (ANN) based classifiers like Back Propogation and Radial Basis Function (RBF) Networks have also been studied. The ANN classifiers are trained in supervised mode using the transform features. Highest recognition rate of 99\% is obtained with RBF using second level approximation coefficients of Haar wavelets as the features on presegmented base characters. © 2002 Springer-Verlag Berlin Heidelberg.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Vijay Kumar, B. and Ramakrishnan, A.G.},
	year = {2002},
	keywords = {Approximation coefficients, Character recognition, Classification (of information), Complex networks, Discrete cosine transforms, Discrete wavelet transforms, Image segmentation, Knowledge based systems, Knowledge-based approach, Line segmentation, Linguistics, Machine recognition, Nearest neighbour, Neural networks, Optical character recognition, Pattern classification techniques, Radial basis function networks, Structural feature, Wavelet transforms, Word segmentation},
	pages = {37--48},
}

@article{baukus_knowledge_2004,
	title = {A knowledge based analysis of cache coherence},
	volume = {3308},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048850470&doi=10.1007%2f978-3-540-30482-1_15&partnerID=40&md5=907bc2cbcd14f06a13ed07fdf5be77c4},
	doi = {10.1007/978-3-540-30482-1_15},
	abstract = {This paper presents a case study of the application of the knowledge-based approach to concurrent systems specification, design and verification. A highly abstract solution to the cache coherence problem is first presented, in the form of a knowledge-based program, that formalises the intuitions underlying the MOESI [Sweazey \& Smith, 1986] characterisation of cache coherency protocols. It is shown that any concrete implementation of this knowledge-based program, which relates a cache's actions to its knowledge about the status of other caches, is a correct solution of the cache coherence problem. Three existing protocols in the MOESI class are shown to be such implementations. The knowledge-based characterisation furthermore raises the question of whether these protocols are optimal in their use of information available to the caches. This question is investigated using by the model checker MCK, which is able to verify specifications in the logic of knowledge and time. © Springer-Verlag Berlin Heidelberg 2004.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Baukus, K. and Van Der Meyden, R.},
	year = {2004},
	note = {Publisher: Springer Verlag},
	keywords = {Cache Coherence, Cache coherency protocol, Cache memory, Concurrent systems, Correct solution, Information use, Knowledge based, Knowledge based programs, Knowledge based systems, Knowledge-based approach, Model checker, Model checking, Specifications},
	pages = {99--114},
}

@article{bayar_using_2016,
	title = {Using immune designed ontologies to monitor disruptions in manufacturing systems},
	volume = {81},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951090611&doi=10.1016%2fj.compind.2015.09.004&partnerID=40&md5=40b6fb7d9be5fbf073c4bc119ec3092c},
	doi = {10.1016/j.compind.2015.09.004},
	abstract = {Manufacturing systems are subject to several kinds of disruptions and risks, which may break the continuity of workflows, disturb pre-set organization, and prevent the production system from reaching its expected levels of performance. Several approaches were proposed to deal with manufacturing system disruptions and risks. Unfortunately, most of them focus more on explaining the causes of the disruption/risk, rather than on determining disruption/risk effects on workflows, pre-set organization and expected performance. Existing approaches usually operate off-line, thus missing current and accurate data about plant activities and changing conditions. Most of them do not offer concepts that allow the design of computerized tools dedicated to disruption/risk monitoring and control. In this paper, we rely on biological immunity to guide the design of a knowledge-based approach, and to use it to monitor disruptions and risks in manufacturing systems. The suggested approach involves functions specifically dedicated to deal with a variety of disruptions and risks, such as detection, identification of consequences and reaction to disruptions. This architecture is intended to be embedded within industrial information and decision support systems, such as ERP (« Enterprise Resource Planning ») and MES (« Manufacturing Execution System »). A prototype implementation using ontologies and multi-agent systems shows the relevance of the suggested approach in monitoring disruptions and risks. A simplified example from the steel industry illustrates the kind of support that can be provided to decision makers. © 2015 Elsevier B.V. All rights reserved.},
	journal = {Computers in Industry},
	author = {Bayar, N. and Darmoul, S. and Hajri-Gabouj, S. and Pierreval, H.},
	year = {2016},
	note = {Publisher: Elsevier},
	keywords = {Artificial Immune System, Artificial intelligence, Biological immune system, Computerized tools, Decision making, Decision support systems, Disruption, Embedded systems, Enterprise resource planning, Immune system, Knowledge based systems, Knowledge-based approach, Manufacture, Manufacturing Execution System, Monitoring and control, Multi agent systems, Ontology, Prototype implementations, Risks, Steelmaking},
	pages = {67--81},
}

@article{amato_novel_2010,
	title = {A novel approach to simulate gene-environment interactions in complex diseases},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77349092838&doi=10.1186%2f1471-2105-11-8&partnerID=40&md5=88180738f3c76830299e5c52255ad522},
	doi = {10.1186/1471-2105-11-8},
	abstract = {Background: Complex diseases are multifactorial traits caused by both genetic and environmental factors. They represent the major part of human diseases and include those with largest prevalence and mortality (cancer, heart disease, obesity, etc.). Despite a large amount of information that has been collected about both genetic and environmental risk factors, there are few examples of studies on their interactions in epidemiological literature. One reason can be the incomplete knowledge of the power of statistical methods designed to search for risk factors and their interactions in these data sets. An improvement in this direction would lead to a better understanding and description of gene-environment interactions. To this aim, a possible strategy is to challenge the different statistical methods against data sets where the underlying phenomenon is completely known and fully controllable, for example simulated ones.Results: We present a mathematical approach that models gene-environment interactions. By this method it is possible to generate simulated populations having gene-environment interactions of any form, involving any number of genetic and environmental factors and also allowing non-linear interactions as epistasis. In particular, we implemented a simple version of this model in a Gene-Environment iNteraction Simulator (GENS), a tool designed to simulate case-control data sets where a one gene-one environment interaction influences the disease risk. The main aim has been to allow the input of population characteristics by using standard epidemiological measures and to implement constraints to make the simulator behaviour biologically meaningful.Conclusions: By the multi-logistic model implemented in GENS it is possible to simulate case-control samples of complex disease where gene-environment interactions influence the disease risk. The user has full control of the main characteristics of the simulated population and a Monte Carlo process allows random variability. A knowledge-based approach reduces the complexity of the mathematical model by using reasonable biological constraints and makes the simulation more understandable in biological terms. Simulated data sets can be used for the assessment of novel statistical methods or for the evaluation of the statistical power when designing a study. © 2010 Amato et al; licensee BioMed Central Ltd.},
	journal = {BMC Bioinformatics},
	author = {Amato, R. and Pinelli, M. and D'Andrea, D. and Miele, G. and Nicodemi, M. and Raiconi, G. and Cocozza, S.},
	year = {2010},
	keywords = {Biological constraints, Computer simulation, Disease, Disease control, Diseases, Environment, Environmental risk factor, Gene-environment interaction, Genes, Genetic Predisposition to Disease, Humans, Knowledge based systems, Knowledge-based approach, Mathematical approach, Mathematical models, Models, Monte Carlo Method, Monte Carlo method, Multi-logistic modeling, Nonlinear interactions, Population characteristics, Risk Factors, Risk assessment, Statistical, Statistical methods, article, environment, general aspects of disease, genetic predisposition, genetics, human, risk factor, statistical model},
}

@article{aletras_computing_2012,
	title = {Computing similarity between items in a digital library of cultural heritage},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979798826&doi=10.1145%2f2399180.2399184&partnerID=40&md5=43fba95c2a2bb9c811f531e342bedfeb},
	doi = {10.1145/2399180.2399184},
	abstract = {Large amounts of cultural heritage content have now been digitized and are available in digital libraries. However, these are often unstructured and difficult to navigate. Automatic techniques for identifying similar items in these collections could be used to improve navigation since it would allow items that are implicitly connected to be linked together and allow sets of similar items to be clustered. Europeana is a large digital library containing more than 20 million digital objects from a set of cultural heritage providers throughout Europe. The diverse nature of this collection means that the items do not have standard metadata to assist navigation. A range of methods for computing the similarity between pairs of texts are applied to metadata records in Europeana in order to estimate the similarity between items. Various methods for computing similarity have been proposed and can be classified into two main approaches: (1) knowledge-based, which make use of external knowledge sources and (2) corpus-based approaches, which rely on analyzing the frequency distributions of words in documents. Both techniques are evaluated against manual judgements obtained for this study and a multiple-choice test created from manually generated categories in cultural heritage collections. We find that a combination of corpus and knowledge-based approaches provide the best results in both experiments. © 2012 ACM.},
	number = {4},
	journal = {Journal on Computing and Cultural Heritage},
	author = {Aletras, N. and Stevenson, M. and Clough, P.},
	year = {2012},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Automatic technique, Corpus-based approaches, Cultural heritage collections, Cultural heritages, Digital libraries, Europeana, Frequency distributions, Knowledge based systems, Knowledge-based approach, Metadata, Semantic similarity},
}

@article{abuzeina_cross-word_2011,
	title = {Cross-word {Arabic} pronunciation variation modeling for speech recognition},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052963053&doi=10.1007%2fs10772-011-9098-0&partnerID=40&md5=e229812773a447db457d9f8f19eaad89},
	doi = {10.1007/s10772-011-9098-0},
	abstract = {One of the problems in the speech recognition of Modern Standard Arabic (MSA) is the cross-word pronunciation variation. Cross-word pronunciation variations alter the phonetic spelling of words beyond their listed forms in the phonetic dictionary, leading to a number of Out-Of-Vocabulary (OOV) wordforms. This paper presents a knowledge-based approach to model cross-word pronunciation variation at both phonetic dictionary and language model levels. The proposed approach is based on modeling cross-word pronunciation variation by expanding the phonetic dictionary and corpus transcription. The Baseline system contains a phonetic dictionary of 14,234 words from a 5.4 hours corpus of Arabic broadcast news. The expanded dictionary contains 15,873 words. Also, the corpus transcription is expanded according to the applied Arabic phonological rules. Using Carnegie Mellon University (CMU) Sphinx speech recognition engine, the Enhanced system achieved Word Error Rate (WER) of 9.91\% on a test set of fully discretized transcription of about 1.1 hours of Arabic broadcast news. The WER is enhanced by 2.3\% compared to the Baseline system. © 2011 Springer Science+Business Media, LLC.},
	number = {3},
	journal = {International Journal of Speech Technology},
	author = {Abuzeina, D. and Al-Khatib, W. and Elshafei, M. and Al-Muhtaseb, H.},
	year = {2011},
	keywords = {Baseline systems, Broadcast news, Carnegie Mellon University, Computational linguistics, Cross-word, Knowledge based systems, Knowledge-based approach, Language model, Modern standards, Phonetic dictionary, Pronunciation variation, Speech, Speech recognition, Speech recognition engine, Test sets, Transcription, Word error rate},
	pages = {227--236},
}

@article{pons_assessing_2017,
	title = {Assessing machine learning classifiers for the detection of animals’ behavior using depth-based tracking},
	volume = {86},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020183627&doi=10.1016%2fj.eswa.2017.05.063&partnerID=40&md5=497b15ef432a128c8e754d35b1689910},
	doi = {10.1016/j.eswa.2017.05.063},
	abstract = {There is growing interest in the automatic detection of animals’ behaviors and body postures within the field of Animal Computer Interaction, and the benefits this could bring to animal welfare, enabling remote communication, welfare assessment, detection of behavioral patterns, interactive and adaptive systems, etc. Most of the works on animals’ behavior recognition rely on wearable sensors to gather information about the animals’ postures and movements, which are then processed using machine learning techniques. However, non-wearable mechanisms such as depth-based tracking could also make use of machine learning techniques and classifiers for the automatic detection of animals’ behavior. These systems also offer the advantage of working in set-ups in which wearable devices would be difficult to use. This paper presents a depth-based tracking system for the automatic detection of animals’ postures and body parts, as well as an exhaustive evaluation on the performance of several classification algorithms based on both a supervised and a knowledge-based approach. The evaluation of the depth-based tracking system and the different classifiers shows that the system proposed is promising for advancing the research on animals’ behavior recognition within and outside the field of Animal Computer Interaction. © 2017 Elsevier Ltd},
	journal = {Expert Systems with Applications},
	author = {Pons, P. and Jaen, J. and Catala, A.},
	year = {2017},
	note = {Publisher: Elsevier Ltd},
	keywords = {Animal-computer interactions, Animals, Artificial intelligence, Automatic Detection, Behavior recognition, Behavioral research, Classification algorithm, Intelligent systems, Knowledge based systems, Knowledge-based approach, Learning algorithms, Learning systems, Machine learning techniques, Pattern recognition, Pattern recognition systems, Remote communication, Tracking (position), Tracking system, Wearable technology},
	pages = {235--246},
}

@article{tang_using_2011,
	title = {Using a knowledge-based approach to remove blocking artifacts in skin images for forensic analysis},
	volume = {6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051752082&doi=10.1109%2fTIFS.2011.2157821&partnerID=40&md5=407a1ea533c834a1ff96f708eec9d4e3},
	doi = {10.1109/TIFS.2011.2157821},
	abstract = {Identifying individuals in evidence images, where their faces are covered or obstructed, is a challenging task. In the legal case, United States v. Michael Joseph Pepe (2008), Craft and Kong, who served as expert witnesses, used pigmented skin marks to identify a suspect in evidence images. Their expert opinions were challenged, partially because the blocking artifacts generated by the standard JPEG algorithm adversely affect the visibility of the small skin marks. In addition to this case, a huge amount of JPEG-compressed child pornography is posted online every day. Although many methods have been developed to remove blocking artifacts, they are ineffective for our target application. In this paper, a knowledge-based (KB) approach, which simultaneously removes JPEG blocking artifacts, and recovers skin features, is proposed. Given a training set containing both original and compressed skin images, the relationship between original blocks and compressed blocks can be established. This prior information is used to infer the original blocks of compressed evidence images. A Markov-model-based algorithm and a faster one-pass algorithm were developed to make inference, and a block synthesis algorithm was developed to handle the cases where compressed blocks are not contained in the training set. An indexing mechanism was also proposed to deal with large datasets efficiently. Extensive experiments were conducted on images with different characteristics and compression ratios. Both subjective and objective evaluations demonstrated that the KB approach is more effective than other methods. In summary, the KB approach is capable of removing blocking artifacts to recover useful skin features. © 2011 IEEE.},
	number = {3 PART 2},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Tang, C. and Kong, A.W.-K. and Craft, N.},
	year = {2011},
	keywords = {Algorithms, Biometrics, Block synthesis, Blocking artifacts, Computer crime, Expert opinion, Expert witness, Forensic analysis, Indexing mechanisms, Inference engines, JPEG algorithms, Knowledge based systems, Knowledge-based approach, Large datasets, Legal case, Markov model, Markov processes, Objective evaluation, One-pass, Pigmented skin, Prior information, Target application, Training sets, Vein pattern, pornography},
	pages = {1038--1049},
}

@article{jang_knowledge-based_2012,
	title = {A knowledge-based approach to arterial stiffness estimation using the digital volume pulse},
	volume = {6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864546702&doi=10.1109%2fTBCAS.2011.2177835&partnerID=40&md5=169beb1f20fd909f3f0b74689941d4be},
	doi = {10.1109/TBCAS.2011.2177835},
	abstract = {We have developed a knowledge based approach for arterial stiffness estimation. The proposed new approach reliably estimates arterial stiffness based on the analysis of age and heart rate normalized reflected wave arrival time. The proposed new approach reduces cost, space, technical expertise, specialized equipment, complexity, and increases the usability compared to recently researched noninvasive arterial stiffness estimators. The proposed method consists of two main stages: pulse feature extraction and linear regression analysis. The new approach extracts the pulse features and establishes a linear prediction equation. On evaluating proposed methodology with pulse wave velocity (PWV) based arterial stiffness estimators, the proposed methodology offered the error rate of 8.36\% for men and 9.52\% for women, respectively. With such low error rates and increased benefits, the proposed approach could be usefully applied as low cost and effective solution for ubiquitous and home healthcare environments. © 2007-2012 IEEE.},
	number = {4},
	journal = {IEEE Transactions on Biomedical Circuits and Systems},
	author = {Jang, D.-G. and Farooq, U. and Park, S.-H. and Goh, C.-W. and Hahn, M.},
	year = {2012},
	keywords = {Algorithms, Ankle Brachial Index, Arrival time, Arterial stiffness, Arteries, Biomedical Engineering, Blood Pressure, Cardio-vascular disease, Computer-Assisted, Computers, Digital volume pulse, Effective solution, Electrocardiography, Electronics, Equipment Design, Error rate, Estimation, Feature extraction, Female, Heart Rate, Heart rates, Home healthcare, Humans, Knowledge based systems, Knowledge-based approach, Linear Models, Linear prediction, Low costs, Male, Photoplethysmography, Pulse, Pulse Wave Analysis, Pulse wave velocity, Pulse-contour analysis, Reflected waves, Regression analysis, Reproducibility of Results, Signal Processing, Specialized equipment, Stiffness, Technical expertise, Vascular Stiffness, algorithm, ankle brachial index, arterial stiffness, artery, article, biomedical engineering, blood pressure, computer, electrocardiography, electronics, equipment, equipment design, female, heart rate, human, male, pathology, photoelectric plethysmography, pulse rate, pulse wave, reproducibility, signal processing, statistical model},
	pages = {366--374},
}

@article{faghihi_automation_2015,
	title = {Automation in construction scheduling: a review of the literature},
	volume = {81},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947493840&doi=10.1007%2fs00170-015-7339-0&partnerID=40&md5=b766c057e7f0fb004c269a593153ea1a},
	doi = {10.1007/s00170-015-7339-0},
	abstract = {Automating the development of construction schedules has been an interesting topic for researchers around the world for almost three decades. Researchers have approached solving scheduling problems with different tools and techniques. Whenever a new artificial intelligence or optimization tool has been introduced, researchers in the construction field have tried to use it to find the answer to one of their key problems—the “better” construction schedule. Each researcher defines this “better” slightly different. This article reviews the research on automation in construction scheduling from 1985 to 2014. It also covers the topic using different approaches, including case-based reasoning, knowledge-based approaches, model-based approaches, genetic algorithms, expert systems, neural networks, and other methods. The synthesis of the results highlights the share of the aforementioned methods in tackling the scheduling challenge, with genetic algorithms shown to be the most dominant approach. Although the synthesis reveals the high applicability of genetic algorithms to the different aspects of managing a project, including schedule, cost, and quality, it exposed a more limited project management application for the other methods. © 2015, Springer-Verlag London.},
	number = {9-12},
	journal = {International Journal of Advanced Manufacturing Technology},
	author = {Faghihi, V. and Nejat, A. and Reinschmidt, K.F. and Kang, J.H.},
	year = {2015},
	note = {Publisher: Springer-Verlag London Ltd},
	keywords = {Algorithms, Artificial intelligence, Automation, Automation in construction, Case based reasoning, Construction projects, Construction schedules, Construction scheduling, Expert systems, Genetic algorithms, Knowledge based systems, Knowledge-based approach, Management applications, Model based approach, Project management, Scheduling, Tools and techniques},
	pages = {1845--1856},
}

@article{li_novel_2010,
	title = {A novel connector-knowledge-based approach for disassembly precedence constraint generation},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953609421&doi=10.1007%2fs00170-009-2384-1&partnerID=40&md5=82b9443d7527e9f54d6dc45b963dfd39},
	doi = {10.1007/s00170-009-2384-1},
	abstract = {Maintenance and recycling of a product often requires its disassembly. To have a successful disassembly process, it essentially asks for a comprehensive study of disassembly constraints. Related studies have revealed that precedence constraints offer important information to remove the infeasible disassembly plans to attain practical and feasible ones. Standard connectors play an important role to fasten two or more components together. By analyzing the disassembly knowledge and experiences of connectors, it is possible to derive precedence constraint directly. In this paper, different approaches for disassembly precedence constraint generation are first reviewed. Then, a novel connector-knowledge-based approach is presented for automatic generation of disassembly precedence constraints. As a preliminary research, threaded fasteners and keys are analyzed and corresponding precedence rules are extracted. By integrating this novel knowledge-based reasoning (KR) with the geometric reasoning approach, both approaches are utilized to their best advantages. Necessary disassembly constraint information can be automatically generated with less computational complexity. A typical shaft assembly in a gear reducer is selected as the case study to illustrate the proposed approach. The result has shown that this KR approach is simple and efficient in generating reasonable disassembly constraints. © 2009 Springer-Verlag London Limited.},
	number = {1-4},
	journal = {International Journal of Advanced Manufacturing Technology},
	author = {Li, J. and Wang, Q. and Huang, P. and Shen, H.},
	year = {2010},
	keywords = {Automatic Generation, Automatically generated, Comprehensive studies, Computational complexity, Constraint analysis, Constraint information, Disassembly process, Gear reducers, Geometric reasoning, Knowledge and experience, Knowledge based systems, Knowledge-based approach, Knowledge-based reasoning, Precedence constraints, Shaft assemblies, Speed reducers, Standard connectors, Threaded fasteners},
	pages = {293--304},
}

@article{mandreoli_knowledge-based_2011,
	title = {Knowledge-based sense disambiguation (almost) for all structures},
	volume = {36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649484022&doi=10.1016%2fj.is.2010.08.004&partnerID=40&md5=43f0983c415f0d49a9b3c1971450f837},
	doi = {10.1016/j.is.2010.08.004},
	abstract = {Structural disambiguation is acknowledged as a very real and frequent problem for many semantic-aware applications. In this paper, we propose a unified answer to sense disambiguation on a large variety of structures both at data and metadata level such as relational schemas, XML data and schemas, taxonomies, and ontologies. Our knowledge-based approach achieves a general applicability by converting the input structures into a common format and by allowing users to tailor the extraction of the context to the specific application needs and structure characteristics. Flexibility is ensured by supporting the combination of different disambiguation methods together with different information extracted from different sources of knowledge. Further, we support both assisted and completely automatic semantic annotation tasks, while several novel feedback techniques allow us to improve the initial disambiguation results without necessarily requiring user intervention. An extensive evaluation of the obtained results shows the good effectiveness of the proposed solutions on a large variety of structure-based information and disambiguation requirements. © 2010 Elsevier B.V. All rights reserved.},
	number = {2},
	journal = {Information Systems},
	author = {Mandreoli, F. and Martoglia, R.},
	year = {2011},
	keywords = {Common format, Data and metadata, Disambiguation method, Feedback techniques, Knowledge based systems, Knowledge-based approach, Metadata, Natural language processing systems, Ontology, Relational schemas, Schemas, Semantic Web, Semantic annotations, Structural disambiguation, Structure characteristic, Structure-based, Structure-based information, User intervention, Word sense disambiguation, XML data},
	pages = {406--430},
}

@article{haridas_critical_2018,
	title = {A critical review and analysis on techniques of speech recognition: {The} road ahead},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044469555&doi=10.3233%2fKES-180374&partnerID=40&md5=a27910e563da1a2afe4bfd72a850928f},
	doi = {10.3233/KES-180374},
	abstract = {Recognition of human speech has long been an intriguing issue among artificial intelligence and processing researchers. Speech is the most crucial and essential method of communication among the human beings. Several research efforts have been prepared in the field of speech recognition in the previous decades. Accordingly, a survey of speech recognition strategies suitable for human identification is discussed in this study. The main motivation of this survey is to explore the existing speech recognition strategies so that the researchers can include all the necessary metrics in their works in this domain and the limitations in the existing ones can be overcome. In this review, diverse issues included in speech recognition methodologies is distinguished and distinctive speech recognition procedures were studied to discover which qualities is tended to in a given system and which is disregarded. Hence, we offer a detailed survey of 50 methods from standard publishers from the year of 2000 to 2015. Here, we categorize the research based on three dissimilar perspectives, like techniques utilized, applications and parameter measures. In addition, this study gives an elaborate idea about speech recognition techniques. © 2018 - IOS Press and the authors. All rights reserved.},
	number = {1},
	journal = {International Journal of Knowledge-Based and Intelligent Engineering Systems},
	author = {Haridas, A.V. and Marimuthu, R. and Sivakumar, V.G.},
	year = {2018},
	note = {Publisher: IOS Press},
	keywords = {Artificial intelligence, Critical review, Fuzzy logic, Hidden Markov models, Human being, Human identification, Human speech, Knowledge based systems, Knowledge-based approach, Recognition strategies, Research efforts, Speech, Speech communication, Speech recognition, Surveys},
	pages = {39--57},
}

@article{ting_hybrid_2011,
	title = {A hybrid knowledge-based approach to supporting the medical prescription for general practitioners: {Real} case in a {Hong} {Kong} medical center},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551682675&doi=10.1016%2fj.knosys.2010.12.011&partnerID=40&md5=bf0a768db0ff504e06cc6fa0e00b397a},
	doi = {10.1016/j.knosys.2010.12.011},
	abstract = {Objective: With the increased complexity and uncertainty in drug information, issuing medical prescriptions has become a vexing issue. As many as 240,000 medicines are available on the market, so this paper proposes a novel approach to the issuing of medical prescriptions. The proposed process will provide general practitioners (GPs) with medication advice and suggest a range of medicines for specific medical conditions by taking into consideration the collective pattern as well as the individual preferences of physicians' prescription decisions. Methods and material: A hybrid approach is described that uses a combination of case-based reasoning (CBR) and Bayesian reasoning. In the CBR process, all the previous knowledge retrieved via similarity measures is made available for the reference of physicians as to what medicines have been prescribed (to a particular patient) in the past. After obtaining the results from CBR, Bayesian reasoning is then applied to model the prescription experience of all physicians within the organization. By comparing the two sets of results, more refined recommendations on a range of medicines are suggested along with the ranking for each recommendation. Results: To validate the proposed approach, a Hong Kong medical center was selected as a testing site. Through application of the hybrid approach in the medical center for a period of one month, the results demonstrated that the approach produced satisfactory performance in terms of user satisfaction, ease of use, flexibility and effectiveness. In addition, the proposed approach yields better results and a faster learning rate than when either CBR or Bayesian reasoning are applied alone. Conclusion: Even with the help of a decision support system, the current approach to anticipating what drugs are to be prescribed is not flexible enough to cater for individual preferences of GPs, and provides little support for managing complex and dynamic changes in drug information. Therefore, with the increase in the amount of information about drugs, it is extremely difficult for physicians to write a good prescription. By integrating CBR and Bayesian reasoning, the general practitioners' prescription practices can be retrieved and compared with the collective prescription experience as modeled by probabilistic reasoning. As a result, physicians can select the drugs which are supported by informed evidential decisions. That is, they can take into consideration the pattern of decisions made by other physicians in similar cases. © 2010 Elsevier B.V. All rights reserved.},
	number = {3},
	journal = {Knowledge-Based Systems},
	author = {Ting, S.L. and Kwok, S.K. and Tsang, A.H.C. and Lee, W.B.},
	year = {2011},
	keywords = {Amount of information, Artificial intelligence, Bayesian networks, Bayesian reasoning, CBr, Case based reasoning, Decision making, Decision support systems, Decision theory, Dynamic changes, Ease of use, General practitioners, Hong-kong, Hospitals, Hybrid approach, Individual preference, Knowledge based systems, Knowledge-based approach, Learning rates, Medical center, Medical conditions, Medical prescription, Probabilistic reasoning, Similar case, Similarity measure, User satisfaction},
	pages = {444--456},
}

@article{khurana_genome_2010,
	title = {Genome scale prediction of substrate specificity for acyl adenylate superfamily of enzymes based on active site residue profiles},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949497027&doi=10.1186%2f1471-2105-11-57&partnerID=40&md5=77a76472a37ba7311fa8a02a90e5c286},
	doi = {10.1186/1471-2105-11-57},
	abstract = {Background: Enzymes belonging to acyl:CoA synthetase (ACS) superfamily activate wide variety of substrates and play major role in increasing the structural and functional diversity of various secondary metabolites in microbes and plants. However, due to the large sequence divergence within the superfamily, it is difficult to predict their substrate preference by annotation transfer from the closest homolog. Therefore, a large number of ACS sequences present in public databases lack any functional annotation at the level of substrate specificity. Recently, several examples have been reported where the enzymes showing high sequence similarity to luciferases or coumarate:CoA ligases have been surprisingly found to activate fatty acyl substrates in experimental studies. In this work, we have investigated the relationship between the substrate specificity of ACS and their sequence/structural features, and developed a novel computational protocol for in silico assignment of substrate preference.Results: We have used a knowledge-based approach which involves compilation of substrate specificity information for various experimentally characterized ACS and derivation of profile HMMs for each subfamily. These HMM profiles can accurately differentiate probable cognate substrates from non-cognate possibilities with high specificity (Sp) and sensitivity (Sn) (Sn = 0.91-1.0, Sp = 0.96-1.0) values. Using homologous crystal structures, we identified a limited number of contact residues crucial for substrate recognition i.e. specificity determining residues (SDRs). Patterns of SDRs from different subfamilies have been used to derive predictive rules for correlating them to substrate preference. The power of the SDR approach has been demonstrated by correct prediction of substrates for enzymes which show apparently anomalous substrate preference. Furthermore, molecular modeling of the substrates in the active site has been carried out to understand the structural basis of substrate selection. A web based prediction tool http://www.nii.res.in/pred\_acs\_substr.html has been developed for automated functional classification of ACS enzymes.Conclusions: We have developed a novel computational protocol for predicting substrate preference for ACS superfamily of enzymes using a limited number of SDRs. Using this approach substrate preference can be assigned to a large number of ACS enzymes present in various genomes. It can potentially help in rational design of novel proteins with altered substrate specificities. © 2010 Khurana et al; licensee BioMed Central Ltd.},
	journal = {BMC Bioinformatics},
	author = {Khurana, P. and Gokhale, R.S. and Mohanty, D.},
	year = {2010},
	keywords = {Acetate-CoA Ligase, Active site residues, Algorithms, Amino Acid, Amino Acid Sequence, Computational protocols, Enzymes, Forecasting, Functional annotation, Functional classification, Genes, Knowledge based systems, Knowledge-based approach, Plants (botany), Protein, Secondary metabolites, Sequence Analysis, Sequence Homology, Structure-Activity Relationship, Substrate Specificity, Substrate recognition, Substrate specificity, Substrates, Tin, acetyl coenzyme A synthetase, algorithm, amino acid sequence, article, chemistry, enzyme specificity, genetics, methodology, sequence analysis, sequence homology, structure activity relation},
}

@article{tang_intelligent_2001,
	title = {An intelligent feature-based design for stamping system},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034802829&doi=10.1007%2fs001700170074&partnerID=40&md5=34b6e8be9d81efc438e60601c0d4152c},
	doi = {10.1007/s001700170074},
	abstract = {In this paper, an intelligent feature-based DFS (design for stamping) system has been proposed for implementing the stampability evaluation. The work includes identification of the aims, criteria, and procedure of stampability evaluation, as well as the formalisation of the stampability evaluation knowledge. The stampability evaluation has been oriented to part shape analysis and cost estimation, based on features, and a knowledge-based system has been developed to enable designers to carry out the stampability evaluation automatically. The effectiveness of the intelligent feature-based DFS system for improving the stampability of part designs lies in the integration of design evaluation and cost estimation into a single knowledge-based system. Such integration should enable consistency in cost estimation and design evaluation. The application of the system is illustrated with an example.},
	number = {3},
	journal = {International Journal of Advanced Manufacturing Technology},
	author = {Tang, D.-B. and Zheng, L. and Li, Z.-Z.},
	year = {2001},
	keywords = {Cost effectiveness, Cost estimation, Evaluation, Intelligent feature based design, Knowledge based systems, Process engineering, Stampability evaluation, Stamping, Stamping system, Systems analysis},
	pages = {193--200},
}

@article{montejo-raez_knowledge-based_2014,
	title = {A knowledge-based approach for polarity classification in {Twitter}},
	volume = {65},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896492997&doi=10.1002%2fasi.22984&partnerID=40&md5=aedbf1ffc5e62eb4a6ceb69eb981e768},
	doi = {10.1002/asi.22984},
	abstract = {Until now, most of the methods published for polarity classification in Twitter have used a supervised approach. The differences between them are only the features selected and the method used for weighting them. In this article, we present an unsupervised method for polarity classification in Twitter. The method is based on the expansion of the concepts expressed in the tweets through the application of PageRank to WordNet. In addition, we integrate SentiWordNet to compute the final value of polarity. The synsets values are weighted with the PageRank scores obtained in the previous random walk process over WordNet. The results obtained show that disambiguation and expansion are good strategies for improving overall performance. © 2013 ASIS\&T.},
	number = {2},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Montejo-Ráez, A. and Martínez-Cámara, E. and Martín-Valdivia, M.T. and Ureña-López, L.A.},
	year = {2014},
	note = {Publisher: John Wiley and Sons Inc.},
	keywords = {Knowledge based systems, Knowledge engineering, Knowledge-based approach, NAtural language processing, Natural language processing systems, Ontology, PageRank, Pagerank score, Polarity classification, Random walk process, SentiWordNet, Social networking (online), Synsets, Unsupervised method, classification},
	pages = {414--425},
}

@article{zouaghi_combination_2012,
	title = {Combination of information retrieval methods with {LESK} algorithm for {Arabic} word sense disambiguation},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870251916&doi=10.1007%2fs10462-011-9249-3&partnerID=40&md5=2a30cfb780338af0e728409b3853156a},
	doi = {10.1007/s10462-011-9249-3},
	abstract = {In this paper, we propose to use Harman, Croft and Okapi measures with Lesk algorithm to develop a system for Arabic word sense disambiguation, that combines unsupervised and knowledge based methods. This system must solve the lexical semantic ambiguity in Arabic language. The information retrieval measures are used to estimate the most relevant sense of the ambiguous word, by returning a semantic coherence score corresponding to the context that is semantically closest to the original sentence containing the ambiguous word. The Lesk algorithm is used to assign and select the adequate sense from those proposed by the information retrieval measures mentioned above. This selection is based on a comparison between the glosses of the word to be disambiguated, and its different contexts of use extracted from a corpus. Our experimental study proves that using of Lesk algorithm with Harman, Croft, and Okapi measures allows us to obtain an accuracy rate of 73\%. © Springer Science+Business Media B.V. 2011.},
	number = {4},
	journal = {Artificial Intelligence Review},
	author = {Zouaghi, A. and Merhbene, L. and Zrigui, M.},
	year = {2012},
	keywords = {Accuracy rate, Algorithms, Arabic languages, Experimental studies, Incremental approach, Information retrieval, Knowledge based systems, Knowledge-based approach, Knowledge-based methods, Lexical semantics, Semantics, Word Sense Disambiguation},
	pages = {257--269},
}

@article{armstrong_knowledge-based_1990,
	title = {A knowledge-based approach for supporting locational decisionmaking},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025657984&doi=10.1068%2fb170341&partnerID=40&md5=55d5907a69f6f55d417d0c9eab446d43},
	doi = {10.1068/b170341},
	abstract = {A system for providing decision support to people who make locational decisions is described in which the domain-specific knowledge of users is combined with the general problem-solving strategies, techniques, and mathematical models of location analysis. The system elicits and stores separately environmental, procedural, and structural knowledge so that experts in particular problem domains can access, examine, and modify this knowledge. A metaplanner interacts with users to generate scenarios which describe the general problem-solving strategy to be pursued. These scenarios are organised into a series of tractable problems which are solved in a subproblem-solver module consisting of location-allocation and other analytical models. The system enables decisionmakers to examine systematically the results of a series of analyses leading to a desired solution. The approach is suitable for location-selection problems in complex geographical decisionmaking environments. -Authors},
	number = {3},
	journal = {Environment \& Planning B: Planning \& Design},
	author = {Armstrong, M.P. and De, S. and Densham, P.J. and Lolonis, P. and Rushton, G. and Tewari, V.K.},
	year = {1990},
	keywords = {decision-making process, knowledge-based approach, location theory},
	pages = {341--364},
}

@article{ma_practical_2015,
	title = {Practical implementation of knowledge-based approaches for steam-assisted gravity drainage production analysis},
	volume = {42},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935029565&doi=10.1016%2fj.eswa.2015.05.047&partnerID=40&md5=6cf1c089eeaed7df61fd9991c0c7e757},
	doi = {10.1016/j.eswa.2015.05.047},
	abstract = {Quantitative appraisal of different operating areas and assessment of uncertainty due to reservoir heterogeneities are crucial elements in optimization of production and development strategies in oil sands operations. Although detailed compositional simulators are available for recovery performance evaluation for steam-assisted gravity drainage (SAGD), the simulation process is usually deterministic and computationally demanding, and it is not quite practical for real-time decision-making and forecasting. Data mining and machine learning algorithms provide efficient modeling alternatives, particularly when the underlying physical relationships between system variables are highly complex, non-linear, and possibly uncertain. In this study, a comprehensive training set encompassing SAGD field data compiled from numerous publicly available sources is analyzed. Exploratory data analysis (EDA) is carried out to interpret and extract relevant attributes describing characteristics associated with reservoir heterogeneities and operating constraints. An extensive dataset consisting of over 70 records is assembled. Because of their ease of implementation and computational efficiency, knowledge-based techniques including artificial neural network (ANN) are employed to facilitate SAGD production performance prediction. The principal components analysis (PCA) technique is implemented to reduce the dimensionality of the input vector, alleviate the effects of over-fitting, and improve forecast quality. Statistical analysis is performed to analyze the uncertainties related to ANN model parameters and dataset. Predictions from the proposed approaches are both successful and reliable. It is demonstrated that model predictability is highly influenced by model parameter uncertainty. This work illustrates that data-driven models are capable of predicting SAGD recovery performance from log-derived and operational variables. The modeling approach can be updated when new information becomes available. The analysis presents an important potential to be integrated directly into existing reservoir management and decision-making routines. © 2015 Elsevier B.V. All rights reserved.},
	number = {21},
	journal = {Expert Systems with Applications},
	author = {Ma, Z. and Leung, J.Y. and Zanon, S. and Dzurman, P.},
	year = {2015},
	note = {Publisher: Elsevier Ltd},
	keywords = {Artificial intelligence, Complex networks, Computational efficiency, Computer system recovery, Data mining, Decision making, Drainage, Exploratory data analysis, Forecasting, Knowledge based systems, Knowledge-based approach, Learning algorithms, Learning systems, Model uncertainties, Neural networks, Oil sands, Petroleum engineering, Petroleum reservoirs, Principal component analysis, Principal components analysis, Production forecasts, Production performance prediction, Quality control, Real time decision-making, Reservoir management, Statistical methods, Steam-assisted gravity drainages, Uncertainty analysis},
	pages = {7326--7343},
}

@article{teijeiro_heartbeat_2018,
	title = {Heartbeat {Classification} {Using} {Abstract} {Features} from the {Abductive} {Interpretation} of the {ECG}},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043290273&doi=10.1109%2fJBHI.2016.2631247&partnerID=40&md5=1073e87e8e1a1675909c5870e2c8b953},
	doi = {10.1109/JBHI.2016.2631247},
	abstract = {Objective: This paper aims to prove that automatic beat classification on ECG signals can be effectively solved with a pure knowledge-based approach, using an appropriate set of abstract features obtained from the interpretation of the physiological processes underlying the signal. Methods: A set of qualitative morphological and rhythm features are obtained for each heartbeat as a result of the abductive interpretation of the ECG. Then, a QRS clustering algorithm is applied in order to reduce the effect of possible errors in the interpretation. Finally, a rule-based classifier assigns a tag to each cluster. Results: The method has been tested with the MIT-BIH Arrhythmia Database records, showing a significantly better performance than any other automatic approach in the state-of-the-art, and even improving most of the assisted approaches that require the intervention of an expert in the process. Conclusion: The most relevant issues in ECG classification, related to a large extent to the variability of the signal patterns between different subjects and even in the same subject over time, will be overcome by changing the reasoning paradigm. Significance: This paper demonstrates the power of an abductive framework for time-series interpretation to make a qualitative leap in the significance of the information extracted from the ECG by automatic methods. © 2013 IEEE.},
	number = {2},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Teijeiro, T. and Felix, P. and Presedo, J. and Castro, D.},
	year = {2018},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Abductive reasoning, Algorithms, Article, Automatic approaches, Beat classification, Biomedical signal processing, Cluster Analysis, Clustering algorithms, Computer-Assisted, Databases, Ecg classifications, Electrocardiography, Factual, Heart Rate, Heartbeat classifications, Humans, Knowledge based systems, Knowledge-based approach, P wave, Physiological process, QRS complex, Rule-based classifier, Signal Processing, T wave, algorithm, atrial fibrillation, bradycardia, cluster analysis, electrocardiography, factual database, heart arrhythmia, heart beat, heart rate, human, physiology, procedures, signal processing, sinus rhythm, tachycardia},
	pages = {409--420},
}

@article{choi_text_2014,
	title = {Text analysis for detecting terrorism-related articles on the web},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897614991&doi=10.1016%2fj.jnca.2013.05.007&partnerID=40&md5=25fe95ef0e8d8c52a9ea63149832f528},
	doi = {10.1016/j.jnca.2013.05.007},
	abstract = {Classifying web documents is considered as one of the most important tasks to reveal the terrorism-related documents. Internet provides a lot of valuable information to the users and the amount of web contents is progressively increasing. This makes it very difficult to identify potentially dangerous documents. Simply extracting keywords from documents is not enough to classify the contents. To build automated document classification systems, many techniques have been studied so far, but they are mostly statistical and knowledge-based approaches. These methods, however, do not yield satisfactory results because of complexity of natural languages. To overcome this deficiency, we propose a method to use word similarity based on WordNet hierarchy and n-gram data frequency. This method was tested with the sampled New York Times articles by querying four distinct words from four different areas. Experimental results show our proposed method effectively extracts context words from the text and identifies terrorism-related documents. © 2013 Elsevier Ltd. All rights reserved.},
	number = {1},
	journal = {Journal of Network and Computer Applications},
	author = {Choi, D. and Ko, B. and Kim, H. and Kim, P.},
	year = {2014},
	note = {Publisher: Academic Press},
	keywords = {Data mining, Document Classification, Document classification systems, Information retrieval systems, Knowledge based systems, Knowledge-based approach, National security, Natural languages, Terrorism, Text analysis, Text anlaysis, Text processing, Web crawler, Word similarity, WordNet hierarchy},
	pages = {16--21},
}

@article{wang_word_2020,
	title = {Word {Sense} {Disambiguation}: {A} comprehensive knowledge exploitation framework},
	volume = {190},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073823506&doi=10.1016%2fj.knosys.2019.105030&partnerID=40&md5=d01b423b4e46553d77bf38dcf1bc201a},
	doi = {10.1016/j.knosys.2019.105030},
	abstract = {Word Sense Disambiguation (WSD) has been a basic and on-going issue since its introduction in natural language processing (NLP) community. Its application lies in many different areas including sentiment analysis, Information Retrieval (IR), machine translation and knowledge graph construction. Solutions to WSD are mostly categorized into supervised and knowledge-based approaches. In this paper, a knowledge-based method is proposed, modeling the problem with semantic space and semantic path hidden behind a given sentence. The approach relies on the well-known Knowledge Base (KB) named WordNet and models the semantic space and semantic path by Latent Semantic Analysis (LSA) and PageRank respectively. Experiments has proven the method's effectiveness, achieving state-of-the-art performance in several WSD datasets. © 2019 The Author(s)},
	journal = {Knowledge-Based Systems},
	author = {Wang, Y. and Wang, M. and Fujita, H.},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Back-ground knowledge, Information retrieval, Knowledge based systems, Knowledge-based approach, Knowledge-based methods, Latent Semantic Analysis, NAtural language processing, Relation exploitation, Semantics, Sentiment analysis, State-of-the-art performance, Word Sense Disambiguation},
}

@article{thorleuchter_weak_2013,
	title = {Weak signal identification with semantic web mining},
	volume = {40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885063026&doi=10.1016%2fj.eswa.2013.03.002&partnerID=40&md5=f67e8c5000bd985bb456b257f5b06fb0},
	doi = {10.1016/j.eswa.2013.03.002},
	abstract = {We investigate an automated identification of weak signals according to Ansoff to improve strategic plan- ning and technological forecasting. Literature shows that weak signals can befound in the organization's environment and that they appear indifferent contexts.We use internet information to represent orga- nization 'senvironment and we select these websites that are related to a given hypothesis.In contrast to related research, a methodology is provided that uses latent semantic indexing (LSI) for the identification of weak signals.This improves existing knowledge based approaches because LSI considers the aspects of meaning and thus, it is able to identify similar textual patterns indifferent contexts.A new weak signal maximization approach is introduced that replaces the commonly used prediction modeli ngapproach in LSI. It enables to calculate the largest number of relevant weak signals represented by singular value decomposition (SVD) dimensions.A case study identifies and analyses weak signals to predict trends in the field of on-site medical oxygen production.This supports the planning of research and develop- ment (R\&D) for a medical oxygen supplier.As a result, it is shown that the proposed methodology enables organizations to identify weak signals from the internet for a given hypothesis.This helps strategic plan- ners to react ahead of time. © 2013 Elsevier Ltd. All rights reserved.},
	number = {12},
	journal = {Expert Systems with Applications},
	author = {Thorleuchter, D. and Van Den Poel, D.},
	year = {2013},
	note = {Publisher: Elsevier Ltd},
	keywords = {Ansoff, Automated identification, Data mining, Forecasting, Indexing (of information), Internet information, Knowledge based systems, Knowledge-based approach, Latent Semantic Indexing, Semantic Web, Singular value decomposition, Strategic planning, Technological forecasting, Weak signal identification, Weak signals, Web Mining, Web crawler},
	pages = {4978--4985},
}

@article{gan_recognizing_2016,
	title = {Recognizing an {Action} {Using} {Its} {Name}: {A} {Knowledge}-{Based} {Approach}},
	volume = {120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959507605&doi=10.1007%2fs11263-016-0893-6&partnerID=40&md5=bc2b1dbc508977ef3bfcba4eac836b3c},
	doi = {10.1007/s11263-016-0893-6},
	abstract = {Existing action recognition algorithms require a set of positive exemplars to train a classifier for each action. However, the amount of action classes is very large and the users’ queries vary dramatically. It is impractical to pre-define all possible action classes beforehand. To address this issue, we propose to perform action recognition with no positive exemplars, which is often known as the zero-shot learning. Current zero-shot learning paradigms usually train a series of attribute classifiers and then recognize the target actions based on the attribute representation. To ensure the maximum coverage of ad-hoc action classes, the attribute-based approaches require large numbers of reliable and accurate attribute classifiers, which are often unavailable in the real world. In this paper, we propose an approach that merely takes an action name as the input to recognize the action of interest without any pre-trained attribute classifiers and positive exemplars. Given an action name, we first build an analogy pool according to an external ontology, and each action in the analogy pool is related to the target action at different levels. The correlation information inferred from the external ontology may be noisy. We then propose an algorithm, namely adaptive multi-model rank-preserving mapping (AMRM), to train a classifier for action recognition, which is able to evaluate the relatedness of each video in the analogy pool adaptively. As multiple mapping models are employed, our algorithm has better capability to bridge the gap between visual features and the semantic information inferred from the ontology. Extensive experiments demonstrate that our method achieves the promising performance for action recognition only using action names, while no attributes and positive exemplars are available. © 2016, Springer Science+Business Media New York.},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Gan, C. and Yang, Y. and Zhu, L. and Zhao, D. and Zhuang, Y.},
	year = {2016},
	note = {Publisher: Springer New York LLC},
	keywords = {Action recognition, Action recognition algorithms, Algorithms, Attribute-based, Knowledge based systems, Knowledge-based approach, Lakes, Learning paradigms, Mapping, Maximum coverage, Preserving mappings, Semantic information, Semantics},
	pages = {61--77},
}

@article{hsieh_novel_2017,
	title = {Novel hierarchical fall detection algorithm using a multiphase fall model},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012164090&doi=10.3390%2fs17020307&partnerID=40&md5=76821a905a46c2eb99415384d470eade},
	doi = {10.3390/s17020307},
	abstract = {Falls are the primary cause of accidents for the elderly in the living environment. Reducing hazards in the living environment and performing exercises for training balance and muscles are the common strategies for fall prevention. However, falls cannot be avoided completely; fall detection provides an alarm that can decrease injuries or death caused by the lack of rescue. The automatic fall detection system has opportunities to provide real-time emergency alarms for improving the safety and quality of home healthcare services. Two common technical challenges are also tackled in order to provide a reliable fall detection algorithm, including variability and ambiguity. We propose a novel hierarchical fall detection algorithm involving threshold-based and knowledge-based approaches to detect a fall event. The threshold-based approach efficiently supports the detection and identification of fall events from continuous sensor data. A multiphase fall model is utilized, including free fall, impact, and rest phases for the knowledge-based approach, which identifies fall events and has the potential to deal with the aforementioned technical challenges of a fall detection system. Seven kinds of falls and seven types of daily activities arranged in an experiment are used to explore the performance of the proposed fall detection algorithm. The overall performances of the sensitivity, specificity, precision, and accuracy using a knowledge-based algorithm are 99.79\%, 98.74\%, 99.05\% and 99.33\%, respectively. The results show that the proposed novel hierarchical fall detection algorithm can cope with the variability and ambiguity of the technical challenges and fulfill the reliability, adaptability, and flexibility requirements of an automatic fall detection system with respect to the individual differences. © 2017 by the authors; licensee MDPI, Basel, Switzerland.},
	number = {2},
	journal = {Sensors (Switzerland)},
	author = {Hsieh, C.-Y. and Liu, K.-C. and Huang, C.-N. and Chu, W.-C. and Chan, C.-T.},
	year = {2017},
	note = {Publisher: MDPI AG},
	keywords = {Accident prevention, Accidental Falls, Algorithms, Ambulatory, Common strategy, Detection and identifications, Fall detection, Home Care Services, Individual Differences, Knowledge based systems, Knowledge-based algorithms, Knowledge-based approach, Living environment, Monitoring, Reproducibility of Results, Signal detection, Technical challenges, Wearable sensors, algorithm, ambulatory monitoring, falling, home care, reproducibility},
}

@article{han_improving_2013,
	title = {Improving word similarity by augmenting {PMI} with estimates of word polysemy},
	volume = {25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897687200&doi=10.1109%2fTKDE.2012.30&partnerID=40&md5=623e0afe0bf3f3b1bc699137844a1ed0},
	doi = {10.1109/TKDE.2012.30},
	abstract = {Pointwise mutual information (PMI) is a widely used word similarity measure, but it lacks a clear explanation of how it works. We explore how PMI differs from distributional similarity, and we introduce a novel metric, PMImax, that augments PMI with information about a word's number of senses. The coefficients of PMImax are determined empirically by maximizing a utility function based on the performance of automatic thesaurus generation. We show that it outperforms traditional PMI in the application of automatic thesaurus generation and in two word similarity benchmark tasks: human similarity ratings and TOEFL synonym questions. PMImax achieves a correlation coefficient comparable to the best knowledge-based approaches on the Miller-Charles similarity rating data set.© 2013 IEEE.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Han, L. and Finin, T. and McNamee, P. and Joshi, A. and Yesha, Y.},
	year = {2013},
	keywords = {Automatic thesaurus generation, Benchmarking, Correlation coefficient, Distributional similarities, Knowledge based systems, Knowledge-based approach, Pointwise mutual information, Semantic similarity, Semantics, Thesauri, Utility functions, Word similarity},
	pages = {1307--1322},
}

@article{moncada_selection_2013,
	title = {Selection of process pathways for biorefinery design using optimization tools: {A} colombian case for conversion of sugarcane bagasse to ethanol, poly-3-hydroxybutyrate ({PHB}), and energy},
	volume = {52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875449629&doi=10.1021%2fie3019214&partnerID=40&md5=9ea9709f8381a5fe50f6ad6ae1086c2c},
	doi = {10.1021/ie3019214},
	abstract = {In this paper a techno-economic and environmental analysis for a sugar cane bagasse biorefinery in Colombia is presented as a case study. An optimization procedure is shown in order to select the most promising process pathway for the production of fuel ethanol, poly-3-hydroxybutyrate (PHB), and electricity. Once the optimization procedure was done, the results served as criteria for the selection of technologies and raw materials distribution. The distribution results and technologies were used to feed the knowledge-based approach in process synthesis. Then three scenarios were simulated using the Aspen Plus software. The first scenario consisted of energy cogeneration, the second one was an arbitrary distribution, and the third one corresponded to the preselected pathway using the optimization subroutine. Each scenario was assessed from the techno-economic and environmental point of view according to the Colombian conditions. The best was subjected to the configuration previously designed through the optimization subroutine. For this case, the obtained economic margin was 53.83\%, the potential environmental impact was 0.16 PEI/kg products, and the biological GHG emissions of the processing stage represented 1.55 kg of biological CO2-e/kg of bagasse. © 2013 American Chemical Society.},
	number = {11},
	journal = {Industrial and Engineering Chemistry Research},
	author = {Moncada, J. and Matallana, L.G. and Cardona, C.A.},
	year = {2013},
	keywords = {Arbitrary distribution, Aspen Plus software, Bagasse, Carbon dioxide, Computer software, Environmental analysis, Environmental impact, Greenhouse gases, Knowledge based systems, Knowledge-based approach, Optimization, Optimization procedures, Optimization tools, Poly3-hydroxybutyrate (PHB), Refining, Sugar-cane bagasse},
	pages = {4132--4145},
}

@article{chen_towards_2003,
	title = {Towards a knowledge-based approach to semantic service composition},
	volume = {2870},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242308022&doi=10.1007%2f978-3-540-39718-2_21&partnerID=40&md5=e516a9bee63e36d7b904e48c26e2e996},
	doi = {10.1007/978-3-540-39718-2_21},
	abstract = {The successful application of Grid and Web Service technologies to real-world problems, such as e-Science [1], requires not only the development of a common vocabulary and meta-data framework as the basis for inter-agent communication and service integration but also the access and use of a rich repository of domain-specific knowledge for problem solving. Both requirements are met by the respective outcomes of ontological and knowledge engineering initiatives. In this paper we discuss a novel, knowledge-based approach to resource synthesis (service composition), which draws on the functionality of semantic web services to represent and expose available resources. The approach we use exploits domain knowledge to guide the service composition process and provide advice on service selection and instantiation. The approach has been implemented in a prototype workflow construction environment that supports the runtime recommendation of a service solution, service discovery via semantic service descriptions, and knowledge-based configuration of selected services. The use of knowledge provides a basis for full automation of service composition via conventional planning algorithms. Workflows produced by this system can be executed through a domain-specific direct mapping mechanism or via a more fluid approach such as WSDL-based service grounding. The approach and prototype have been used to demonstrate practical benefits in the context of the Geodise initiative [2]. © Springer-Verlag Berlin Heidelberg 2003.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Chen, L. and Shadbolt, N.R. and Goble, C. and Tao, F. and Cox, S.J. and Puleston, C. and Smart, P.R.},
	year = {2003},
	note = {Publisher: Springer Verlag},
	keywords = {Construction environment, Domain-specific knowledge, Inter-agent communications, Knowledge based systems, Knowledge-based approach, Problem solving, Quality of service, Semantic Web, Semantic service composition, Semantic service descriptions, Service composition process, Social networking (online), Web service technology, Web services, Websites},
	pages = {319--334},
}

@article{patel_knowledge-based_2009,
	title = {Knowledge-based approach to de {Novo} design using reaction vectors},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-66249098082&doi=10.1021%2fci800413m&partnerID=40&md5=d911c2dbe06c90cad1f6a1a89f346304},
	doi = {10.1021/ci800413m},
	abstract = {A knowledge-based approach to the de novo design of synthetically feasible molecules is described. The method is based on reaction vectors which represent the structural changes that take place at the reaction center along with the environment in which the reaction occurs. The reaction vectors are derived automatically from a database of reactions which is not restricted by size or reaction complexity. A structure generation algorithm has been developed whereby reaction vectors can be applied to previously unseen starting materials in order to suggest novel syntheses. The approach has been implemented in KNIME and is validated by reproducing known synthetic routes. We then present applications of the method in different drug design scenarios including lead optimization and library enumeration. The method offers great potential for capturing and using the growing body of data on reactions that is becoming available through electronic laboratory notebooks. © 2009 American Chemical Society.},
	number = {5},
	journal = {Journal of Chemical Information and Modeling},
	author = {Patel, H. and Bodkin, M.J. and Chen, B. and Gillet, V.J.},
	year = {2009},
	keywords = {De novo design, Drug Design, Electronic laboratory notebook, Knowledge based systems, Knowledge-based approach, Lead optimization, Reaction center, Starting materials, Structural change, Structure generation, Synthetic routes, Vectors},
	pages = {1163--1184},
}

@article{rahimifard_barriers_2009,
	title = {Barriers, drivers and challenges for sustainable product recovery and recycling},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957737776&doi=10.1080%2f19397030903019766&partnerID=40&md5=adc2659d56ad8efada5f8cd1a23ffb60},
	doi = {10.1080/19397030903019766},
	abstract = {There has been a significant growth in research and applications of product recovery and recycling over the last two decades, in particular with the view of recent product take-back legislation which has extended the responsibility of manufacturers to include the recovery and safe disposal of their products. However, at present, the global scale of product recovery applications is significantly disproportional to the total manufacturing output. Hence, to achieve the idealistic goal of 'zero landfill', there is a need to significantly improve and extend both the scale of product recovery activities and the range of manufacturing applications in which such activities have yet to be implemented. This paper examines a range of barriers, drivers and challenges in research and development for the next generation of product recovery initiatives. A range of existing applications and case studies undertaken for the UK market has been used to analyse issues related to: the need for improvement and expansion of current legislation on producer responsibility; product take-back and reverse logistic models for collection of used products; knowledge-based approaches for end-of-life considerations during the design phase; improved technologies and increased automation in pre- and post-fragmentation recycling processes and most importantly, the requirement for sustainable business models for establishing value recovery chains which can be based on the provision of services rather than products. The paper concludes by summarising the results of this analysis to bridge the gap between existing and future sustainable solutions for product recovery. © 2009 Taylor \& Francis.},
	number = {2},
	journal = {International Journal of Sustainable Engineering},
	author = {Rahimifard, S. and Coates, G. and Staikos, T. and Edwards, C. and Abu-Bakar, M.},
	year = {2009},
	keywords = {Applications and case studies, Design phase, End-of-life, Extended producer responsibility, Global scale, Improved technology, Knowledge based systems, Knowledge-based approach, Manufacturing applications, Manufacturing output, Product recovery, Product recycling, Product take-back, Recovery, Recycling, Recycling process, Research, Research and application, Research and development, Reverse logistics, Safe disposals, Sustainable business, Sustainable development, Sustainable products, Sustainable solution, Used product, Value recovery},
	pages = {80--90},
}

@article{lee_geometric_2000,
	title = {Geometric structure analysis of document images: {A} knowledge-based approach},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013207282&doi=10.1109%2f34.888708&partnerID=40&md5=e6f7b6feb02235ab28078a08f94c578f},
	doi = {10.1109/34.888708},
	abstract = {Geometric structure analysis is a prerequisite to create electronic documents from logical components extracted from document images. This paper presents a knowledge-based method for sophisticated geometric structure analysis of technical journal pages. The proposed knowledge base encodes geometric characteristics that are not only common in technical journals but also publication-specific in the form of rules. The method takes the hybrid of top-down and bottom-up techniques and consists of two phases: region segmentation and identification. Generally, the result of the segmentation process does not have a one-to-one matching with composite layout components. Therefore, the proposed method identifies nontext objects, such as images, drawings, and tables, as well as text objects, such as text lines and equations, by splitting or grouping segmented regions into composite layout components. Experimental results with 372 images scanned from the IEEE Transactions on Pattern Analysis and Machine Intelligence show that the proposed method has performed geometric structure analysis successfully on more than 99 percent of the test images, resulting in impressive performance compared with previous works. © 2000 IEEE.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lee, K.-H. and Choy, Y.-C. and Cho, S.-B.},
	year = {2000},
	pages = {1224--1240},
}

@article{ding_knowledge-based_2014,
	title = {Knowledge-based approaches in software documentation: {A} systematic literature review},
	volume = {56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897060718&doi=10.1016%2fj.infsof.2014.01.008&partnerID=40&md5=e7202d5f9f17bae069df579fcb17ae4c},
	doi = {10.1016/j.infsof.2014.01.008},
	abstract = {Context Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design. Objective The objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches. Method We use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol. Results Sixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation. Conclusions The findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively. © 2014 Elsevier B.V. All rights reserved.},
	number = {6},
	journal = {Information and Software Technology},
	author = {Ding, W. and Liang, P. and Tang, A. and Van Vliet, H.},
	year = {2014},
	note = {Publisher: Elsevier B.V.},
	keywords = {Application programs, Computer aided software engineering, Computer software reusability, Computer software selection and evaluation, Cost benefit analysis, Costs, Documentation of software, Engineering community, Future research directions, Information retrieval, Knowledge based systems, Knowledge-based approach, Quality of softwares, Software architecture, Software architecture design, Software design, Software documentation, Systematic literature review},
	pages = {545--567},
}

@article{mishra_land_2011,
	title = {Land cover classification of palsar images by knowledge based decision tree classi-fier and supervised classifiers based on sar observables},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956370894&doi=10.2528%2fPIERB11011405&partnerID=40&md5=487e30fe262c6f17034d857238a22f60},
	doi = {10.2528/PIERB11011405},
	abstract = {The intent of this paper is to explore the application of information obtained from fully polarimetric data for land cover classification. Various land cover classification techniques are available in the literature, but still uncertainty exists in labeling various clusters to their own classes without using any a priori information. Therefore, the present work is focused on analyzing useful intrinsic information extracted from SAR observables obtained by various decomposition techniques. The eigenvalue decomposition and Pauli decomposition have been carried out to separate classes on the basis of their scattering mechanisms. The various classification techniques (supervised: minimum distance, maximum likelihood, parallelepiped and unsupervised: Wishart) were applied in order to see possible differences among SAR observables in terms of information that they contain and their usefulness in classifying particular land cover type. Another important issue is labeling the clusters, and this work is carried out by decision tree classification that uses knowledge based approach. This classifier is implemented by scrupulous knowledge of data obtained by empirical evidence and their experimental validation. It has been demonstrated quantitatively that standard polarimetric parameters such as polarized backscatter coefficients (linear, circular and linear 45°), co and cross-pol ratios for both linear and circular polarizations can be used as information bearing features for making decision boundaries. This forms the basis of discrimination between various classes in sequential format. The classification approach has been evaluated for fully polarimetric ALOS PALSAR L-band level 1.1 data. The classifier uses these data to classify individual pixel into one of the five categories: water, tall vegetation, short vegetation, urban and bare soil surface. The quantitative results shown by this classifier give classification accuracy of about 86\%, which is better than other classification techniques.},
	number = {30},
	journal = {Progress In Electromagnetics Research B},
	author = {Mishra, P. and Singh, D. and Yamaguchi, Y.},
	year = {2011},
	note = {Publisher: Electromagnetics Academy},
	keywords = {Backscatter coefficients, Classification (of information), Classification technique, Decision tree classification, Decision trees, Eigenvalue decomposition, Eigenvalues and eigenfunctions, Experimental validations, Image classification, Knowledge based systems, Knowledge-based approach, Knowledge-based decision trees, Land cover classification, Maximum likelihood, Polarimeters, Radar imaging, Vegetation},
	pages = {47--70},
}

@article{chen_high-security_2016,
	title = {A {High}-{Security} {EEG}-{Based} {Login} {System} with {RSVP} {Stimuli} and {Dry} {Electrodes}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991113044&doi=10.1109%2fTIFS.2016.2577551&partnerID=40&md5=c52abc0110610b6bcb7644c3c32682e5},
	doi = {10.1109/TIFS.2016.2577551},
	abstract = {Lately, electroencephalography (EEG)-based auth-entication has received considerable attention from the scientific community. However, the limited usability of wet EEG electrodes as well as low accuracy for large numbers of users have so far prevented this new technology to become commonplace. In this study a novel EEG-based authentication system is presented, which is based on the rapid serial visual presentation paradigm and uses a knowledge-based approach for authentication. Twenty-nine subjects' data were recorded and analyzed with wet EEG electrodes as well as dry ones. A true acceptance rate of 100\% can be reached for all subjects with an average required login time of 13.5 s for wet and 27 s for dry electrodes. Average false acceptance rates for the dry electrode setup were estimated to be \$3.33×10-5. © 2016 IEEE.},
	number = {12},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Chen, Y. and Atnafu, A.D. and Schlattner, I. and Weldtsadik, W.T. and Roh, M.-C. and Kim, H.J. and Lee, S.-W. and Blankertz, B. and Fazli, S.},
	year = {2016},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Authentication, Authentication systems, Biometrics, Brain computer interface, Dry electrode, Electrodes, Electroencephalography, Electrophysiology, Enterprise resource planning, False acceptance rate, Interfaces (computer), Knowledge based systems, Knowledge-based approach, New technologies, RSVP, Rapid serial visual presentations, Scientific community, Security of data},
	pages = {2635--2647},
}

@article{barillari_hot-spots-guided_2008,
	title = {Hot-spots-guided receptor-based pharmacophores ({HS}-pharm): {A} knowledge-based approach to identify ligand-anchoring atoms in protein cavities and prioritize structure-based pharmacophores},
	volume = {48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449085235&doi=10.1021%2fci800064z&partnerID=40&md5=a6bb420e72582994115bcf37e9d92020},
	doi = {10.1021/ci800064z},
	abstract = {The design of biologically active compounds from ligand-free protein structures using a structure-based approach is still a major challenge. In this paper, we present a fast knowledge-based approach (HS-Pharm) that allows the prioritization of cavity atoms that should be targeted for ligand binding, by training machine learning algorithms with atom-based fingerprints of known ligand-binding pockets. The knowledge of hot spots for ligand binding is here used for focusing structure-based pharmacophore models. Three targets of pharmacological interest (neuraminidase, β2 adrenergic receptor, and cyclooxygenase-2) were used to test the evaluated methodology, and the derived structure-based pharmacophores were used in retrospective virtual screening studies. The current study shows that structure-based pharmacophore screening is a powerful technique for the fast identification of potential hits in a chemical library, and that it is a valid alternative to virtual screening by molecular docking. © 2008 American Chemical Society.},
	number = {7},
	journal = {Journal of Chemical Information and Modeling},
	author = {Barillari, C. and Marcou, G. and Rognan, D.},
	year = {2008},
	keywords = {Arsenic compounds, Artificial intelligence, Atomic physics, Atoms, Biochemistry, Biologically active compounds, Chemical libraries, Chlorine compounds, Cyclo-oxygenase 2, Digital libraries, Hot spotting, Hot-spots, Hydraulic structures, Knowledge based systems, Knowledge-based approach, Learning algorithms, Learning systems, Ligand-binding, Ligand-binding pockets, Ligand-free, Ligands, Machine-learning algorithms, Models, Molecular, Molecular docking, Neuraminidase, Pharmacodynamics, Pharmacophore, Pharmacophore modeling, Pharmacophores, Prioritization, Protein Conformation, Protein structures, Proteins, Virtual Screening, article, chemical structure, chemistry, ligand, protein, protein conformation, validation study},
	pages = {1396--1410},
}

@article{cambria_knowledge-based_2013,
	title = {Knowledge-based approaches to concept-level sentiment analysis},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880172556&doi=10.1109%2fMIS.2013.45&partnerID=40&md5=197160d73d226e19ee0f3436f40d4f37},
	doi = {10.1109/MIS.2013.45},
	abstract = {The guest editors introduce novel approaches to opinion mining and sentiment analysis that go beyond a mere word-level analysis of text and provide concept-level methods. Such approaches allow a more efficient passage from (unstructured) textual information to (structured) machine-processable data, in potentially any domain. © 2001-2011 IEEE.},
	number = {2},
	journal = {IEEE Intelligent Systems},
	author = {Cambria, E. and Schuller, B. and Liu, B. and Wang, H. and Havasi, C.},
	year = {2013},
	keywords = {Data mining, Intelligent systems, Knowledge based systems, Knowledge mining, Knowledge-based approach, Opinion mining, Sentiment analysis, Social datum, Textual information},
	pages = {12--14},
}

@article{rudd_generic_2010,
	title = {A generic knowledge-based approach to the analysis of partial discharge data},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77249159419&doi=10.1109%2fTDEI.2010.5412013&partnerID=40&md5=41b918f0d6d97674c92466f5bddcf277},
	doi = {10.1109/TDEI.2010.5412013},
	abstract = {Partial Discharge (PD) diagnosis is a recognized technique to detect defects within high voltage insulation in power system equipment. A variety of methods exist to capture the signals that are emitted during PD, and this paper focuses on the ultra high frequency (UHF) and IEC 60270 techniques. Phase-resolved patterns can be constructed from the PD data captured using either of these techniques and due to the individual signatures that different defects generate, experts can examine the phase-resolved pattern to classify the defect that created it. In recent years, knowledge regarding PD phenomena and phase-resolved patterns has increased, providing an opportunity to employ a knowledge-based system (KBS) to automate defect classification. Due to the consistent physical nature of PD across different high voltage apparatus and the ability to construct phase-resolved patterns from various sensors, the KBS offers a generic approach to the analysis of PD by taking the phase-resolved pattern as its input and identifying the physical PD processes associate with the pattern. This paper describes the advances of this KBS, highlighting its generic application through the use of several case studies, which present the diagnosis of defects captured through both the IEC 60270 and UHF techniques. This paper also demonstrates, in one of the case studies, how a limitation of previous pattern recognition techniques can be overcome by mimicking the approach of a PD expert when the pulses occur over the zero crossings of the voltage waveform of the phase-resolved pattern. © 2010 IEEE.},
	number = {1},
	journal = {IEEE Transactions on Dielectrics and Electrical Insulation},
	author = {Rudd, S. and McArthur, S.D.J. and Judd, M.D.},
	year = {2010},
	keywords = {Condition monitoring, Defect classification, Defects, Failure analysis, Fault diagnosis, Gas insulated substations, Generic approach, High voltage apparatus, High voltage insulation, IEC 60270, Knowledge based systems, Knowledge-based approach, Partial discharges, Pattern recognition, Pattern recognition techniques, Physical nature, Power system equipments, Transformers, UHF devices, UHF technique, Ultra-high frequency, Voltage waveforms, Zero-crossings},
	pages = {149--156},
}

@article{papageorgiou_application_2009,
	title = {Application of fuzzy cognitive maps for cotton yield management in precision farming},
	volume = {36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-69249232198&doi=10.1016%2fj.eswa.2009.04.046&partnerID=40&md5=121df8be4f6af5e4513a0eb198db3373},
	doi = {10.1016/j.eswa.2009.04.046},
	abstract = {The management of cotton yield behavior in agricultural areas is a very important task because it influences and specifies the cotton yield production. An efficient knowledge-based approach utilizing the method of fuzzy cognitive maps (FCMs) for characterizing cotton yield behavior is presented in this research work. FCM is a modelling approach based on exploiting knowledge and experience. The novelty of the method is based on the use of the soft computing method of fuzzy cognitive maps to handle experts' knowledge and on the unsupervised learning algorithm for FCMs to assess measurement data and update initial knowledge. The advent of precision farming generates data which, because of their type and complexity, are not efficiently analyzed by traditional methods. The FCM technique has been proved from the literature efficient and flexible to handle experts' knowledge and through the appropriate learning algorithms can update the initial knowledge. The FCM model developed consists of nodes linked by directed edges, where the nodes represent the main factors in cotton crop production such as texture, organic matter, pH, K, P, Mg, N, Ca, Na and cotton yield, and the directed edges show the cause-effect (weighted) relationships between the soil properties and cotton field. The proposed method was evaluated for 360 cases measured for three subsequent years (2001, 2003 and 2006) in a 5 ha experimental cotton yield. The proposed FCM model enhanced by the unsupervised nonlinear Hebbian learning algorithm, was achieved a success of 75.55\%, 68.86\% and 71.32\%, respectively for the years referred, in estimating/predicting the yield between two possible categories ("low" and "high"). The main advantage of this approach is the sufficient interpretability and transparency of the proposed FCM model, which make it a convenient consulting tool in describing cotton yield behavior. © 2009 Elsevier Ltd. All rights reserved.},
	number = {10},
	journal = {Expert Systems with Applications},
	author = {Papageorgiou, E.I. and Markinos, A. and Gemptos, T.},
	year = {2009},
	keywords = {Agricultural areas, Calcium, Cause-effect, Cellular radio systems, Conformal mapping, Cotton, Cotton yield, Crop production, Cultivation, Decision making, Education, Expert knowledge, Fuzzy cognitive map, Fuzzy cognitive maps, Fuzzy rules, Fuzzy systems, Geologic models, Hebbian learning algorithm, Interpretability, Knowledge and experience, Knowledge based systems, Knowledge-based approach, Large scale systems, Learning algorithms, Measurement data, Modeling, Organic compounds, Organic matter, Precision farming, Sodium, Soft computing, Soft computing methods, Soil property, Soils, Unsupervised learning, Yield, pH effects},
	pages = {12399--12413},
}

@article{oliver_automatic_2012,
	title = {Automatic microcalcification and cluster detection for digital and digitised mammograms},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856227864&doi=10.1016%2fj.knosys.2011.11.021&partnerID=40&md5=38efcf3e60fd76f44001968502a633ca},
	doi = {10.1016/j.knosys.2011.11.021},
	abstract = {In this paper we present a knowledge-based approach for the automatic detection of microcalcifications and clusters in mammographic images. Our proposal is based on using local features extracted from a bank of filters to obtain a local description of the microcalcifications morphology. The developed approach performs an initial training step in order to automatically learn and select the most salient features, which are subsequently used in a boosted classifier to perform the detection of individual microcalcifications. Subsequently, the microcalcification detection method is extended in order to detect clusters. The validity of our approach is extensively demonstrated using two digitised databases and one full-field digital database. The experimental evaluation is performed in terms of ROC analysis for the microcalcification detection and FROC analysis for the cluster detection, resulting in better than 80\% sensitivity at 1 false positive cluster per image. © 2011 Elsevier B.V. All rights reserved.},
	journal = {Knowledge-Based Systems},
	author = {Oliver, A. and Torrent, A. and Lladó, X. and Tortajada, M. and Tortajada, L. and Sentís, M. and Freixenet, J. and Zwiggelaar, R.},
	year = {2012},
	keywords = {Automatic Detection, Boosted classifiers, Calcification (biochemistry), Cluster detection, Computer-aided detection, Digital database, Experimental evaluation, False positive, Feature extraction, Full-field, Image analysis, Knowledge based systems, Knowledge-based approach, Local description, Local feature, Mammographic images, Mammography, Microcalcification clusters, Microcalcification detection, Microcalcifications, ROC analysis, Salient features, X ray screens},
	pages = {68--75},
}

@article{ghahramani_knowledge_2014,
	title = {A knowledge based approach for selecting energy-aware and comfort-driven {HVAC} temperature set points},
	volume = {85},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910034243&doi=10.1016%2fj.enbuild.2014.09.055&partnerID=40&md5=8797c4f6a428ca6d6f19928b663a6a29},
	doi = {10.1016/j.enbuild.2014.09.055},
	abstract = {HVAC systems are responsible for providing acceptable thermal conditions and indoor air quality for building occupants. Increasing thermal comfort and reducing HVAC related energy consumption are often seen as conflicting goals. Few researchers have investigated the feasibility of reducing HVAC related energy consumption by integrating occupants' personalized thermal comfort preferences into the HVAC control logic. In this study, we introduce a knowledge-based approach for improving HVAC system operations through coupling personalized thermal comfort preferences and energy consumption patterns. In our approach, thermal comfort preferences are learned online and then modeled as zone level personalized comfort profiles. Zone temperature set points are then selected through solving an optimization problem for energy, with comfort, indoor air quality, and system performance constraints taken into consideration. In the case that acceptable comfort levels for all occupants of a zone were not achievable, the approach selects set points that minimize the overall thermal discomfort level. Compared to an operational strategy focusing on comfort only, evaluation of our approach, which aims for both maintaining or improving comfort and reducing energy consumption, showed improvements by reducing average daily airflows for about 57.6 m3/h (12.08\%) in three target zones. © 2014 Elsevier B.V. All rights reserved.},
	journal = {Energy and Buildings},
	author = {Ghahramani, A. and Jazizadeh, F. and Becerik-Gerber, B.},
	year = {2014},
	note = {Publisher: Elsevier Ltd},
	keywords = {Energy aware, Energy conservation, Energy tradeoff, HVAC system, Knowledge-based approach, Office buildings, Setpoints, Thermal comfort},
	pages = {536--548},
}

@article{hofmann_knowledge-based_2009,
	title = {Knowledge-based approach towards hydrolytic degradation of polymer-based biomaterials},
	volume = {21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70249118953&doi=10.1002%2fadma.200802213&partnerID=40&md5=6fc5786057bac8d94c8b78ecb8d6dfbe},
	doi = {10.1002/adma.200802213},
	abstract = {The concept of hydrolytically degradable biomaterials was developed to enable the design of temporary implants that substitute or fulfill a certain function as long as required to support (wound) healing processes or to control the release of drugs. Examples are surgical implants, e.g., sutures, or implantable drug depots for treatment of cancer. In both cases degradability can help to avoid a second surgical procedure fór explanation. Although degradable surgical sutures are established in the clinical practice for more than 30 years, still more than 40\% of surgical sutures applied in clinics today are nondegradable.[1] A major limitation of the established degradable suture materials is the fact that their degradation behavior cannot reliably be predicted by applying existing experimental methodologies. Similar concerns also apply to other degradable implants. Therefore, a knowledge-based approach is clearly needed to overcome the described problems and to enable the tailored design of biodegradable polymer materials. In this Progress Report we describe two methods (as examples for tools for this fundamental approach): molecular modeling combining atomistic bulk interface models with quantum chemical studies and experimental investigations of macromolecule degradation in monolayers on Langmuir-Blodgett (LB) troughs. Finally, an outlook on related future research strategies is provided. © 2009 WILEY-VCH Verlag GmbH \& Co. KCaA, Weinheim.},
	number = {32-33},
	journal = {Advanced Materials},
	author = {Hofmann, D. and Entrialgo-Castaño, M. and Kratz, K. and Lendlein, A.},
	year = {2009},
	keywords = {Biodegradable polymers, Biological materials, Clinical practices, Degradability, Degradable biomaterials, Degradation, Degradation behavior, Experimental investigations, Experimental methodology, Healing process, Hydrolytic degradation, Implants (surgical), Interface model, Knowledge based systems, Knowledge-based approach, Langmuir-blodgett, Macromolecule degradation, Monolayers, Progress report, Quantum chemical studies, Research strategy, Scheduling, Surgical procedures, Surgical sutures, Suture materials},
	pages = {3237--3245},
}

@article{abbey_knowledge-based_2009,
	title = {A knowledge-based approach for control of two-level energy storage for wind energy systems},
	volume = {24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-67349222565&doi=10.1109%2fTEC.2008.2001453&partnerID=40&md5=71b068dc8159253802d6953d40d4160d},
	doi = {10.1109/TEC.2008.2001453},
	abstract = {The percentage of wind energy in a generation mix will ultimately be limited by its intermittency and uncertainty as a source of power. However, the pairing of wind with energy storage systems could be utilized in order to produce dispatchable power. This paper considers a two-level energy storage system for application to wind energy systems. A knowledge-based management algorithm is proposed in order to schedule the power from the two levels. The system is tested for two possible power systems applications and its performance is compared with that of an alternate scheduling approach. Results demonstrate that the proposed algorithm requires a lower storage rating due to its ability to better coordinate operation of the two devices. © 2009 IEEE.},
	number = {2},
	journal = {IEEE Transactions on Energy Conversion},
	author = {Abbey, C. and Strunz, K. and Joós, G.},
	year = {2009},
	keywords = {Energy storage, Energy storage systems, Flywheels, Generation mix, Intermittency, Knowledge based systems, Knowledge-based approach, Knowledge-based management, Neural networks, Power systems application, Wind energy, Wind energy systems, Wind power},
	pages = {539--547},
}

@article{agirre_random_2014,
	title = {Random {Walks} for {Knowledge}-{Based} {Word} {Sense} {Disambiguation}},
	volume = {40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897681540&doi=10.1162%2fCOLI_a_00164&partnerID=40&md5=587997054c96ca31b5545bccfa552955},
	doi = {10.1162/COLI_a_00164},
	abstract = {Word Sense Disambiguation (WSD) systems automatically choose the intended meaning of a word in context. In this article we present a WSD algorithm based on random walks over large Lexical Knowledge Bases (LKB). We show that our algorithm performs better than other graph-based methods when run on a graph built from WordNet and eXtended WordNet. Our algorithm and LKB combination compares favorably to other knowledge-based approaches in the literature that use similar knowledge on a variety of English data sets and a data set on Spanish.We include a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are publicly available, and the results easily reproducible. © 2014 Association for Computational Linguistics.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Agirre, E. and de Lacalle, O.L. and Soroa, A.},
	year = {2014},
	note = {Publisher: MIT Press Journals},
	keywords = {Graph algorithms, Graph-based methods, Graphic methods, In contexts, Knowledge based, Knowledge based systems, Knowledge-based approach, Lexical knowledge, Natural language processing systems, Ontology, Random Walk, Random processes, WSD algorithms, Word Sense Disambiguation},
	pages = {57--84},
}

@article{wright_automated_2010,
	title = {An automated technique for identifying associations between medications, laboratory results and problems},
	volume = {43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649328064&doi=10.1016%2fj.jbi.2010.09.009&partnerID=40&md5=4979c6719d33a37c8f195a8d04bc3731},
	doi = {10.1016/j.jbi.2010.09.009},
	abstract = {Background: The patient problem list is an important component of clinical medicine. The problem list enables decision support and quality measurement, and evidence suggests that patients with accurate and complete problem lists may have better outcomes. However, the problem list is often incomplete. Objective: To determine whether association rule mining, a data mining technique, has utility for identifying associations between medications, laboratory results and problems. Such associations may be useful for identifying probable gaps in the problem list. Design: Association rule mining was performed on structured electronic health record data for a sample of 100,000 patients receiving care at the Brigham and Women's Hospital, Boston, MA. The dataset included 272,749 coded problems, 442,658 medications and 11,801,068 laboratory results. Measurements: Candidate medication-problem and laboratory-problem associations were generated using support, confidence, chi square, interest, and conviction statistics. High-scoring candidate pairs were compared to a gold standard: the Lexi-Comp drug reference database for medications and Mosby's Diagnostic and Laboratory Test Reference for laboratory results. Results: We were able to successfully identify a large number of clinically accurate associations. A high proportion of high-scoring associations were adjudged clinically accurate when evaluated against the gold standard (89.2\% for medications with the best-performing statistic, chi square, and 55.6\% for laboratory results using interest). Conclusion: Association rule mining appears to be a useful tool for identifying clinically accurate associations between medications, laboratory results and problems and has several important advantages over alternative knowledge-based approaches. © 2010 Elsevier Inc.},
	number = {6},
	journal = {Journal of Biomedical Informatics},
	author = {Wright, A. and Chen, E.S. and Maloney, F.L.},
	year = {2010},
	keywords = {Association rule mining, Association rules, Associative processing, Automated techniques, Clinical, Clinical Laboratory Techniques, Clinical decision support, Clinical medicine, Complete problems, Computerized, Data Collection, Data Mining, Data mining, Data mining techniques, Data sets, Databases, Decision Support Systems, Decision support systems, Decision supports, Electronic Health Records, Electronic health record, Factual, Forms and Records Control, Gold standards, Hospital, Humans, Knowledge Bases, Knowledge based systems, Knowledge-based approach, Laboratories, Laboratory test, Medical Records Systems, Medication Systems, Medicine, Quality measurements, Records management, Reference database, accuracy, adult, algorithm, article, chi square distribution, clinical decision making, controlled study, data mining, decision support system, female, gold standard, health care, human, laboratory automation, laboratory test, major clinical study, male, medical information, medication error, medication therapy management, natural language processing, patient coding, priority journal, quality control, scoring system, statistics, technique},
	pages = {891--901},
}

@article{wiener_taxonomy_2009,
	title = {Taxonomy of human wayfinding tasks: {A} knowledge-based approach},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885894192&doi=10.1080%2f13875860902906496&partnerID=40&md5=790ecb4d31d02f8c603c6d869ceae050},
	doi = {10.1080/13875860902906496},
	abstract = {Although the term "Wayfinding" has been defined by several authors, it subsumes a whole set of tasks that involve different cognitive processes, drawing on different cognitive components. Research on wayfinding has been conducted with different paradigms using a variety of wayfinding tasks. This makes it difficult to compare the results and implications of many studies. A systematic classification is needed in order to determine and investigate the cognitive processes and structural components of how humans solve wayfinding problems. Current classifications of wayfinding distinguish tasks on a rather coarse level or do not take the navigator's knowledge, a key factor in wayfinding, into account.We present an extended taxonomy of wayfinding that distinguishes tasks by external constraints as well as by the level of spatial knowledge that is available to the navigator. The taxonomy will help to decrease ambiguity of wayfinding tasks and it will facilitate understanding of the differentiated demands a navigator faces when solving wayfinding problems. Copyright © Taylor \& Francis Group, LLC.},
	number = {2},
	journal = {Spatial Cognition and Computation},
	author = {Wiener, J.M. and Büchner, S.J. and Hölscher, C.},
	year = {2009},
	keywords = {Cognitive components, Cognitive process, Cognitive systems, External constraints, Knowledge based systems, Knowledge-based approach, Navigation, Spatial knowledge, Structural component, Taxonomies, Way-finding, Wayfinding tasks},
	pages = {152--165},
}
